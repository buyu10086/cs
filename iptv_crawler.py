import requests
import time
import random
import json
from datetime import datetime, timedelta
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# -------------------------- é…ç½®ä¼˜åŒ– --------------------------
IPTV_SOURCE_URLS = [
    "https://raw.githubusercontent.com/TianmuTNT/iptv/refs/heads/main/iptv.m3u",
    "https://raw.githubusercontent.com/kakaxi-1/zubo/refs/heads/main/IPTV.txt",
    "https://raw.githubusercontent.com/vbskycn/iptv/refs/heads/master/tv/iptv4.m3u",
    "https://raw.githubusercontent.com/mytv-android/China-TV-Live-M3U8/refs/heads/main/webview.m3u"
]
TIMEOUT = 3
OUTPUT_FILE = "iptv_playlist.m3u8"
REMOVE_DUPLICATE_CHANNELS = True
MIN_VALID_SOURCES = 3
MAX_THREADS = 30

# ç¼“å­˜é…ç½®
CACHE_FILE = "iptv_verified_cache.json"
CACHE_EXPIRE_HOURS = 24

# åçˆ¬éšæœºå»¶è¿Ÿ
MIN_DELAY = 0.1
MAX_DELAY = 0.5

# çº¿ç¨‹å®‰å…¨é”
channel_sources_map = {}
map_lock = threading.Lock()
verified_urls = set()
url_lock = threading.Lock()

# -------------------------- ç¼“å­˜ç›¸å…³å‡½æ•° --------------------------
def load_verified_cache():
    global verified_urls
    try:
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            cache_data = json.load(f)
        
        cache_time_str = cache_data.get("cache_time", "")
        if not cache_time_str:
            print("âš ï¸  ç¼“å­˜æ–‡ä»¶æ— æ—¶é—´æˆ³ï¼Œè·³è¿‡åŠ è½½")
            return
        
        cache_time = datetime.strptime(cache_time_str, '%Y-%m-%d %H:%M:%S')
        expire_time = cache_time + timedelta(hours=CACHE_EXPIRE_HOURS)
        current_time = datetime.now()
        
        if current_time > expire_time:
            print(f"âš ï¸  ç¼“å­˜å·²è¿‡æœŸï¼ˆè¶…è¿‡{CACHE_EXPIRE_HOURS}å°æ—¶ï¼‰ï¼Œè·³è¿‡åŠ è½½")
            return
        
        valid_urls = cache_data.get("verified_urls", [])
        verified_urls = set(valid_urls)
        print(f"âœ… æˆåŠŸåŠ è½½æœ¬åœ°ç¼“å­˜ï¼Œå…± {len(verified_urls)} ä¸ªå·²éªŒè¯æºï¼ˆç¼“å­˜æ—¶é—´ï¼š{cache_time_str}ï¼‰")
    
    except FileNotFoundError:
        print(f"â„¹ï¸  æœªæ‰¾åˆ°ç¼“å­˜æ–‡ä»¶ {CACHE_FILE}ï¼Œå°†åœ¨è¿è¡Œååˆ›å»º")
    except json.JSONDecodeError:
        print(f"âš ï¸  ç¼“å­˜æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œæ— æ³•åŠ è½½")
    except Exception as e:
        print(f"âš ï¸  åŠ è½½ç¼“å­˜å¤±è´¥ï¼š{str(e)[:50]}")

def save_verified_cache():
    try:
        cache_data = {
            "cache_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "verified_urls": list(verified_urls)
        }
        
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æˆåŠŸä¿å­˜ç¼“å­˜åˆ° {CACHE_FILE}ï¼Œå…± {len(verified_urls)} ä¸ªå·²éªŒè¯æº")
    except Exception as e:
        print(f"âŒ ä¿å­˜ç¼“å­˜å¤±è´¥ï¼š{str(e)[:50]}")

# -------------------------- åçˆ¬å»¶è¿Ÿå‡½æ•° --------------------------
def add_random_delay():
    delay = random.uniform(MIN_DELAY, MAX_DELAY)
    time.sleep(delay)

# -------------------------- å¹¶è¡ŒæŠ“å–æ•°æ®æº --------------------------
def fetch_single_source(url, idx):
    add_random_delay()
    try:
        response = requests.get(
            url,
            timeout=10,
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Referer": "https://github.com/",
                "Accept": "*/*"
            }
        )
        response.raise_for_status()
        lines = [line.strip() for line in response.text.splitlines() if line.strip() and not line.startswith("//")]
        print(f"âœ… æ•°æ®æº {idx+1} æŠ“å–æˆåŠŸï¼Œæœ‰æ•ˆè¡Œ {len(lines)}")
        return True, lines
    except Exception as e:
        print(f"âŒ æ•°æ®æº {idx+1} æŠ“å–å¤±è´¥ï¼š{str(e)[:50]}")
        return False, []

def fetch_raw_iptv_data_parallel(url_list):
    all_lines = []
    valid_source_count = 0
    with ThreadPoolExecutor(max_workers=min(MAX_THREADS, len(url_list))) as executor:
        future_to_idx = {executor.submit(fetch_single_source, url, idx): idx for idx, url in enumerate(url_list)}
        for future in as_completed(future_to_idx):
            success, lines = future.result()
            if success and lines:
                all_lines.extend(lines)
                valid_source_count += 1
    print(f"\nğŸ“Š å¹¶è¡ŒæŠ“å–å®Œæˆï¼šå°è¯• {len(url_list)} æºï¼Œå¯ç”¨ {valid_source_count} æº")
    return all_lines

# -------------------------- é¢‘é“åç§°æå– --------------------------
def extract_channel_name(line):
    if line.startswith("#EXTINF:"):
        match = re.search(r',([^,]+)$', line)
        if not match:
            match = re.search(r'tvg-name="([^"]+)"', line)
        if match:
            return match.group(1).strip()
    return None

# -------------------------- æºéªŒè¯ --------------------------
def verify_single_source(url, channel_name):
    add_random_delay()
    if not url.startswith(("http://", "https://")):
        return None, None
    
    with url_lock:
        if url in verified_urls:
            return channel_name, url
    
    try:
        response = requests.get(
            url,
            timeout=TIMEOUT,
            allow_redirects=True,
            headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36"},
            stream=True
        )
        if response.status_code in [200, 206, 301, 302, 307, 308]:
            with url_lock:
                verified_urls.add(url)
            return channel_name, url
    except:
        pass
    return None, None

# -------------------------- è‡ªå®šä¹‰é¢‘é“åˆ†ç»„ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼‰ --------------------------
def get_channel_group(channel_name):
    """ä¿®æ”¹ä¸ºè‡ªå®šä¹‰åˆ†ç±»ï¼šå¤®è§†é¢‘é“ã€å«è§†é¢‘é“ã€æ•°å­—é¢‘é“ã€åŒ—äº¬é¢‘é“ã€æ²³å—çœçº§ã€å…¶ä»–é¢‘é“"""
    if not channel_name:
        return "ğŸ¬ å…¶ä»–é¢‘é“"
    
    # 1. å¤®è§†é¢‘é“
    cctv_keywords = ["CCTV", "å¤®è§†", "ä¸­å¤®", "å¤®è§†é¢‘", "CCTV-", "ä¸­è§†"]
    if any(keyword in channel_name for keyword in cctv_keywords):
        return "ğŸ“º å¤®è§†é¢‘é“"
    
    # 2. å«è§†é¢‘é“
    if "å«è§†" in channel_name:
        return "ğŸ“¡ å«è§†é¢‘é“"
    
    # 3. æ•°å­—é¢‘é“
    digital_keywords = ["æ•°å­—", "ä»˜è´¹", "é«˜æ¸…", "4K", "å½±è§†é¢‘é“", "ç»¼è‰ºé¢‘é“"]
    if any(keyword in channel_name for keyword in digital_keywords):
        return "ğŸ”¢ æ•°å­—é¢‘é“"
    
    # 4. åŒ—äº¬é¢‘é“
    if "åŒ—äº¬" in channel_name and "å«è§†" not in channel_name:
        return "ğŸ™ï¸ åŒ—äº¬é¢‘é“"
    
    # 5. æ²³å—çœçº§
    if "æ²³å—" in channel_name and "å«è§†" not in channel_name:
        return "ğŸŒ æ²³å—çœçº§"
    
    # å…¶ä»–
    return "ğŸ¬ å…¶ä»–é¢‘é“"

# -------------------------- ç”ŸæˆM3U8ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼šåˆ†ç±»ä¸­å¢åŠ æ›´æ–°æ—¶é—´ï¼‰ --------------------------
def generate_m3u8_parallel(raw_lines):
    update_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')  # åŒ—äº¬æ—¶åˆ»ï¼ˆç³»ç»Ÿæ—¶åŒºé»˜è®¤ä¸œå…«åŒºï¼‰
    m3u8_header = f"""#EXTM3U x-tvg-url="https://iptv-org.github.io/epg/guides/cn/tv.cctv.com.epg.xml"
# åŒ—äº¬æ—¶åˆ»æ›´æ–°æ—¶é—´ï¼š{update_time}
# æ”¯æŒå¤šæºåˆ‡æ¢+è‡ªå®šä¹‰é¢‘é“åˆ†ç»„
"""
    valid_lines = [m3u8_header]
    # å¢åŠ â€œæ›´æ–°æ—¶é—´â€ä½œä¸ºç‰¹æ®Šæ¡ç›®ï¼ˆå¯¹åº”ç•Œé¢çš„â€œæ›´æ–°æ—¶é—´â€åˆ†ç±»ï¼‰
    valid_lines.append(f"#EXTINF:-1 group-title='ğŸ“¢ ç³»ç»Ÿä¿¡æ¯',ğŸ“… åŒ—äº¬æ—¶åˆ»æ›´æ–°æ—¶é—´ï¼š{update_time}")
    valid_lines.append("#")

    # æå–å¾…éªŒè¯çš„(é¢‘é“å, url)å¯¹
    task_list = []
    temp_channel = None
    for line in raw_lines:
        line = line.strip()
        if not line:
            continue
        if line.startswith("#EXTINF:"):
            temp_channel = extract_channel_name(line)
        elif line.startswith(("http://", "https://")) and temp_channel:
            task_list.append((line, temp_channel))
            temp_channel = None
    print(f"\nğŸ” å¾…éªŒè¯æºæ€»æ•°ï¼š{len(task_list)}ï¼ˆå·²å¤ç”¨æœ¬åœ°ç¼“å­˜ï¼‰")

    # å¹¶è¡ŒéªŒè¯æº
    total_valid = 0
    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
        future_to_task = {executor.submit(verify_single_source, url, chan): (url, chan) for url, chan in task_list}
        for future in as_completed(future_to_task):
            chan_name, valid_url = future.result()
            if chan_name and valid_url:
                total_valid += 1
                with map_lock:
                    if chan_name not in channel_sources_map:
                        channel_sources_map[chan_name] = []
                    if valid_url not in channel_sources_map[chan_name]:
                        channel_sources_map[chan_name].append(valid_url)

    # è‡ªå®šä¹‰åˆ†ç±»é¡ºåºï¼ˆä¸ç•Œé¢ä¸€è‡´ï¼šæ›´æ–°æ—¶é—´ã€å¤®è§†é¢‘é“ã€å«è§†é¢‘é“ã€æ•°å­—é¢‘é“ã€åŒ—äº¬é¢‘é“ã€æ²³å—çœçº§ã€å…¶ä»–ï¼‰
    group_order = [
        "ğŸ“¢ ç³»ç»Ÿä¿¡æ¯",
        "ğŸ“º å¤®è§†é¢‘é“",
        "ğŸ“¡ å«è§†é¢‘é“",
        "ğŸ”¢ æ•°å­—é¢‘é“",
        "ğŸ™ï¸ åŒ—äº¬é¢‘é“",
        "ğŸŒ æ²³å—çœçº§",
        "ğŸ¬ å…¶ä»–é¢‘é“"
    ]
    grouped_channels = {group: [] for group in group_order}

    # æŒ‰åˆ†ç±»æ•´ç†é¢‘é“
    for channel_name, sources in channel_sources_map.items():
        if sources:
            group = get_channel_group(channel_name)
            grouped_channels[group].append((channel_name, sources))

    # ç”Ÿæˆåˆ†ç±»å†…å®¹ï¼ˆåŒ…å«æ›´æ–°æ—¶é—´ï¼‰
    for group_name in group_order:
        channels = grouped_channels[group_name]
        if not channels and group_name != "ğŸ“¢ ç³»ç»Ÿä¿¡æ¯":  # è·³è¿‡ç©ºåˆ†ç±»ï¼ˆé™¤äº†ç³»ç»Ÿä¿¡æ¯ï¼‰
            continue
        valid_lines.append(f"\n# {group_name}")
        for channel_name, sources in channels:
            valid_lines.append(f"#EXTINF:-1 group-title='{group_name}',{channel_name}ï¼ˆ{len(sources)}ä¸ªæºï¼‰")
            for url in sources:
                valid_lines.append(url)
                print(f"ğŸ“º [{group_name}] [{channel_name}] - æœ‰æ•ˆæºï¼š{url[:50]}...")

    # å®¹é”™é€»è¾‘
    if total_valid < MIN_VALID_SOURCES:
        print(f"\nâš ï¸  æœ‰æ•ˆæº({total_valid})ä½äºé˜ˆå€¼({MIN_VALID_SOURCES})ï¼Œç”ŸæˆåŸºç¡€æ–‡ä»¶")
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            f.write(f"""#EXTM3U
# åŒ—äº¬æ—¶åˆ»æ›´æ–°æ—¶é—´ï¼š{update_time}
#EXTINF:-1 group-title='ğŸ“¢ ç³»ç»Ÿä¿¡æ¯',ğŸ“… åŒ—äº¬æ—¶åˆ»æ›´æ–°æ—¶é—´ï¼š{update_time}
#
#EXTINF:-1 group-title='ğŸ“¢ ç³»ç»Ÿä¿¡æ¯',âš ï¸  æœ‰æ•ˆæºè¾ƒå°‘ï¼Œå»ºè®®ç¨åé‡è¯•
#""")
        return False

    # å†™å…¥æ–‡ä»¶
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(valid_lines))

    print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡ï¼šéªŒè¯ {len(task_list)} æºï¼Œæœ‰æ•ˆ {total_valid} æºï¼Œæœ‰æ•ˆé¢‘é“ {len(channel_sources_map)} ä¸ª")
    for group_name in group_order:
        if grouped_channels[group_name]:
            print(f"   ğŸ“‹ {group_name}ï¼š{len(grouped_channels[group_name])} é¢‘é“")
    print(f"âœ… ç”Ÿæˆå®Œæˆï¼š{OUTPUT_FILE}ï¼ˆåŒ—äº¬æ—¶åˆ»æ›´æ–°ï¼š{update_time}ï¼‰")
    return True

if __name__ == "__main__":
    start_time = time.time()
    print("========== å¹¶è¡ŒåŒ–IPTVæºæŠ“å–ï¼ˆè‡ªå®šä¹‰åˆ†ç±»+åŒ—äº¬æ—¶åˆ»ï¼‰ ==========")
    
    load_verified_cache()
    raw_data = fetch_raw_iptv_data_parallel(IPTV_SOURCE_URLS)
    if raw_data:
        generate_m3u8_parallel(raw_data)
    
    save_verified_cache()
    
    total_time = time.time() - start_time
    print(f"\nâ±ï¸  æ€»è¿è¡Œæ—¶é—´ï¼š{total_time:.2f} ç§’")
    print("========== æŠ“å–å®Œæˆï¼Œç¼“å­˜å·²ä¿å­˜ ==========")
