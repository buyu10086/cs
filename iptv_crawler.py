#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
IPTVæºé“¾æ¥çˆ¬è™«å·¥å…·ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆï¼‰
æ ¸å¿ƒä¼˜åŒ–ï¼šåç¨‹å¼‚æ­¥æµ‹é€Ÿ + æŒ‰åŸŸåé™æµ + æµ‹é€Ÿç¼“å­˜ + åŠ¨æ€å¹¶å‘ + HEADè¯·æ±‚ä¼˜å…ˆ
åŠŸèƒ½ï¼šæ¸…ç†å¤±æ•ˆé“¾æ¥+é«˜æ€§èƒ½å¹¶å‘æµ‹é€Ÿ+ç­›é€‰æœ€å¿«6æ¡+å¤±æ•ˆé“¾æ¥å½’æ¡£ï¼ˆæœ€å¤š10æ¡ï¼‰
"""
import re
import time
import asyncio
import aiohttp
from datetime import datetime
from pathlib import Path
from collections import defaultdict
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from concurrent.futures import ThreadPoolExecutor, as_completed, ProcessPoolExecutor

# ===============================
# å…¨å±€é…ç½®ï¼ˆå¯æ ¹æ®å®é™…éœ€æ±‚è°ƒæ•´ï¼‰
# ===============================
CONFIG = {
    # æ–‡ä»¶è·¯å¾„é…ç½®
    "SOURCE_TXT_FILE": "iptv_sources.txt",    # åŸå§‹IPTVæºé“¾æ¥æ–‡ä»¶
    "OLD_SOURCES_FILE": "old_sources.txt",    # å¤±æ•ˆé“¾æ¥å½’æ¡£æ–‡ä»¶
    "OUTPUT_FILE": "iptv_playlist.m3u8",      # æœ€ç»ˆç”Ÿæˆçš„æ’­æ”¾åˆ—è¡¨æ–‡ä»¶
    # æ ¸å¿ƒè§„åˆ™é…ç½®
    "MAX_OLD_RECORDS": 10,                    # å¤±æ•ˆé“¾æ¥å½’æ¡£æœ€å¤šä¿ç•™10æ¡
    "MAX_FAST_SOURCES": 6,                    # é€‰å–é€Ÿåº¦æœ€å¿«çš„6æ¡æœ‰æ•ˆé“¾æ¥
    # ç½‘ç»œè¯·æ±‚é…ç½®ï¼ˆæ€§èƒ½ä¼˜åŒ–æ ¸å¿ƒï¼‰
    "HEADERS": {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Connection": "keep-alive",           # å¤ç”¨è¿æ¥ï¼Œå‡å°‘æ¡æ‰‹å¼€é”€
        "Accept": "*/*"                       # ç®€åŒ–è¯·æ±‚å¤´ï¼Œå‡å°‘æœåŠ¡å™¨å¤„ç†å¼€é”€
    },
    "TEST_TIMEOUT_TOTAL": 3,                  # å•é“¾æ¥æ€»è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    "TEST_TIMEOUT_CONNECT": 1,                # è¿æ¥è¶…æ—¶ï¼ˆç§’ï¼‰ï¼šå»ºç«‹TCPè¿æ¥çš„æœ€å¤§æ—¶é—´
    "TEST_TIMEOUT_READ": 2,                   # è¯»å–è¶…æ—¶ï¼ˆç§’ï¼‰ï¼šè¯»å–å“åº”å¤´çš„æœ€å¤§æ—¶é—´
    "BASE_MAX_CONCURRENT": 60,                # åŸºç¡€æœ€å¤§å¹¶å‘æ•°ï¼ˆåç¨‹ï¼‰
    "DOMAIN_MAX_CONCURRENT": 5,               # å•ä¸ªåŸŸåæœ€å¤§å¹¶å‘æ•°ï¼ˆé¿å…å°ç¦ï¼‰
    "CACHE_EXPIRE_SECONDS": 600,              # æµ‹é€Ÿç¼“å­˜æœ‰æ•ˆæœŸï¼ˆ10åˆ†é’Ÿï¼‰
    "RETRY_TIMES": 1,                         # åŒæ­¥è¯·æ±‚é‡è¯•æ¬¡æ•°
    # çˆ¬è™«è¾…åŠ©é…ç½®
    "TOP_K": 3,                               # æ¯ä¸ªé¢‘é“ä¿ç•™æœ€ä¼˜æºæ•°é‡
    "IPTV_DISCLAIMER": "æœ¬æ–‡ä»¶ä»…ç”¨äºæŠ€æœ¯ç ”ç©¶ï¼Œè¯·å‹¿ç”¨äºå•†ä¸šç”¨é€”ï¼Œç›¸å…³ç‰ˆæƒå½’åŸä½œè€…æ‰€æœ‰"
}

# å…¨å±€æµ‹é€Ÿç¼“å­˜ï¼ˆkey=url, value=(æµ‹é€Ÿæ—¶é—´æˆ³, å“åº”æ—¶é—´ms)ï¼‰
SPEED_CACHE = {}

# ===============================
# é¢‘é“åˆ†ç±»æ˜ å°„ï¼ˆæŒ‰éœ€æ‰©å±•ï¼‰
# ===============================
CHANNEL_CATEGORIES = {
    "å¤®è§†é¢‘é“": ["CCTV1", "CCTV2", "CCTV3", "CCTV4", "CCTV5", "CCTV5+", "CCTV6", "CCTV7", "CCTV8", "CCTV9", "CCTV10", "CCTV11", "CCTV12", "CCTV13", "CCTV14", "CCTV15", "CCTV16", "CCTV17"],
    "å«è§†é¢‘é“": ["æ¹–å—å«è§†", "æµ™æ±Ÿå«è§†", "æ±Ÿè‹å«è§†", "ä¸œæ–¹å«è§†", "åŒ—äº¬å«è§†", "å¹¿ä¸œå«è§†", "æ²³å—å«è§†", "æ¹–åŒ—å«è§†", "å››å·å«è§†", "é‡åº†å«è§†"],
    "åœ°æ–¹é¢‘é“": ["æ¹–åŒ—å…¬å…±æ–°é—»", "æ­¦æ±‰æ–°é—»ç»¼åˆ", "éƒ‘å·1æ–°é—»ç»¼åˆ", "æ´›é˜³-1æ–°é—»ç»¼åˆ"]
}

# ===============================
# åŒæ­¥å·¥å…·å‡½æ•°ï¼ˆä»…ç”¨äºé“¾æ¥æœ‰æ•ˆæ€§æ£€æŸ¥ï¼‰
# ===============================
def create_stable_session():
    """åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„ç¨³å®šrequestsä¼šè¯ï¼ˆåŒæ­¥ï¼Œä»…ç”¨äºé“¾æ¥æœ‰æ•ˆæ€§æ£€æŸ¥ï¼‰"""
    session = requests.Session()
    retry_strategy = Retry(
        total=CONFIG["RETRY_TIMES"],
        backoff_factor=0.1,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET", "HEAD"]
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("https://", adapter)
    session.mount("http://", adapter)
    session.headers.update(CONFIG["HEADERS"])
    return session

def check_url_validity(url):
    """æ£€æŸ¥å•ä¸ªURLæ˜¯å¦æœ‰æ•ˆï¼ˆåŒæ­¥HEADè¯·æ±‚ï¼Œè½»é‡é«˜æ•ˆï¼‰"""
    session = create_stable_session()
    try:
        response = session.head(
            url,
            timeout=CONFIG["TEST_TIMEOUT_TOTAL"],
            allow_redirects=True,
            verify=False
        )
        return url, 200 <= response.status_code < 300
    except Exception:
        return url, False

def extract_domain(url):
    """æå–URLçš„åŸŸåï¼ˆç”¨äºæŒ‰åŸŸåé™æµï¼‰"""
    try:
        if not url.startswith(("http://", "https://")):
            url = f"http://{url}"
        domain = url.split("//")[1].split("/")[0]
        # å»é™¤ç«¯å£å·ï¼ˆå¦‚xxx.com:8080 â†’ xxx.comï¼‰
        if ":" in domain:
            domain = domain.split(":")[0]
        return domain.lower()
    except Exception:
        return "unknown_domain"

# ===============================
# å¼‚æ­¥æ ¸å¿ƒå‡½æ•°ï¼ˆæ€§èƒ½ä¼˜åŒ–é‡ç‚¹ï¼‰
# ===============================
async def create_async_session():
    """åˆ›å»ºé«˜æ€§èƒ½å¼‚æ­¥ä¼šè¯ï¼ˆå¸¦è¿æ¥æ± ã€ç²¾ç»†åŒ–è¶…æ—¶ã€ç¦ç”¨Cookieï¼‰"""
    # ç²¾ç»†åŒ–è¶…æ—¶é…ç½®
    timeout = aiohttp.ClientTimeout(
        connect=CONFIG["TEST_TIMEOUT_CONNECT"],
        sock_read=CONFIG["TEST_TIMEOUT_READ"],
        total=CONFIG["TEST_TIMEOUT_TOTAL"]
    )
    # è¿æ¥æ± é…ç½®ï¼šè½»é‡ã€é«˜æ•ˆã€é¿å…ç«¯å£è€—å°½
    connector = aiohttp.TCPConnector(
        limit=CONFIG["BASE_MAX_CONCURRENT"] * 2,  # è¿æ¥æ± å¤§å°ï¼ˆç•¥å¤§äºå¹¶å‘æ•°ï¼‰
        limit_per_host=CONFIG["DOMAIN_MAX_CONCURRENT"],  # å•ä¸ªåŸŸåé»˜è®¤è¿æ¥æ•°
        ttl_dns_cache=300,  # DNSç¼“å­˜5åˆ†é’Ÿï¼Œå‡å°‘é‡å¤è§£æ
        use_tcp_cork=True,  # å¯ç”¨TCP Corkï¼Œå‡å°‘å°åŒ…ä¼ è¾“
        fast_open=True      # å¯ç”¨TCPå¿«é€Ÿæ‰“å¼€ï¼ˆéœ€ç³»ç»Ÿæ”¯æŒï¼‰
    )
    # ç¦ç”¨Cookieï¼ˆIPTVæºæ— éœ€ç™»å½•ï¼Œå‡å°‘å¼€é”€ï¼‰
    session = aiohttp.ClientSession(
        timeout=timeout,
        connector=connector,
        headers=CONFIG["HEADERS"],
        cookie_jar=aiohttp.DummyCookieJar(),
        trust_env=True
    )
    return session

async def test_single_url_speed_async(url, semaphore):
    """
    å¼‚æ­¥æµ‹é€Ÿå•ä¸ªURLï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼‰
    ç­–ç•¥ï¼š1. å…ˆæŸ¥ç¼“å­˜ 2. HEADè¯·æ±‚ä¼˜å…ˆ 3. å¤±è´¥åˆ™é™çº§GETè¯»å–1å­—èŠ‚ 4. ç»“æœç¼“å­˜
    """
    # 1. æ£€æŸ¥ç¼“å­˜ï¼ˆæœªè¿‡æœŸåˆ™ç›´æ¥è¿”å›ï¼‰
    now = time.time()
    if url in SPEED_CACHE:
        cache_time, cache_rt = SPEED_CACHE[url]
        if now - cache_time < CONFIG["CACHE_EXPIRE_SECONDS"]:
            print(f"ğŸ“Œ ç¼“å­˜å¤ç”¨ | {url} | å“åº”æ—¶é—´ï¼š{cache_rt}ms")
            return url, cache_rt

    # 2. ä¿¡å·é‡é™æµï¼ˆæŒ‰åŸŸå/å…¨å±€ï¼‰
    async with semaphore:
        start_time = time.time()
        response_time = None
        try:
            async with await create_async_session() as session:
                # 3. ä¼˜å…ˆä½¿ç”¨HEADè¯·æ±‚ï¼ˆæœ€è½»é‡ï¼‰
                try:
                    async with session.head(
                        url,
                        allow_redirects=True,
                        ssl=False  # å¿½ç•¥SSLè¯ä¹¦é”™è¯¯
                    ) as resp:
                        if 200 <= resp.status < 300:
                            response_time = round((time.time() - start_time) * 1000, 2)
                except Exception:
                    # 4. HEADå¤±è´¥åˆ™é™çº§ä¸ºGETï¼ˆä»…è¯»å–1å­—èŠ‚ï¼Œè§¦å‘è¿æ¥å³å¯ï¼‰
                    async with session.get(
                        url,
                        allow_redirects=True,
                        ssl=False,
                        stream=True
                    ) as resp:
                        await resp.content.read(1)  # ä»…è¯»å–1å­—èŠ‚ï¼Œé¿å…ä¸‹è½½å¤§æ–‡ä»¶
                        if 200 <= resp.status < 300:
                            response_time = round((time.time() - start_time) * 1000, 2)
            
            # 5. ç¼“å­˜æœ‰æ•ˆæµ‹é€Ÿç»“æœ
            if response_time and response_time > 0:
                SPEED_CACHE[url] = (now, response_time)
                print(f"âœ… æµ‹é€ŸæˆåŠŸ | {url} | å“åº”æ—¶é—´ï¼š{response_time}ms")
            else:
                print(f"âŒ æµ‹é€Ÿå¤±è´¥ | {url} | åŸå› ï¼šé2xxçŠ¶æ€ç ")
        
        except Exception as e:
            print(f"âŒ æµ‹é€Ÿå¼‚å¸¸ | {url} | é”™è¯¯ï¼š{str(e)[:50]}")
        
        return url, response_time

async def dynamic_concurrent_speed_test(url_list):
    """
    åŠ¨æ€å¹¶å‘æµ‹é€Ÿï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼‰
    ç­–ç•¥ï¼š1. é¢„çƒ­æµ‹è¯• 2. æŒ‰æˆåŠŸç‡è°ƒæ•´å¹¶å‘ 3. æŒ‰åŸŸåé™æµ 4. å¼‚æ­¥æ‰¹é‡å¤„ç†
    """
    if not url_list:
        return []
    
    speed_results = []
    print(f"\nâš¡ å¼€å§‹é«˜æ€§èƒ½å¹¶å‘æµ‹é€Ÿ | å¾…æµ‹é€Ÿé“¾æ¥æ•°ï¼š{len(url_list)} | åŸºç¡€å¹¶å‘æ•°ï¼š{CONFIG['BASE_MAX_CONCURRENT']}")

    # ========== æ­¥éª¤1ï¼šé¢„çƒ­æµ‹è¯•ï¼ŒåŠ¨æ€è°ƒæ•´å¹¶å‘æ•° ==========
    warmup_size = max(10, len(url_list) // 10)  # å–10%æˆ–æœ€å°‘10æ¡åšé¢„çƒ­
    warmup_urls = url_list[:warmup_size]
    warmup_sem = asyncio.Semaphore(CONFIG["BASE_MAX_CONCURRENT"] // 2)  # é¢„çƒ­å¹¶å‘å‡åŠ
    
    # æ‰§è¡Œé¢„çƒ­æµ‹é€Ÿ
    warmup_tasks = [test_single_url_speed_async(url, warmup_sem) for url in warmup_urls]
    warmup_results = await asyncio.gather(*warmup_tasks)
    
    # è®¡ç®—é¢„çƒ­æˆåŠŸç‡ï¼ŒåŠ¨æ€è°ƒæ•´æœ€ç»ˆå¹¶å‘æ•°
    warmup_success = len([r for r in warmup_results if r[1] is not None and r[1] > 0])
    success_rate = warmup_success / len(warmup_urls) if warmup_urls else 1.0
    
    if success_rate < 0.8:
        final_max_concurrent = CONFIG["BASE_MAX_CONCURRENT"] // 2
        print(f"âš ï¸  é¢„çƒ­æˆåŠŸç‡{success_rate:.1%} < 80%ï¼Œé™ä½å¹¶å‘æ•°è‡³ï¼š{final_max_concurrent}")
    else:
        final_max_concurrent = CONFIG["BASE_MAX_CONCURRENT"]
        print(f"âœ… é¢„çƒ­æˆåŠŸç‡{success_rate:.1%} â‰¥ 80%ï¼Œä½¿ç”¨åŸºç¡€å¹¶å‘æ•°ï¼š{final_max_concurrent}")

    # ========== æ­¥éª¤2ï¼šæŒ‰åŸŸååˆ†ç»„ï¼Œç²¾ç»†åŒ–é™æµ ==========
    domain_groups = defaultdict(list)
    for url in url_list:
        domain = extract_domain(url)
        domain_groups[domain].append(url)
    
    # ä¸ºæ¯ä¸ªåŸŸååˆ›å»ºç‹¬ç«‹ä¿¡å·é‡ï¼ˆé¿å…å¯¹å•ä¸€åŸŸåæ‰“æ»¡ï¼‰
    domain_semaphores = {
        domain: asyncio.Semaphore(min(CONFIG["DOMAIN_MAX_CONCURRENT"], final_max_concurrent // 2))
        for domain in domain_groups.keys()
    }

    # ========== æ­¥éª¤3ï¼šæ‰¹é‡æ‰§è¡Œå¼‚æ­¥æµ‹é€Ÿ ==========
    tasks = []
    for domain, urls in domain_groups.items():
        sem = domain_semaphores[domain]
        for url in urls:
            tasks.append(test_single_url_speed_async(url, sem))
    
    # å¼‚æ­¥æ”¶é›†æ‰€æœ‰ç»“æœï¼ˆé«˜æ•ˆæ‰¹é‡å¤„ç†ï¼‰
    all_results = await asyncio.gather(*tasks)

    # ========== æ­¥éª¤4ï¼šè¿‡æ»¤æœ‰æ•ˆç»“æœå¹¶å¼‚æ­¥æ’åº ==========
    valid_results = [(url, rt) for url, rt in all_results if rt is not None and rt > 0]
    
    # å¼‚æ­¥æ’åºï¼ˆé¿å…é˜»å¡ä¸»çº¿ç¨‹ï¼‰
    def sort_results(results):
        return sorted(results, key=lambda x: x[1])
    
    # ä½¿ç”¨è¿›ç¨‹æ± æ‰§è¡Œæ’åºï¼ˆå¤§æ•°æ®é‡æ›´é«˜æ•ˆï¼‰
    loop = asyncio.get_running_loop()
    with ProcessPoolExecutor(max_workers=1) as executor:
        sorted_results = await loop.run_in_executor(executor, sort_results, valid_results)

    print(f"\nğŸ“Š æµ‹é€Ÿå®Œæˆ | æˆåŠŸ{len(sorted_results)}æ¡ | å¤±è´¥{len(url_list)-len(sorted_results)}æ¡")
    return sorted_results

# ===============================
# ä¸šåŠ¡é€»è¾‘å‡½æ•°ï¼ˆä¿ç•™åŸæœ‰æ ¸å¿ƒåŠŸèƒ½ï¼‰
# ===============================
def archive_invalid_urls(invalid_urls):
    """å½’æ¡£å¤±æ•ˆé“¾æ¥åˆ°old_sources.txtï¼Œä»…ä¿ç•™æœ€æ–°10æ¡"""
    if not invalid_urls:
        return
    
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    new_records = [f"[{current_time}] {url}" for url in invalid_urls]
    
    old_file = Path(CONFIG["OLD_SOURCES_FILE"])
    old_records = []
    if old_file.exists():
        with open(old_file, "r", encoding="utf-8") as f:
            old_records = [line.strip() for line in f.readlines() if line.strip()]
    
    # åˆå¹¶+è§£æ+å»é‡+æ’åº
    all_records = new_records + old_records
    parsed_records = []
    pattern = re.compile(r"\[(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})\] (.*)")
    
    for record in all_records:
        match = pattern.match(record)
        if match:
            try:
                record_time = datetime.strptime(match.group(1), "%Y-%m-%d %H:%M:%S")
                url = match.group(2)
                parsed_records.append((record_time, url))
            except Exception:
                continue
    
    # å»é‡ï¼šåŒä¸€é“¾æ¥ä¿ç•™æœ€æ–°è®°å½•
    unique_dict = {}
    for rt, url in parsed_records:
        if url not in unique_dict or rt > unique_dict[url][0]:
            unique_dict[url] = (rt, url)
    
    # æŒ‰æ—¶é—´é™åºæ’åºï¼Œä¿ç•™å‰10æ¡
    sorted_records = sorted(unique_dict.values(), key=lambda x: x[0], reverse=True)
    final_records = sorted_records[:CONFIG["MAX_OLD_RECORDS"]]
    
    # å†™å…¥æ–‡ä»¶
    with open(old_file, "w", encoding="utf-8") as f:
        f.write("\n".join([f"[{rt.strftime('%Y-%m-%d %H:%M:%S')}] {url}" for rt, url in final_records]) + "\n")
    
    print(f"\nğŸ“ å¤±æ•ˆé“¾æ¥å½’æ¡£å®Œæˆ | æ–°å¢{len(invalid_urls)}æ¡ | å½’æ¡£æ–‡ä»¶ä¿ç•™{len(final_records)}/{CONFIG['MAX_OLD_RECORDS']}æ¡")

def clean_invalid_sources():
    """æ¸…ç†å¤±æ•ˆé“¾æ¥ï¼ˆåŒæ­¥å¹¶å‘ï¼‰ï¼Œè¿”å›æœ‰æ•ˆé“¾æ¥åˆ—è¡¨"""
    source_file = Path(CONFIG["SOURCE_TXT_FILE"])
    
    if not source_file.exists():
        print(f"âš ï¸  æºæ–‡ä»¶ {source_file.name} ä¸å­˜åœ¨ï¼Œç¨‹åºé€€å‡º")
        return []
    
    # è¯»å–å¹¶é¢„å¤„ç†é“¾æ¥
    with open(source_file, "r", encoding="utf-8") as f:
        raw_urls = [line.strip() for line in f.readlines()]
    original_urls = list(set([url for url in raw_urls if url]))  # å»é‡
    if not original_urls:
        print(f"âš ï¸  æºæ–‡ä»¶ {source_file.name} ä¸­æ— æœ‰æ•ˆé“¾æ¥ï¼Œç¨‹åºé€€å‡º")
        return []
    
    print(f"ğŸ” å¼€å§‹æ£€æŸ¥ {len(original_urls)} ä¸ªIPTVæºé“¾æ¥æœ‰æ•ˆæ€§...")
    
    # åŒæ­¥å¹¶å‘æ£€æŸ¥æœ‰æ•ˆæ€§
    valid_urls = []
    invalid_urls = []
    with ThreadPoolExecutor(max_workers=CONFIG["BASE_MAX_CONCURRENT"] // 2) as executor:
        future_tasks = {executor.submit(check_url_validity, url): url for url in original_urls}
        for future in as_completed(future_tasks):
            url, is_valid = future.result()
            if is_valid:
                valid_urls.append(url)
            else:
                invalid_urls.append(url)
    
    # å†™å›æœ‰æ•ˆé“¾æ¥
    with open(source_file, "w", encoding="utf-8") as f:
        f.write("\n".join(valid_urls))
    print(f"\nğŸ§¹ é“¾æ¥æ¸…ç†å®Œæˆ | åŸå§‹{len(original_urls)}æ¡ | æœ‰æ•ˆ{len(valid_urls)}æ¡ | å¤±æ•ˆ{len(invalid_urls)}æ¡")
    
    # å½’æ¡£å¤±æ•ˆé“¾æ¥
    archive_invalid_urls(invalid_urls)
    
    return valid_urls

def generate_iptv_playlist(fastest_urls):
    """åŸºäºæœ€å¿«çš„6æ¡é“¾æ¥ç”ŸæˆM3U8æ’­æ”¾åˆ—è¡¨"""
    if not fastest_urls:
        print("\nâš ï¸  æ— å¯ç”¨é“¾æ¥ï¼Œæ— æ³•ç”Ÿæˆæ’­æ”¾åˆ—è¡¨")
        return
    
    print(f"\nğŸ“„ å¼€å§‹ç”ŸæˆIPTVæ’­æ”¾åˆ—è¡¨ï¼ˆåŸºäº{len(fastest_urls)}æ¡æœ€å¿«é“¾æ¥ï¼‰...")
    m3u8_content = [
        "#EXTM3U",
        f"# ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"# {CONFIG['IPTV_DISCLAIMER']}",
        "#"
    ]
    
    for i, url in enumerate(fastest_urls, 1):
        m3u8_content.append(f"#EXTINF:-1 group-title=\"IPTVæº\" tvg-name=\"æº{i}\",IPTVæº_{i}")
        m3u8_content.append(url)
    
    with open(CONFIG["OUTPUT_FILE"], "w", encoding="utf-8") as f:
        f.write("\n".join(m3u8_content))
    
    print(f"âœ… æ’­æ”¾åˆ—è¡¨ç”Ÿæˆå®Œæˆ | è·¯å¾„ï¼š{CONFIG['OUTPUT_FILE']}")

# ===============================
# ä¸»æµç¨‹ï¼ˆæ•´åˆåŒæ­¥+å¼‚æ­¥é€»è¾‘ï¼‰
# ===============================
async def main_async():
    """å¼‚æ­¥ä¸»æµç¨‹ï¼šæ¸…ç†â†’åŠ¨æ€å¹¶å‘æµ‹é€Ÿâ†’ç­›é€‰â†’ç”Ÿæˆæ’­æ”¾åˆ—è¡¨"""
    print("="*60)
    print("ğŸ¬ IPTVæºé“¾æ¥çˆ¬è™«å·¥å…·ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆ v2.0ï¼‰")
    print(f"ğŸ•’ è¿è¡Œæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)
    
    try:
        # æ­¥éª¤1ï¼šåŒæ­¥æ¸…ç†å¤±æ•ˆé“¾æ¥ï¼Œè·å–æœ‰æ•ˆé“¾æ¥
        valid_urls = clean_invalid_sources()
        if not valid_urls:
            return
        
        # æ­¥éª¤2ï¼šé«˜æ€§èƒ½å¼‚æ­¥æµ‹é€Ÿ+ç­›é€‰æœ€å¿«6æ¡
        if len(valid_urls) <= CONFIG["MAX_FAST_SOURCES"]:
            print(f"\nâœ… æœ‰æ•ˆé“¾æ¥æ•°({len(valid_urls)})â‰¤{CONFIG['MAX_FAST_SOURCES']}ï¼Œæ— éœ€æµ‹é€Ÿï¼Œç›´æ¥ä½¿ç”¨æ‰€æœ‰æœ‰æ•ˆé“¾æ¥")
            fastest_urls = valid_urls
        else:
            # æ‰§è¡ŒåŠ¨æ€å¹¶å‘æµ‹é€Ÿ
            sorted_speed_results = await dynamic_concurrent_speed_test(valid_urls)
            # é€‰å–å‰6æ¡æœ€å¿«çš„é“¾æ¥
            fastest_urls = [item[0] for item in sorted_speed_results[:CONFIG["MAX_FAST_SOURCES"]]]
            # è¾“å‡ºæ’å
            print(f"\nğŸ† æœ€å¿«{CONFIG['MAX_FAST_SOURCES']}æ¡é“¾æ¥æ’åï¼š")
            for i, (url, rt) in enumerate(sorted_speed_results[:CONFIG["MAX_FAST_SOURCES"]], 1):
                print(f"   {i}. {url} | {rt}ms")
        
        # æ­¥éª¤3ï¼šç”Ÿæˆæ’­æ”¾åˆ—è¡¨
        generate_iptv_playlist(fastest_urls)
        
        print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼")
    
    except KeyboardInterrupt:
        print("\nâš ï¸  ç¨‹åºè¢«ç”¨æˆ·æ‰‹åŠ¨ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™ï¼š{str(e)}")

def main():
    """ç¨‹åºå…¥å£ï¼ˆé€‚é…å¼‚æ­¥é€»è¾‘ï¼‰"""
    # è§£å†³Windowsä¸‹asyncioçš„äº‹ä»¶å¾ªç¯é—®é¢˜
    if sys.platform == "win32":
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main_async())

if __name__ == "__main__":
    import sys
    main()
