import requests
import time
import random
import json
from datetime import datetime, timedelta
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from pathlib import Path
import logging

# -------------------------- å…¨å±€é…ç½®ï¼ˆæ‰€æœ‰é…ç½®ä¿ç•™ï¼Œæ–°å¢æ›´æ–°æ—¶é—´æ ¼å¼é…ç½®ï¼‰ --------------------------
# 1. æ•°æ®æºé…ç½®
IPTV_SOURCE_URLS = [
    "https://raw.githubusercontent.com/kakaxi-1/zubo/refs/heads/main/IPTV.txt",
    "https://raw.githubusercontent.com/kakaxi-1/IPTV/refs/heads/main/ipv4.txt",
    "https://raw.githubusercontent.com/vbskycn/iptv/refs/heads/master/tv/iptv4.m3u",
    "https://raw.githubusercontent.com/8080713/iptv-api666/refs/heads/main/output/result.m3u",
    "https://raw.githubusercontent.com/Guovin/iptv-api/refs/heads/gd/output/result.m3u",
    "https://raw.githubusercontent.com/BurningC4/Chinese-IPTV/master/TV-IPV4.m3u",
    "https://raw.githubusercontent.com/Kimentanm/aptv/master/m3u/iptv.m3u",
    "https://raw.githubusercontent.com/audyfan/tv/refs/heads/main/live.m3u"
]

# 2. éªŒè¯ä¸è¶…æ—¶é…ç½®
TIMEOUT_VERIFY = 3
TIMEOUT_FETCH = 10
MIN_VALID_CHANNELS = 3
MAX_THREADS_VERIFY = 30
MAX_THREADS_FETCH = 5

# 3. è¾“å‡ºä¸å»é‡é…ç½®
OUTPUT_FILE = "iptv_playlist.m3u8"
REMOVE_DUPLICATE_CHANNELS = True
REMOVE_LOCAL_URLS = True

# 4. ç¼“å­˜é…ç½®
CACHE_FILE = "iptv_verified_cache.json"
CACHE_EXPIRE_HOURS = 24

# 5. åçˆ¬ä¸åŸºç¡€é…ç½®
MIN_DELAY = 0.1
MAX_DELAY = 0.5
CHANNEL_SORT_ENABLE = True

# ========== M3U8ç¾åŒ–ä¸“å±é…ç½® ==========
URL_TRUNCATE_LENGTH = 60
GROUP_SEPARATOR = "#" * 40
SHOW_BOTTOM_STAT = True
CHANNEL_NAME_CLEAN = True
SOURCE_NUM_PREFIX = "æº"

# ========== æ™ºèƒ½é€‰æºæ ¸å¿ƒé…ç½® ==========
MAX_SOURCES_PER_CHANNEL = 3  # æ¯ä¸ªé¢‘é“ä¿ç•™æœ€å¿«çš„3ä¸ªæº
SORT_SOURCE_BY_SPEED = True  # æŒ‰é€Ÿåº¦ä»å¿«åˆ°æ…¢æ’åº

# ========== æ–°å¢ï¼šM3U8æ›´æ–°æ—¶åˆ»é…ç½®ï¼ˆå¯è‡ªå®šä¹‰æ ¼å¼ï¼‰ ==========
UPDATE_TIME_FORMAT = "%Y-%m-%d %H:%M:%S"  # æ›´æ–°æ—¶é—´æ˜¾ç¤ºæ ¼å¼ï¼Œå¯æŒ‰éœ€ä¿®æ”¹
# ç¤ºä¾‹æ ¼å¼ï¼š%Y-%m-%d %H:%M â†’ 2026-01-31 20:30ï¼›%Y/%m/%d %H:%M:%S â†’ 2026/01/31 20:30:00

# -------------------------- å…¨å±€åˆå§‹åŒ–ï¼ˆæ—¥å¿—+Session+çº¿ç¨‹å®‰å…¨ï¼‰ --------------------------
def init_logger():
    logger = logging.getLogger("IPTV_Spider")
    logger.setLevel(logging.INFO)
    if logger.handlers:
        logger.handlers.clear()
    fmt = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s", datefmt="%Y-%m-%d %H:%M:%S")
    stream_handler = logging.StreamHandler()
    file_handler = logging.FileHandler("iptv_spider.log", encoding="utf-8", mode="a")
    stream_handler.setFormatter(fmt)
    file_handler.setFormatter(fmt)
    logger.addHandler(stream_handler)
    logger.addHandler(file_handler)
    return logger

logger = init_logger()

# å…¨å±€Sessionï¼ˆæŠ“å–/éªŒè¯åˆ†ç¦»ï¼‰
FETCH_SESSION = requests.Session()
VERIFY_SESSION = requests.Session()
DEFAULT_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Referer": "https://github.com/",
    "Accept": "*/*",
    "Cache-Control": "no-cache"
}
FETCH_SESSION.headers.update(DEFAULT_HEADERS)
VERIFY_SESSION.headers.update(DEFAULT_HEADERS)

# çº¿ç¨‹å®‰å…¨æ•°æ®
channel_sources_map = {}  # æ ¼å¼ï¼š{é¢‘é“å: [(url, è€—æ—¶), ...]}
map_lock = threading.Lock()
verified_urls = set()
url_lock = threading.Lock()
temp_verified_results = threading.local()
temp_verified_results.data = []
# å…¨å±€æ›´æ–°æ—¶é—´ï¼ˆè„šæœ¬è¿è¡Œæ—¶ç”Ÿæˆï¼Œæ‰€æœ‰é¢‘é“ç»Ÿä¸€æ˜¾ç¤ºï¼‰
GLOBAL_UPDATE_TIME = datetime.now().strftime(UPDATE_TIME_FORMAT)

# -------------------------- å·¥å…·å‡½æ•°ï¼ˆå…¨é‡å…¼å®¹+ä¼˜åŒ–ï¼‰ --------------------------
def add_random_delay():
    delay = random.uniform(MIN_DELAY, MAX_DELAY)
    time.sleep(delay)

def filter_invalid_urls(url):
    if not url or not url.startswith(("http://", "https://")):
        return False
    if REMOVE_LOCAL_URLS and any(host in url.lower() for host in ["localhost", "127.0.0.1", "192.168.", "10.", "172."]):
        return False
    return True

def safe_extract_channel_name(line):
    if not line.startswith("#EXTINF:"):
        return None
    patterns = [
        r',\s*([^,]+)\s*$',
        r'tvg-name="([^"]+)"',
        r'title="([^"]+)"',
        r'[^"]+\s+([^,\s]+)$'
    ]
    for pattern in patterns:
        match = re.search(pattern, line, re.IGNORECASE)
        if match:
            channel_name = match.group(1).strip()
            if channel_name and not channel_name.isdigit():
                return channel_name
    return "æœªçŸ¥é¢‘é“"

def clean_channel_name(name):
    if not CHANNEL_NAME_CLEAN or not name:
        return name
    name = re.sub(r'\s+', ' ', name).strip()
    name = re.sub(r'[^\u4e00-\u9fff_a-zA-Z0-9\-\(\)ï¼ˆï¼‰Â·ã€]', '', name)
    name = replace("(", "ï¼ˆ").replace(")", "ï¼‰")
    return name

def truncate_url(url, length=URL_TRUNCATE_LENGTH):
    if len(url) <= length:
        return url
    return url[:length].strip() + "..."

def normalize_channel_name(name):
    """é¢‘é“åå½’ä¸€åŒ–ï¼Œå®ç°æ¨¡ç³Šå»é‡"""
    if not name:
        return "æœªçŸ¥é¢‘é“"
    norm_name = re.sub(r'[^\u4e00-\u9fff0-9a-zA-Z]', '', name).upper()
    # å¤®è§†é¢‘é“å½’ä¸€åŒ–æ˜ å°„
    cctv_map = {
        "å¤®è§†1å¥—": "CCTV1", "ä¸­å¤®1å¥—": "CCTV1", "å¤®è§†2å¥—": "CCTV2", "ä¸­å¤®2å¥—": "CCTV2",
        "å¤®è§†3å¥—": "CCTV3", "ä¸­å¤®3å¥—": "CCTV3", "å¤®è§†4å¥—": "CCTV4", "ä¸­å¤®4å¥—": "CCTV4",
        "å¤®è§†5å¥—": "CCTV5", "ä¸­å¤®5å¥—": "CCTV5", "å¤®è§†6å¥—": "CCTV6", "ä¸­å¤®6å¥—": "CCTV6",
        "å¤®è§†7å¥—": "CCTV7", "ä¸­å¤®7å¥—": "CCTV7", "å¤®è§†8å¥—": "CCTV8", "ä¸­å¤®8å¥—": "CCTV8",
        "å¤®è§†9å¥—": "CCTV9", "ä¸­å¤®9å¥—": "CCTV9", "å¤®è§†10å¥—": "CCTV10", "ä¸­å¤®10å¥—": "CCTV10",
        "å¤®è§†11å¥—": "CCTV11", "ä¸­å¤®11å¥—": "CCTV11", "å¤®è§†12å¥—": "CCTV12", "ä¸­å¤®12å¥—": "CCTV12",
        "å¤®è§†13å¥—": "CCTV13", "ä¸­å¤®13å¥—": "CCTV13", "å¤®è§†14å¥—": "CCTV14", "ä¸­å¤®14å¥—": "CCTV14",
        "å¤®è§†15å¥—": "CCTV15", "ä¸­å¤®15å¥—": "CCTV15", "å¤®è§†16å¥—": "CCTV16", "ä¸­å¤®16å¥—": "CCTV16",
        "å¤®è§†17å¥—": "CCTV17", "ä¸­å¤®17å¥—": "CCTV17"
    }
    for key, val in cctv_map.items():
        if key.upper() in norm_name:
            norm_name = val
            break
    return norm_name if norm_name else "æœªçŸ¥é¢‘é“"

# -------------------------- ç¼“å­˜å‡½æ•°ï¼ˆå¢é‡æ›´æ–°+æ— æ•ˆæ¸…ç†ï¼‰ --------------------------
def load_verified_cache():
    global verified_urls
    try:
        cache_path = Path(CACHE_FILE)
        if not cache_path.exists():
            logger.info(f"æœªæ‰¾åˆ°ç¼“å­˜æ–‡ä»¶ {CACHE_FILE}ï¼Œé¦–æ¬¡è¿è¡Œå°†åˆ›å»º")
            return
        with open(cache_path, "r", encoding="utf-8") as f:
            cache_data = json.load(f)
        cache_time_str = cache_data.get("cache_time", "")
        valid_urls = cache_data.get("verified_urls", [])
        if not cache_time_str or not valid_urls:
            logger.warning("ç¼“å­˜æ–‡ä»¶æ— æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡åŠ è½½")
            return
        try:
            cache_time = datetime.strptime(cache_time_str, '%Y-%m-%d %H:%M:%S')
        except ValueError:
            logger.warning("ç¼“å­˜æ—¶é—´æ ¼å¼é”™è¯¯ï¼Œè·³è¿‡åŠ è½½")
            return
        if datetime.now() > cache_time + timedelta(hours=CACHE_EXPIRE_HOURS):
            logger.warning(f"ç¼“å­˜å·²è¿‡æœŸï¼ˆè¶…è¿‡{CACHE_EXPIRE_HOURS}å°æ—¶ï¼‰ï¼Œè·³è¿‡åŠ è½½")
            return
        valid_urls_filtered = [url for url in valid_urls if filter_invalid_urls(url)]
        verified_urls = set(valid_urls_filtered)
        logger.info(f"æˆåŠŸåŠ è½½æœ¬åœ°ç¼“å­˜ï¼Œå…± {len(verified_urls)} ä¸ªæœ‰æ•ˆå·²éªŒè¯æºï¼ˆç¼“å­˜æ—¶é—´ï¼š{cache_time_str}ï¼‰")
    except json.JSONDecodeError:
        logger.error("ç¼“å­˜æ–‡ä»¶æ ¼å¼æŸåï¼Œæ— æ³•åŠ è½½")
    except Exception as e:
        logger.error(f"åŠ è½½ç¼“å­˜å¤±è´¥ï¼š{str(e)[:50]}", exc_info=False)

def save_verified_cache():
    try:
        cache_path = Path(CACHE_FILE)
        cache_path.parent.mkdir(parents=True, exist_ok=True)
        valid_verified_urls = [url for url in verified_urls if filter_invalid_urls(url)]
        cache_data = {
            "cache_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "verified_urls": valid_verified_urls
        }
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        logger.info(f"æˆåŠŸä¿å­˜ç¼“å­˜åˆ° {CACHE_FILE}ï¼Œå…± {len(valid_verified_urls)} ä¸ªæœ‰æ•ˆæºï¼ˆå·²æ¸…ç†æ— æ•ˆç¼“å­˜ï¼‰")
    except Exception as e:
        logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥ï¼š{str(e)[:50]}", exc_info=False)

# -------------------------- æ•°æ®æºæŠ“å–å‡½æ•°ï¼ˆæµå¼è¯»å–+Sessionå¤ç”¨ï¼‰ --------------------------
def fetch_single_source(url, idx):
    add_random_delay()
    try:
        with FETCH_SESSION.get(url, timeout=TIMEOUT_FETCH, stream=True) as response:
            response.raise_for_status()
            response.encoding = response.apparent_encoding or "utf-8"
            valid_lines = []
            for line in response.iter_lines(decode_unicode=True):
                if not line:
                    continue
                line_strip = line.strip()
                if line_strip and not line_strip.startswith("//"):
                    valid_lines.append(line_strip)
        logger.info(f"æ•°æ®æº {idx+1} æŠ“å–æˆåŠŸï¼Œæœ‰æ•ˆè¡Œ {len(valid_lines)}")
        return True, valid_lines
    except requests.exceptions.Timeout:
        logger.error(f"æ•°æ®æº {idx+1} æŠ“å–è¶…æ—¶ï¼ˆè¶…è¿‡{TIMEOUT_FETCH}ç§’ï¼‰")
    except requests.exceptions.HTTPError as e:
        logger.error(f"æ•°æ®æº {idx+1} HTTPé”™è¯¯ï¼š{str(e)[:50]}")
    except requests.exceptions.ConnectionError:
        logger.error(f"æ•°æ®æº {idx+1} è¿æ¥å¤±è´¥ï¼Œå¯èƒ½ç½‘ç»œ/æºå¤±æ•ˆ")
    except Exception as e:
        logger.error(f"æ•°æ®æº {idx+1} æŠ“å–å¤±è´¥ï¼š{str(e)[:50]}", exc_info=False)
    return False, []

def fetch_raw_iptv_data_parallel(url_list):
    all_lines = []
    valid_source_count = 0
    fetch_threads = min(MAX_THREADS_FETCH, len(url_list))
    logger.info(f"å¼€å§‹å¹¶è¡ŒæŠ“å–æ•°æ®æºï¼Œçº¿ç¨‹æ•°ï¼š{fetch_threads}ï¼Œå¾…æŠ“å–æºæ•°ï¼š{len(url_list)}")
    with ThreadPoolExecutor(max_workers=fetch_threads) as executor:
        future_to_idx = {executor.submit(fetch_single_source, url, idx): idx for idx, url in enumerate(url_list)}
        for future in as_completed(future_to_idx):
            success, lines = future.result()
            if success and lines:
                all_lines.extend(lines)
                valid_source_count += 1
    logger.info(f"å¹¶è¡ŒæŠ“å–å®Œæˆï¼šå°è¯• {len(url_list)} æºï¼Œå¯ç”¨ {valid_source_count} æºï¼Œæ€»æœ‰æ•ˆè¡Œï¼š{len(all_lines)}")
    return all_lines

# -------------------------- æºéªŒè¯å‡½æ•°ï¼ˆç²¾ç»†åŒ–éªŒè¯+è®°å½•å“åº”è€—æ—¶ï¼‰ --------------------------
def verify_single_source(url, channel_name):
    if not filter_invalid_urls(url):
        return None, None, None
    add_random_delay()
    if url in verified_urls:
        return channel_name, url, 0.0  # ç¼“å­˜æºè€—æ—¶æ ‡è®°ä¸º0ï¼ˆæœ€å¿«ï¼‰
    # ç²¾ç»†åŒ–è¶…æ—¶é…ç½®
    connect_timeout = 1
    read_timeout = TIMEOUT_VERIFY - connect_timeout if TIMEOUT_VERIFY - connect_timeout > 0 else 1
    valid_stream_suffix = (".m3u8", ".ts", ".flv", ".rtmp", ".rtsp")
    valid_content_types = ["video/", "application/x-mpegurl", "audio/", "application/octet-stream"]
    
    try:
        # æ ¸å¿ƒï¼šè®°å½•è¯·æ±‚å¼€å§‹æ—¶é—´ï¼Œè®¡ç®—å®é™…å“åº”è€—æ—¶
        start_time = time.time()
        with VERIFY_SESSION.get(
            url,
            timeout=(connect_timeout, read_timeout),
            allow_redirects=True,
            stream=True,
            headers={"Range": "bytes=0-1024"}
        ) as response:
            # è®¡ç®—è€—æ—¶ï¼ˆä¿ç•™3ä½å°æ•°ï¼Œè½¬æ¯«ç§’æ›´ç›´è§‚ï¼‰
            response_time = round((time.time() - start_time) * 1000, 3)
            
            # ä¸‰é‡éªŒè¯ï¼šçŠ¶æ€ç +å“åº”å¤´+æµæ ¼å¼
            if response.status_code not in [200, 206, 301, 302, 307, 308]:
                return None, None, None
            content_type = response.headers.get("Content-Type", "").lower()
            if not any(ct in content_type for ct in valid_content_types):
                return None, None, None
            final_url = response.url.lower()
            if not final_url.endswith(valid_stream_suffix):
                return None, None, None
            # m3u8æ–‡ä»¶å¤´é¢å¤–éªŒè¯
            if final_url.endswith(".m3u8"):
                stream_data = response.content[:1024].decode("utf-8", errors="ignore")
                if "#EXTM3U" not in stream_data:
                    return None, None, None
            
            # éªŒè¯é€šè¿‡ï¼ŒåŠ å…¥ç¼“å­˜
            with url_lock:
                verified_urls.add(url)
            temp_verified_results.data.append((channel_name, url, response_time))
            return channel_name, url, response_time
    except Exception:
        return None, None, None

def get_channel_group(channel_name):
    if not channel_name:
        return "ğŸ¬ å…¶ä»–é¢‘é“"
    cctv_keywords = ["CCTV", "å¤®è§†", "ä¸­å¤®", "å¤®è§†é¢‘", "CCTV-", "ä¸­è§†"]
    if any(keyword in channel_name for keyword in cctv_keywords):
        return "ğŸ“º å¤®è§†é¢‘é“"
    if "å«è§†" in channel_name:
        return "ğŸ“¡ å«è§†é¢‘é“"
    province_city = ["åŒ—äº¬", "ä¸Šæµ·", "å¤©æ´¥", "é‡åº†", "æ²³åŒ—", "å±±è¥¿", "è¾½å®", "å‰æ—", "é»‘é¾™æ±Ÿ",
                     "æ±Ÿè‹", "æµ™æ±Ÿ", "å®‰å¾½", "ç¦å»º", "æ±Ÿè¥¿", "å±±ä¸œ", "æ²³å—", "æ¹–åŒ—", "æ¹–å—",
                     "å¹¿ä¸œ", "å¹¿è¥¿", "æµ·å—", "å››å·", "è´µå·", "äº‘å—", "é™•è¥¿", "ç”˜è‚ƒ", "é’æµ·",
                     "å†…è’™å¤", "å®å¤", "æ–°ç–†", "è¥¿è—", "é¦™æ¸¯", "æ¾³é—¨", "å°æ¹¾",
                     "å¹¿å·", "æ·±åœ³", "æ­å·", "å—äº¬", "æˆéƒ½", "æ­¦æ±‰", "è¥¿å®‰", "éƒ‘å·", "é’å²›"]
    for area in province_city:
        if area in channel_name and "å«è§†" not in channel_name:
            return "ğŸ™ï¸ åœ°æ–¹é¢‘é“"
    return "ğŸ¬ å…¶ä»–é¢‘é“"

# -------------------------- æ ¸å¿ƒï¼šæ™ºèƒ½é€‰æº+ç¾åŒ–M3U8+æ›´æ–°æ—¶åˆ»æ ‡æ³¨ --------------------------
def generate_m3u8_parallel(raw_lines):
    # å¸¦æ›´æ–°æ—¶åˆ»çš„å¤´éƒ¨
    m3u8_header = f"""#EXTM3U x-tvg-url="https://iptv-org.github.io/epg/guides/cn/tv.cctv.com.epg.xml"
# ================================= IPTVç›´æ’­æºä¿¡æ¯ =================================
# ç”Ÿæˆæ—¶é—´    ï¼š{GLOBAL_UPDATE_TIME}
# ç¼“å­˜çŠ¶æ€    ï¼š{"å·²åŠ è½½æœ¬åœ°ç¼“å­˜ï¼ˆæœ‰æ•ˆæœŸ24å°æ—¶ï¼‰" if len(verified_urls) > 0 else "æœªåŠ è½½ç¼“å­˜ï¼ˆé¦–æ¬¡è¿è¡Œ/ç¼“å­˜è¿‡æœŸï¼‰"}
# ç”Ÿæ•ˆé…ç½®    ï¼šé¢‘é“å»é‡={REMOVE_DUPLICATE_CHANNELS} | æœ¬åœ°URLè¿‡æ»¤={REMOVE_LOCAL_URLS} | é¢‘é“æ’åº={CHANNEL_SORT_ENABLE}
# éªŒè¯è§„åˆ™    ï¼šè¶…æ—¶{TIMEOUT_VERIFY}ç§’ | ç²¾ç»†åŒ–éªŒè¯ï¼ˆçŠ¶æ€ç +æµæ ¼å¼+æ–‡ä»¶å¤´ï¼‰
# æ™ºèƒ½é€‰æº    ï¼šæ¯ä¸ªé¢‘é“ä¿ç•™æœ€å¿«{MAX_SOURCES_PER_CHANNEL}ä¸ªæº | æ’åºè§„åˆ™={SORT_SOURCE_BY_SPEED and 'ä»å¿«åˆ°æ…¢' or 'åŸé¡ºåº'}
# æ’­æ”¾å™¨å…¼å®¹  ï¼šæ”¯æŒæ‰€æœ‰æ ‡å‡†M3U8æ’­æ”¾å™¨ï¼ˆKodi/å®Œç¾è§†é¢‘/TVBoxç­‰ï¼‰
# ================================================================================
"""
    valid_lines = [m3u8_header]
    total_valid_source = 0

    # æå–å¾…éªŒè¯ä»»åŠ¡
    task_list = []
    temp_channel = None
    seen_urls = set()
    for line in raw_lines:
        line_strip = line.strip()
        if not line_strip:
            continue
        if line_strip.startswith("#EXTINF:"):
            temp_channel = safe_extract_channel_name(line_strip)
        elif filter_invalid_urls(line_strip) and temp_channel:
            if line_strip not in seen_urls:
                seen_urls.add(line_strip)
                task_list.append((line_strip, temp_channel))
            temp_channel = None
    if not task_list:
        logger.error("æœªæå–åˆ°å¾…éªŒè¯çš„æºï¼Œæ— æ³•ç”ŸæˆM3U8")
        return False
    logger.info(f"å¾…éªŒè¯æºæ€»æ•°ï¼š{len(task_list)}ï¼ˆå·²å»é‡+è¿‡æ»¤æ— æ•ˆURL+å¤ç”¨æœ¬åœ°ç¼“å­˜ï¼‰")

    # åŠ¨æ€çº¿ç¨‹æ± éªŒè¯+æµ‹é€Ÿ
    verify_threads = max(5, min(len(task_list)//2, 50))
    logger.info(f"å¼€å§‹å¹¶è¡ŒéªŒè¯æº+æµ‹é€Ÿï¼ŒåŠ¨æ€çº¿ç¨‹æ•°ï¼š{verify_threads}")
    with ThreadPoolExecutor(max_workers=verify_threads) as executor:
        future_to_task = {executor.submit(verify_single_source, url, chan): (url, chan) for url, chan in task_list}
        success_count = 0
        fail_count = 0
        for future in as_completed(future_to_task):
            chan_name, valid_url, response_time = future.result()
            if chan_name and valid_url:
                success_count += 1
                # å­˜å…¥å¸¦è€—æ—¶çš„æºæ•°æ®
                with map_lock:
                    if chan_name not in channel_sources_map:
                        channel_sources_map[chan_name] = []
                    # å»é‡ç›¸åŒURLï¼ˆé¿å…åŒä¸€é¢‘é“å¤šä¸ªç›¸åŒæºï¼‰
                    if not any(item[0] == valid_url for item in channel_sources_map[chan_name]):
                        channel_sources_map[chan_name].append((valid_url, response_time))
            else:
                fail_count += 1
    logger.info(f"æºéªŒè¯+æµ‹é€Ÿå®Œæˆï¼šæˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {fail_count} ä¸ªï¼ŒéªŒè¯æˆåŠŸç‡ï¼š{success_count/(success_count+fail_count)*100:.1f}%")

    # é¢‘é“æ¨¡ç³Šå»é‡
    if REMOVE_DUPLICATE_CHANNELS:
        dedup_map = {}
        norm_name_map = {}
        for chan_name, sources in channel_sources_map.items():
            if not sources:
                continue
            norm_name = normalize_channel_name(chan_name)
            # ä¿ç•™æºæ•°æœ€å¤šçš„é¢‘é“ï¼ˆä¿è¯å¤‡ç”¨æºå……è¶³ï¼‰
            if norm_name not in norm_name_map or len(sources) > len(norm_name_map[norm_name][1]):
                norm_name_map[norm_name] = (chan_name, sources)
        for norm_name, (orig_name, sources) in norm_name_map.items():
            dedup_map[orig_name] = sources
        channel_sources_map.clear()
        channel_sources_map.update(dedup_map)
        logger.info(f"é¢‘é“æ¨¡ç³Šå»é‡å®Œæˆï¼Œå‰©ä½™æœ‰æ•ˆé¢‘é“ {len(channel_sources_map)} ä¸ª")

    # æ ¸å¿ƒï¼šæ™ºèƒ½é€‰æºé€»è¾‘ï¼ˆæŒ‰é€Ÿåº¦ç­›é€‰+æ’åºï¼‰
    smart_selected_map = {}
    total_selected_source = 0
    for chan_name, sources in channel_sources_map.items():
        if not sources:
            continue
        # 1. æŒ‰å“åº”è€—æ—¶æ’åºï¼ˆ0=ç¼“å­˜æºï¼Œæœ€å¿«æ’æœ€å‰ï¼‰
        if SORT_SOURCE_BY_SPEED:
            sorted_sources = sorted(sources, key=lambda x: x[1])
        else:
            sorted_sources = sources
        # 2. ä¿ç•™æœ€å¿«çš„Nä¸ªæºï¼ˆMAX_SOURCES_PER_CHANNELï¼‰
        selected_sources = sorted_sources[:MAX_SOURCES_PER_CHANNEL]
        smart_selected_map[chan_name] = selected_sources
        total_selected_source += len(selected_sources)
    # æ›¿æ¢ä¸ºæ™ºèƒ½é€‰æºåçš„ç»“æœ
    channel_sources_map = smart_selected_map
    logger.info(f"æ™ºèƒ½é€‰æºå®Œæˆï¼šæ¯ä¸ªé¢‘é“ä¿ç•™æœ€å¿«{MAX_SOURCES_PER_CHANNEL}ä¸ªæºï¼Œæœ€ç»ˆç­›é€‰å‡º {total_selected_source} ä¸ªä¼˜è´¨æº")

    # åˆ†ç»„æ•´ç†
    grouped_channels = {"ğŸ“º å¤®è§†é¢‘é“": [], "ğŸ“¡ å«è§†é¢‘é“": [], "ğŸ™ï¸ åœ°æ–¹é¢‘é“": [], "ğŸ¬ å…¶ä»–é¢‘é“": []}
    for channel_name, sources in channel_sources_map.items():
        if not sources:
            continue
        clean_name = clean_channel_name(channel_name)
        group = get_channel_group(clean_name)
        grouped_channels[group].append((clean_name, sources))

    # ç³»ç»Ÿä¿¡æ¯åˆ†ç»„ï¼ˆæ–°å¢æ›´æ–°æ—¶åˆ»æ ‡æ³¨ï¼‰
    valid_lines.append(f"\n# ğŸ“¢ ç³»ç»Ÿä¿¡æ¯ï¼ˆå…±1é¡¹ï¼‰")
    valid_lines.append(f"#EXTINF:-1 group-title='ğŸ“¢ ç³»ç»Ÿä¿¡æ¯',ç›´æ’­æºç”Ÿæˆç»Ÿè®¡ã€æ›´æ–°äºï¼š{GLOBAL_UPDATE_TIME}ã€‘")
    valid_lines.append(f"# æœ‰æ•ˆé¢‘é“æ•°ï¼š{len(channel_sources_map)} ä¸ª | ä¼˜è´¨æºæ€»æ•°ï¼š{total_selected_source} ä¸ª | éªŒè¯æˆåŠŸç‡ï¼š{success_count/(success_count+fail_count)*100:.1f}%")
    valid_lines.append(f"# æ™ºèƒ½é€‰æºï¼šæ¯ä¸ªé¢‘é“ä¿ç•™æœ€å¿«{MAX_SOURCES_PER_CHANNEL}ä¸ªæºï¼Œæ’­æ”¾å™¨å»ºè®®ä¼˜å…ˆé€‰æ‹©ç¬¬ä¸€ä¸ªæºï¼ˆæœ€å¿«ï¼‰")
    valid_lines.append(f"# æºæ›´æ–°æ—¶åˆ»ï¼š{GLOBAL_UPDATE_TIME} | ç¼“å­˜æºæ ‡è®°ä¸ºã€ç¼“å­˜Â·æœ€å¿«ã€‘")
    valid_lines.append("#")

    # ç¾åŒ–åˆ†ç»„è¾“å‡ºï¼šã€æ ¸å¿ƒã€‘åœ¨æ¯ä¸ªé¢‘é“æ’­æ”¾æ®µæ ‡æ³¨æ›´æ–°æ—¶åˆ»
    for group_name, channels in grouped_channels.items():
        if not channels:
            continue
        if CHANNEL_SORT_ENABLE:
            channels.sort(key=lambda x: x[0])
        valid_lines.append(f"\n{GROUP_SEPARATOR}")
        valid_lines.append(f"# {group_name}ï¼ˆå…±{len(channels)}ä¸ªé¢‘é“ | æºæ›´æ–°äºï¼š{GLOBAL_UPDATE_TIME}ï¼‰")
        valid_lines.append(GROUP_SEPARATOR)
        for channel_name, sources in channels:
            source_count = len(sources)
            # é¢‘é“æ ‡é¢˜æ æ ‡æ³¨æ›´æ–°æ—¶åˆ»
            valid_lines.append(f"\n#EXTINF:-1 group-title='{group_name}',{channel_name}ï¼ˆ{source_count}ä¸ªä¼˜è´¨æºÂ·ä»å¿«åˆ°æ…¢Â·æ›´æ–°äºï¼š{GLOBAL_UPDATE_TIME}ï¼‰")
            # è¾“å‡ºå¸¦è€—æ—¶+æ›´æ–°æ—¶åˆ»çš„æºï¼ˆæ³¨é‡Šæ ‡æ³¨ï¼Œä¸å½±å“æ’­æ”¾ï¼‰
            for idx, (url, response_time) in enumerate(sources, 1):
                trunc_url = truncate_url(url)
                # è€—æ—¶æ ‡æ³¨ï¼šç¼“å­˜æºæ˜¾ç¤ºã€ç¼“å­˜Â·æœ€å¿«ã€‘ï¼Œå…¶ä»–æ˜¾ç¤ºå…·ä½“æ¯«ç§’
                speed_note = "ã€ç¼“å­˜Â·æœ€å¿«ã€‘" if response_time == 0.0 else f"ã€{response_time}msã€‘"
                # æºè¡Œæ³¨é‡Šæ·»åŠ æ›´æ–°æ—¶åˆ»
                valid_lines.append(f"# {SOURCE_NUM_PREFIX}{idx} {speed_note} | æºæ›´æ–°äºï¼š{GLOBAL_UPDATE_TIME}ï¼š{trunc_url}")
                valid_lines.append(url)
                logger.debug(f"ä¼˜è´¨æºï¼š[{group_name}] [{channel_name}] - {SOURCE_NUM_PREFIX}{idx} {speed_note}ï¼š{trunc_url}")

    # åº•éƒ¨æ±‡æ€»ç»Ÿè®¡ï¼ˆå¼ºåŒ–æ›´æ–°æ—¶åˆ»æ˜¾ç¤ºï¼‰
    if SHOW_BOTTOM_STAT and len(channel_sources_map) >= MIN_VALID_CHANNELS:
        valid_lines.append(f"\n{GROUP_SEPARATOR}")
        bottom_stat = f"""# ================================= æ±‡æ€»ç»Ÿè®¡ =================================
# æºæ›´æ–°æ—¶åˆ»  ï¼š{GLOBAL_UPDATE_TIME}
# æ€»æœ‰æ•ˆé¢‘é“  ï¼š{len(channel_sources_map)} ä¸ª
# ä¼˜è´¨æºæ€»æ•°  ï¼š{total_selected_source} ä¸ª
# éªŒè¯æˆåŠŸç‡  ï¼š{success_count/(success_count+fail_count)*100:.1f}%
# åˆ†ç»„æ˜ç»†    ï¼šå¤®è§†é¢‘é“{len(grouped_channels['ğŸ“º å¤®è§†é¢‘é“'])}ä¸ª | å«è§†é¢‘é“{len(grouped_channels['ğŸ“¡ å«è§†é¢‘é“'])}ä¸ª | åœ°æ–¹é¢‘é“{len(grouped_channels['ğŸ™ï¸ åœ°æ–¹é¢‘é“'])}ä¸ª | å…¶ä»–é¢‘é“{len(grouped_channels['ğŸ¬ å…¶ä»–é¢‘é“'])}ä¸ª
# æ™ºèƒ½é€‰æº    ï¼šæ¯ä¸ªé¢‘é“ä¿ç•™æœ€å¿«{MAX_SOURCES_PER_CHANNEL}ä¸ªæºï¼Œç¼“å­˜æºæ ‡è®°ä¸ºã€ç¼“å­˜Â·æœ€å¿«ã€‘
# ç¼“å­˜è¯´æ˜    ï¼šå·²ä¿å­˜{len(verified_urls)}ä¸ªæœ‰æ•ˆæºåˆ°æœ¬åœ°ç¼“å­˜ï¼Œä¸‹æ¬¡è¿è¡Œæ— éœ€é‡å¤éªŒè¯
# æ’­æ”¾å™¨æç¤º  ï¼šæ‰€æœ‰æºå‡æ›´æ–°äº{GLOBAL_UPDATE_TIME}ï¼Œå¡é¡¿è¯·åˆ‡æ¢åç»­å¤‡ç”¨æº
# ================================================================================"""
        valid_lines.append(bottom_stat)

    # å®¹é”™é€»è¾‘
    valid_channel_count = len(channel_sources_map)
    if valid_channel_count < MIN_VALID_CHANNELS:
        logger.warning(f"æœ‰æ•ˆé¢‘é“({valid_channel_count})ä½äºé˜ˆå€¼({MIN_VALID_CHANNELS})ï¼Œç”ŸæˆåŸºç¡€æé†’æ–‡ä»¶")
        output_path = Path(OUTPUT_FILE)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        error_content = f"""#EXTM3U x-tvg-url="https://iptv-org.github.io/epg/guides/cn/tv.cctv.com.epg.xml"
# ================================= IPTVç›´æ’­æºä¿¡æ¯ =================================
# ç”Ÿæˆæ—¶é—´    ï¼š{GLOBAL_UPDATE_TIME}
# ç”ŸæˆçŠ¶æ€    ï¼šæœ‰æ•ˆé¢‘é“æ•°ä¸è¶³ï¼ˆä»…{valid_channel_count}ä¸ªï¼‰ï¼Œå»ºè®®ç¨åé‡è¯•
# é‡è¯•å»ºè®®    ï¼šæ£€æŸ¥ç½‘ç»œ/ç­‰å¾…æ•°æ®æºæ›´æ–°/é™ä½MIN_VALID_CHANNELSé˜ˆå€¼
# éªŒè¯ç»Ÿè®¡    ï¼šå…±éªŒè¯{len(task_list)}ä¸ªæºï¼ŒæˆåŠŸ{success_count}ä¸ªï¼ŒæˆåŠŸç‡{success_count/(success_count+fail_count)*100:.1f}%
# ================================================================================

# ğŸ“¢ ç³»ç»Ÿä¿¡æ¯ï¼ˆå…±1é¡¹ï¼‰
#EXTINF:-1 group-title='ğŸ“¢ ç³»ç»Ÿä¿¡æ¯',ç”Ÿæˆå¤±è´¥æé†’ã€æ›´æ–°äºï¼š{GLOBAL_UPDATE_TIME}ã€‘
# æœ‰æ•ˆé¢‘é“æ•°ä½äºé˜ˆå€¼{MIN_VALID_CHANNELS}ï¼Œè¯·ç¨åé‡æ–°è¿è¡Œè„šæœ¬
#"""
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(error_content)
        return False

    # å†™å…¥æœ€ç»ˆç¾åŒ–ç‰ˆM3U8
    try:
        output_path = Path(OUTPUT_FILE)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            f.write("\n".join(valid_lines))
    except Exception as e:
        logger.error(f"å†™å…¥è¾“å‡ºæ–‡ä»¶å¤±è´¥ï¼š{str(e)[:50]}", exc_info=False)
        return False

    # æœ€ç»ˆç»Ÿè®¡ï¼ˆå«æ›´æ–°æ—¶åˆ»ï¼‰
    logger.info(f"\nğŸ“Š æœ€ç»ˆç”Ÿæˆç»Ÿè®¡ï¼ˆæ™ºèƒ½é€‰æº+æ›´æ–°æ—¶åˆ»ç‰ˆï¼‰ï¼š")
    logger.info(f"   æºæ›´æ–°æ—¶åˆ»ï¼š{GLOBAL_UPDATE_TIME}")
    logger.info(f"   å¾…éªŒè¯æºï¼š{len(task_list)} ä¸ª | éªŒè¯æˆåŠŸï¼š{success_count} ä¸ª | éªŒè¯æˆåŠŸç‡ï¼š{success_count/(success_count+fail_count)*100:.1f}%")
    logger.info(f"   æœ‰æ•ˆé¢‘é“ï¼š{valid_channel_count} ä¸ª | ä¼˜è´¨æºæ€»æ•°ï¼š{total_selected_source} ä¸ª | å¹³å‡æ¯é¢‘é“ï¼š{total_selected_source/valid_channel_count:.1f} ä¸ªæº")
    for group_name, channels in grouped_channels.items():
        if channels:
            logger.info(f"   {group_name}ï¼š{len(channels)} ä¸ªé¢‘é“")
    logger.info(f"âœ… æ›´æ–°æ—¶åˆ»ç‰ˆM3U8ç”Ÿæˆå®Œæˆï¼š{OUTPUT_FILE}ï¼ˆç»å¯¹è·¯å¾„ï¼š{Path(OUTPUT_FILE).absolute()}ï¼‰")
    return True

# -------------------------- ä¸»ç¨‹åºï¼ˆå…¨æµç¨‹æ•´åˆï¼‰ --------------------------
if __name__ == "__main__":
    start_time = time.time()
    logger.info("="*70)
    logger.info("  å¹¶è¡ŒåŒ–IPTVæºæŠ“å–ï¼ˆæ›´æ–°æ—¶åˆ»æ ‡æ³¨+æ™ºèƒ½é€‰æº+ç²¾ç»†åŒ–éªŒè¯ï¼‰FINALç»ˆæç‰ˆ  ")
    logger.info("="*70)
    logger.info(f"è„šæœ¬å¯åŠ¨ï¼ŒM3U8æ›´æ–°æ—¶åˆ»å°†æ ‡è®°ä¸ºï¼š{GLOBAL_UPDATE_TIME}")
    # æ ¸å¿ƒæµç¨‹ï¼šåŠ è½½ç¼“å­˜ â†’ æŠ“å–æ•°æ® â†’ æ™ºèƒ½é€‰æº+ç”ŸæˆM3U8 â†’ ä¿å­˜ç¼“å­˜
    load_verified_cache()
    raw_data = fetch_raw_iptv_data_parallel(IPTV_SOURCE_URLS)
    if raw_data:
        generate_m3u8_parallel(raw_data)
    else:
        logger.error("æœªæŠ“å–åˆ°ä»»ä½•åŸå§‹æ•°æ®ï¼Œæ— æ³•ç”ŸæˆM3U8æ–‡ä»¶")
    save_verified_cache()
    # æ€»è¿è¡Œæ—¶é—´ç»Ÿè®¡
    total_time = time.time() - start_time
    logger.info("="*70)
    logger.info(f"è¿è¡Œå®Œæˆï¼Œæ€»è€—æ—¶ï¼š{total_time:.2f} ç§’ï¼ˆçº¦ {total_time/60:.1f} åˆ†é’Ÿï¼‰")
    logger.info(f"æ—¥å¿—æ–‡ä»¶ï¼šiptv_spider.log | ç¼“å­˜æ–‡ä»¶ï¼š{CACHE_FILE} | è¾“å‡ºæ–‡ä»¶ï¼š{OUTPUT_FILE}")
    logger.info(f"æ ¸å¿ƒæç¤ºï¼šæœ¬æ¬¡ç”Ÿæˆçš„M3U8æ‰€æœ‰æºç»Ÿä¸€æ ‡æ³¨æ›´æ–°æ—¶åˆ» â†’ {GLOBAL_UPDATE_TIME}")
    logger.info(f"ä½¿ç”¨æç¤ºï¼šæ’­æ”¾å™¨å¯¼å…¥åï¼Œå¯åœ¨é¢‘é“æ ‡é¢˜/æ³¨é‡Šä¸­æŸ¥çœ‹æºæ›´æ–°æ—¶é—´ï¼Œä¼˜å…ˆé€‰æ‹©ç¬¬ä¸€ä¸ªæœ€å¿«æº")
    logger.info("="*70)
