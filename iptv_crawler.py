import requests
import time
import random
import json
from datetime import datetime, timedelta
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from pathlib import Path

# -------------------------- å…¨å±€é…ç½®ï¼ˆæ–°å¢ã€ç¾åŒ–ç›¸å…³é…ç½®ã€‘ï¼Œå¯è‡ªå®šä¹‰ï¼‰ --------------------------
# 1. æ•°æ®æºé…ç½®
IPTV_SOURCE_URLS = [
    "https://raw.githubusercontent.com/kakaxi-1/zubo/refs/heads/main/IPTV.txt",
    "https://raw.githubusercontent.com/kakaxi-1/IPTV/refs/heads/main/ipv4.txt",
    "https://raw.githubusercontent.com/vbskycn/iptv/refs/heads/master/tv/iptv4.m3u",
    "https://raw.githubusercontent.com/8080713/iptv-api666/refs/heads/main/output/result.m3u",
    "https://raw.githubusercontent.com/Guovin/iptv-api/refs/heads/gd/output/result.m3u",
    "http://wx.thego.cn/ak.m3u
    "https://raw.githubusercontent.com/BurningC4/Chinese-IPTV/master/TV-IPV4.m3u",
    "https://raw.githubusercontent.com/xzw832/cmys/refs/heads/main/S_CCTV.txt",
    "https://raw.githubusercontent.com/xzw832/cmys/refs/heads/main/S_weishi.txt",
    "https://raw.githubusercontent.com/YueChan/Live/main/APTV.m3u",
    "http://aktv.top/live.m3u",
    "https://raw.githubusercontent.com/Kimentanm/aptv/master/m3u/iptv.m3u",
    "https://raw.githubusercontent.com/audyfan/tv/refs/heads/main/live.m3u"
]

# 2. éªŒè¯ä¸è¶…æ—¶é…ç½®
TIMEOUT_VERIFY = 3
TIMEOUT_FETCH = 10
MIN_VALID_CHANNELS = 3
MAX_THREADS_VERIFY = 30
MAX_THREADS_FETCH = 5

# 3. è¾“å‡ºä¸å»é‡é…ç½®
OUTPUT_FILE = "iptv_playlist.m3u8"
REMOVE_DUPLICATE_CHANNELS = True
REMOVE_LOCAL_URLS = True

# 4. ç¼“å­˜é…ç½®
CACHE_FILE = "iptv_verified_cache.json"
CACHE_EXPIRE_HOURS = 24

# 5. åçˆ¬ä¸åŸºç¡€é…ç½®
MIN_DELAY = 0.1
MAX_DELAY = 0.5
CHANNEL_SORT_ENABLE = True

# ========== æ–°å¢ï¼šM3U8ç¾åŒ–ä¸“å±é…ç½®ï¼ˆå¯æ ¹æ®å–œå¥½ä¿®æ”¹ï¼‰ ==========
URL_TRUNCATE_LENGTH = 60  # URLæˆªæ–­é•¿åº¦ï¼ˆä¿ç•™æ ¸å¿ƒåŸŸå+è·¯å¾„ï¼Œé¿å…è¶…é•¿ï¼‰
GROUP_SEPARATOR = "#" * 40  # åˆ†ç»„é—´åˆ†éš”ç¬¦ï¼ˆè§†è§‰åˆ†éš”ï¼Œä¸å½±å“æ’­æ”¾å™¨ï¼‰
SHOW_BOTTOM_STAT = True  # æ˜¯å¦æ˜¾ç¤ºåº•éƒ¨æ±‡æ€»ç»Ÿè®¡
CHANNEL_NAME_CLEAN = True  # æ˜¯å¦æ¸…ç†é¢‘é“åå¤šä½™ç©ºæ ¼/ç‰¹æ®Šå­—ç¬¦ï¼ˆæ ‡å‡†åŒ–ï¼‰
SOURCE_NUM_PREFIX = "æº"  # å¤šæºç¼–å·å‰ç¼€ï¼ˆå¦‚â€œæº1â€/â€œç¬¬1æºâ€ï¼Œå¯æ”¹ï¼‰

# -------------------------- çº¿ç¨‹å®‰å…¨æ•°æ® --------------------------
channel_sources_map = {}
map_lock = threading.Lock()
verified_urls = set()
url_lock = threading.Lock()

# -------------------------- å·¥å…·å‡½æ•°ï¼ˆæ–°å¢ã€é¢‘é“åæ ‡å‡†åŒ–ã€‘å‡½æ•°ï¼‰ --------------------------
def add_random_delay():
    delay = random.uniform(MIN_DELAY, MAX_DELAY)
    time.sleep(delay)

def filter_invalid_urls(url):
    if not url or not url.startswith(("http://", "https://")):
        return False
    if REMOVE_LOCAL_URLS and any(host in url.lower() for host in ["localhost", "127.0.0.1", "192.168.", "10.", "172."]):
        return False
    return True

def safe_extract_channel_name(line):
    if not line.startswith("#EXTINF:"):
        return None
    patterns = [
        r',\s*([^,]+)\s*$',
        r'tvg-name="([^"]+)"',
        r'title="([^"]+)"',
        r'[^"]+\s+([^,\s]+)$'
    ]
    for pattern in patterns:
        match = re.search(pattern, line, re.IGNORECASE)
        if match:
            channel_name = match.group(1).strip()
            if channel_name and not channel_name.isdigit():
                return channel_name
    return "æœªçŸ¥é¢‘é“"

# ========== æ–°å¢ï¼šé¢‘é“åæ ‡å‡†åŒ–æ¸…ç†ï¼ˆç¾åŒ–æ ¸å¿ƒï¼‰ ==========
def clean_channel_name(name):
    """æ¸…ç†é¢‘é“åå¤šä½™ç©ºæ ¼ã€ç‰¹æ®Šç¬¦å·ï¼Œå®ç°æ ‡å‡†åŒ–"""
    if not CHANNEL_NAME_CLEAN or not name:
        return name
    # è¿‡æ»¤å¤šä½™ç©ºæ ¼ï¼ˆå¤šä¸ªç©ºæ ¼å˜ä¸€ä¸ªï¼‰ã€é¦–å°¾ç©ºæ ¼
    name = re.sub(r'\s+', ' ', name).strip()
    # è¿‡æ»¤æ— ç”¨ç‰¹æ®Šç¬¦å·ï¼ˆä¿ç•™ä¸­æ–‡/è‹±æ–‡/æ•°å­—/å¸¸è§ç¬¦å·ï¼‰
    name = re.sub(r'[^\u4e00-\u9fff_a-zA-Z0-9\-\(\)ï¼ˆï¼‰Â·ã€]', '', name)
    # ç»Ÿä¸€æ‹¬å·æ ¼å¼ï¼ˆè‹±æ–‡()å˜ä¸­æ–‡ï¼ˆï¼‰ï¼‰
    name = name.replace("(", "ï¼ˆ").replace(")", "ï¼‰")
    return name

# ========== æ–°å¢ï¼šURLè§„èŒƒæˆªæ–­ï¼ˆç¾åŒ–æ ¸å¿ƒï¼‰ ==========
def truncate_url(url, length=URL_TRUNCATE_LENGTH):
    """URLæˆªæ–­ï¼Œè¶…é•¿æ—¶æœ«å°¾åŠ ...ï¼Œä¿ç•™æ ¸å¿ƒè¯†åˆ«éƒ¨åˆ†"""
    if len(url) <= length:
        return url
    return url[:length].strip() + "..."

# -------------------------- ç¼“å­˜å‡½æ•° --------------------------
def load_verified_cache():
    global verified_urls
    try:
        cache_path = Path(CACHE_FILE)
        if not cache_path.exists():
            print(f"â„¹ï¸  æœªæ‰¾åˆ°ç¼“å­˜æ–‡ä»¶ {CACHE_FILE}ï¼Œå°†åœ¨è¿è¡Œååˆ›å»º")
            return
        with open(cache_path, "r", encoding="utf-8") as f:
            cache_data = json.load(f)
        cache_time_str = cache_data.get("cache_time", "")
        if not cache_time_str:
            print("âš ï¸  ç¼“å­˜æ–‡ä»¶æ— æœ‰æ•ˆæ—¶é—´æˆ³ï¼Œè·³è¿‡åŠ è½½")
            return
        try:
            cache_time = datetime.strptime(cache_time_str, '%Y-%m-%d %H:%M:%S')
        except ValueError:
            print("âš ï¸  ç¼“å­˜æ—¶é—´æˆ³æ ¼å¼é”™è¯¯ï¼Œè·³è¿‡åŠ è½½")
            return
        expire_time = cache_time + timedelta(hours=CACHE_EXPIRE_HOURS)
        current_time = datetime.now()
        if current_time > expire_time:
            print(f"âš ï¸  ç¼“å­˜å·²è¿‡æœŸï¼ˆè¶…è¿‡{CACHE_EXPIRE_HOURS}å°æ—¶ï¼‰ï¼Œè·³è¿‡åŠ è½½")
            return
        valid_urls = cache_data.get("verified_urls", [])
        verified_urls = set(filter(filter_invalid_urls, valid_urls))
        print(f"âœ… æˆåŠŸåŠ è½½æœ¬åœ°ç¼“å­˜ï¼Œå…± {len(verified_urls)} ä¸ªæœ‰æ•ˆå·²éªŒè¯æºï¼ˆç¼“å­˜æ—¶é—´ï¼š{cache_time_str}ï¼‰")
    except json.JSONDecodeError:
        print(f"âš ï¸  ç¼“å­˜æ–‡ä»¶æ ¼å¼æŸåï¼Œæ— æ³•åŠ è½½")
    except Exception as e:
        print(f"âš ï¸  åŠ è½½ç¼“å­˜å¤±è´¥ï¼š{str(e)[:50]}")

def save_verified_cache():
    try:
        cache_path = Path(CACHE_FILE)
        cache_path.parent.mkdir(parents=True, exist_ok=True)
        cache_data = {
            "cache_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "verified_urls": list(verified_urls)
        }
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… æˆåŠŸä¿å­˜ç¼“å­˜åˆ° {CACHE_FILE}ï¼Œå…± {len(verified_urls)} ä¸ªå·²éªŒè¯æº")
    except Exception as e:
        print(f"âŒ ä¿å­˜ç¼“å­˜å¤±è´¥ï¼š{str(e)[:50]}")

# -------------------------- æ•°æ®æºæŠ“å–å‡½æ•° --------------------------
def fetch_single_source(url, idx):
    add_random_delay()
    try:
        response = requests.get(
            url,
            timeout=TIMEOUT_FETCH,
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Referer": "https://github.com/",
                "Accept": "*/*"
            },
            stream=False
        )
        response.raise_for_status()
        response.encoding = response.apparent_encoding or "utf-8"
        lines = response.text.splitlines()
        valid_lines = []
        for line in lines:
            line_strip = line.strip()
            if line_strip and not line_strip.startswith("//"):
                valid_lines.append(line_strip)
        print(f"âœ… æ•°æ®æº {idx+1} æŠ“å–æˆåŠŸï¼Œæœ‰æ•ˆè¡Œ {len(valid_lines)}")
        return True, valid_lines
    except requests.exceptions.Timeout:
        print(f"âŒ æ•°æ®æº {idx+1} æŠ“å–è¶…æ—¶ï¼ˆè¶…è¿‡{TIMEOUT_FETCH}ç§’ï¼‰")
    except requests.exceptions.HTTPError as e:
        print(f"âŒ æ•°æ®æº {idx+1} HTTPé”™è¯¯ï¼š{str(e)[:50]}")
    except requests.exceptions.ConnectionError:
        print(f"âŒ æ•°æ®æº {idx+1} è¿æ¥å¤±è´¥")
    except Exception as e:
        print(f"âŒ æ•°æ®æº {idx+1} æŠ“å–å¤±è´¥ï¼š{str(e)[:50]}")
    return False, []

def fetch_raw_iptv_data_parallel(url_list):
    all_lines = []
    valid_source_count = 0
    fetch_threads = min(MAX_THREADS_FETCH, len(url_list))
    with ThreadPoolExecutor(max_workers=fetch_threads) as executor:
        future_to_idx = {executor.submit(fetch_single_source, url, idx): idx for idx, url in enumerate(url_list)}
        for future in as_completed(future_to_idx):
            success, lines = future.result()
            if success and lines:
                all_lines.extend(lines)
                valid_source_count += 1
    print(f"\nğŸ“Š å¹¶è¡ŒæŠ“å–å®Œæˆï¼šå°è¯• {len(url_list)} æºï¼Œå¯ç”¨ {valid_source_count} æº")
    return all_lines

# -------------------------- æºéªŒè¯å‡½æ•° --------------------------
def verify_single_source(url, channel_name):
    if not filter_invalid_urls(url):
        return None, None
    add_random_delay()
    if url in verified_urls:
        return channel_name, url
    try:
        with requests.get(
            url,
            timeout=TIMEOUT_VERIFY,
            allow_redirects=True,
            headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36"},
            stream=True
        ) as response:
            valid_status_codes = [200, 206, 301, 302, 307, 308]
            if response.status_code in valid_status_codes:
                with url_lock:
                    verified_urls.add(url)
                return channel_name, url
    except:
        pass
    return None, None

def get_channel_group(channel_name):
    if not channel_name:
        return "ğŸ¬ å…¶ä»–é¢‘é“"
    cctv_keywords = ["CCTV", "å¤®è§†", "ä¸­å¤®", "å¤®è§†é¢‘", "CCTV-", "ä¸­è§†"]
    if any(keyword in channel_name for keyword in cctv_keywords):
        return "ğŸ“º å¤®è§†é¢‘é“"
    if "å«è§†" in channel_name:
        return "ğŸ“¡ å«è§†é¢‘é“"
    province_city = ["åŒ—äº¬", "ä¸Šæµ·", "å¤©æ´¥", "é‡åº†", "æ²³åŒ—", "å±±è¥¿", "è¾½å®", "å‰æ—", "é»‘é¾™æ±Ÿ",
                     "æ±Ÿè‹", "æµ™æ±Ÿ", "å®‰å¾½", "ç¦å»º", "æ±Ÿè¥¿", "å±±ä¸œ", "æ²³å—", "æ¹–åŒ—", "æ¹–å—",
                     "å¹¿ä¸œ", "å¹¿è¥¿", "æµ·å—", "å››å·", "è´µå·", "äº‘å—", "é™•è¥¿", "ç”˜è‚ƒ", "é’æµ·",
                     "å†…è’™å¤", "å®å¤", "æ–°ç–†", "è¥¿è—", "é¦™æ¸¯", "æ¾³é—¨", "å°æ¹¾",
                     "å¹¿å·", "æ·±åœ³", "æ­å·", "å—äº¬", "æˆéƒ½", "æ­¦æ±‰", "è¥¿å®‰", "éƒ‘å·", "é’å²›"]
    for area in province_city:
        if area in channel_name and "å«è§†" not in channel_name:
            return "ğŸ™ï¸ åœ°æ–¹é¢‘é“"
    return "ğŸ¬ å…¶ä»–é¢‘é“"

# -------------------------- æ ¸å¿ƒï¼šç¾åŒ–åçš„M3U8ç”Ÿæˆå‡½æ•° --------------------------
def generate_m3u8_parallel(raw_lines):
    update_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    # ========== ç¾åŒ–1ï¼šé¡¶éƒ¨å…ƒä¿¡æ¯å¢å¼ºï¼ˆè¯¦ç»†ã€ç»“æ„åŒ–ï¼Œä¸å½±å“æ’­æ”¾å™¨ï¼‰ ==========
    m3u8_header = f"""#EXTM3U x-tvg-url="https://iptv-org.github.io/epg/guides/cn/tv.cctv.com.epg.xml"
# ================================= IPTVç›´æ’­æºä¿¡æ¯ =================================
# ç”Ÿæˆæ—¶é—´    ï¼š{update_time}
# ç¼“å­˜çŠ¶æ€    ï¼š{"å·²åŠ è½½æœ¬åœ°ç¼“å­˜ï¼ˆæœ‰æ•ˆæœŸ24å°æ—¶ï¼‰" if len(verified_urls) > 0 else "æœªåŠ è½½ç¼“å­˜ï¼ˆé¦–æ¬¡è¿è¡Œ/ç¼“å­˜è¿‡æœŸï¼‰"}
# ç”Ÿæ•ˆé…ç½®    ï¼šé¢‘é“å»é‡={REMOVE_DUPLICATE_CHANNELS} | æœ¬åœ°URLè¿‡æ»¤={REMOVE_LOCAL_URLS} | é¢‘é“æ’åº={CHANNEL_SORT_ENABLE}
# éªŒè¯è§„åˆ™    ï¼šè¶…æ—¶{TIMEOUT_VERIFY}ç§’ | ä»…ä¿ç•™HTTP/HTTPSæœ‰æ•ˆé“¾æ¥
# æ’­æ”¾å™¨å…¼å®¹  ï¼šæ”¯æŒæ‰€æœ‰æ ‡å‡†M3U8æ’­æ”¾å™¨ï¼ˆKodi/å®Œç¾è§†é¢‘/TVBoxç­‰ï¼‰
# ================================================================================
"""
    valid_lines = [m3u8_header]
    total_valid_source = 0  # ç»Ÿè®¡æ€»æœ‰æ•ˆæºæ•°ï¼ˆç”¨äºåº•éƒ¨æ±‡æ€»ï¼‰

    # æå–å¾…éªŒè¯ä»»åŠ¡
    task_list = []
    temp_channel = None
    seen_urls = set()
    for line in raw_lines:
        line_strip = line.strip()
        if not line_strip:
            continue
        if line_strip.startswith("#EXTINF:"):
            temp_channel = safe_extract_channel_name(line_strip)
        elif filter_invalid_urls(line_strip) and temp_channel:
            if line_strip not in seen_urls:
                seen_urls.add(line_strip)
                task_list.append((line_strip, temp_channel))
            temp_channel = None
    print(f"\nğŸ” å¾…éªŒè¯æºæ€»æ•°ï¼š{len(task_list)}ï¼ˆå·²å»é‡+è¿‡æ»¤æ— æ•ˆURL+å¤ç”¨æœ¬åœ°ç¼“å­˜ï¼‰")

    # å¹¶è¡ŒéªŒè¯æº
    with ThreadPoolExecutor(max_workers=MAX_THREADS_VERIFY) as executor:
        future_to_task = {executor.submit(verify_single_source, url, chan): (url, chan) for url, chan in task_list}
        for future in as_completed(future_to_task):
            chan_name, valid_url = future.result()
            if chan_name and valid_url:
                with map_lock:
                    if chan_name not in channel_sources_map:
                        channel_sources_map[chan_name] = []
                    if valid_url not in channel_sources_map[chan_name]:
                        channel_sources_map[chan_name].append(valid_url)

    # é¢‘é“å»é‡
    if REMOVE_DUPLICATE_CHANNELS:
        dedup_map = {}
        for chan_name, sources in channel_sources_map.items():
            if chan_name not in dedup_map or len(sources) > len(dedup_map[chan_name]):
                dedup_map[chan_name] = sources
        channel_sources_map.clear()
        channel_sources_map.update(dedup_map)
        print(f"\nâœ¨ é¢‘é“å»é‡å®Œæˆï¼Œå‰©ä½™æœ‰æ•ˆé¢‘é“ {len(channel_sources_map)} ä¸ª")

    # åˆ†ç»„æ•´ç†
    grouped_channels = {"ğŸ“º å¤®è§†é¢‘é“": [], "ğŸ“¡ å«è§†é¢‘é“": [], "ğŸ™ï¸ åœ°æ–¹é¢‘é“": [], "ğŸ¬ å…¶ä»–é¢‘é“": []}
    for channel_name, sources in channel_sources_map.items():
        if not sources:
            continue
        clean_name = clean_channel_name(channel_name)  # æ ‡å‡†åŒ–é¢‘é“å
        group = get_channel_group(clean_name)
        grouped_channels[group].append((clean_name, sources))
        total_valid_source += len(sources)  # ç´¯è®¡æ€»æœ‰æ•ˆæºæ•°

    # ========== ç¾åŒ–2ï¼šç³»ç»Ÿä¿¡æ¯ç‹¬ç«‹åˆ†ç»„ï¼ˆé†’ç›®ï¼Œå’Œç›´æ’­é¢‘é“åˆ†éš”ï¼‰ ==========
    valid_lines.append(f"\n# ğŸ“¢ ç³»ç»Ÿä¿¡æ¯ï¼ˆå…±1é¡¹ï¼‰")
    valid_lines.append(f"#EXTINF:-1 group-title='ğŸ“¢ ç³»ç»Ÿä¿¡æ¯',ç›´æ’­æºç”Ÿæˆç»Ÿè®¡")
    valid_lines.append(f"# æœ‰æ•ˆé¢‘é“æ•°ï¼š{len(channel_sources_map)} ä¸ª | æ€»æœ‰æ•ˆæºæ•°ï¼š{total_valid_source} ä¸ª")
    valid_lines.append("#")  # ç©ºè¡Œå ä½ï¼Œä¸å½±å“æ’­æ”¾å™¨

    # ========== ç¾åŒ–3ï¼šåˆ†ç»„å¯è§†åŒ–ï¼ˆåˆ†éš”ç¬¦+æ•°é‡ç»Ÿè®¡+æœ‰åºæ’åºï¼‰ ==========
    for group_name, channels in grouped_channels.items():
        if not channels:
            continue
        # åˆ†ç»„æ’åº
        if CHANNEL_SORT_ENABLE:
            channels.sort(key=lambda x: x[0])
        # åˆ†ç»„å¤´ï¼šåˆ†éš”ç¬¦+åˆ†ç»„å+é¢‘é“æ•°
        valid_lines.append(f"\n{GROUP_SEPARATOR}")
        valid_lines.append(f"# {group_name}ï¼ˆå…±{len(channels)}ä¸ªé¢‘é“ï¼‰")
        valid_lines.append(GROUP_SEPARATOR)
        
        # ========== ç¾åŒ–4ï¼šé¢‘é“æ ‡å‡†åŒ–+å¤šæºæœ‰åºæ ‡æ³¨ï¼ˆæ ¸å¿ƒç¾åŒ–ï¼‰ ==========
        for channel_name, sources in channels:
            source_count = len(sources)
            # é¢‘é“è¡Œï¼šæ ‡å‡†åŒ–åç§°+æºæ•°æ ‡æ³¨ï¼ˆæ’­æ”¾å™¨å¯æ­£å¸¸è¯†åˆ«é¢‘é“åï¼‰
            valid_lines.append(f"\n#EXTINF:-1 group-title='{group_name}',{channel_name}ï¼ˆ{source_count}ä¸ªæœ‰æ•ˆæºï¼‰")
            # å¤šæºï¼šæœ‰åºç¼–å·+URLæˆªæ–­+æ³¨é‡Šæ ‡æ³¨ï¼ˆè§†è§‰æ•´æ´ï¼Œæ–¹ä¾¿è¯†åˆ«ï¼‰
            for idx, url in enumerate(sources, 1):
                trunc_url = truncate_url(url)
                valid_lines.append(f"# {SOURCE_NUM_PREFIX}{idx}ï¼š{trunc_url}")
                valid_lines.append(url)  # åŸå§‹URLï¼ˆæ’­æ”¾å™¨æ ¸å¿ƒè¯†åˆ«ï¼Œå¿…é¡»ä¿ç•™ï¼‰
                print(f"ğŸ“º [{group_name}] [{channel_name}] - {SOURCE_NUM_PREFIX}{idx}ï¼š{trunc_url}")

    # ========== ç¾åŒ–5ï¼šåº•éƒ¨æ±‡æ€»ç»Ÿè®¡ï¼ˆå¯é€‰ï¼Œç»“æ„åŒ–ä¿¡æ¯ï¼‰ã€å·²ä¿®å¤ï¼šä¸‰é‡å¼•å·åŒ…è£¹å¤šè¡Œf-stringã€‘ ==========
    if SHOW_BOTTOM_STAT and len(channel_sources_map) >= MIN_VALID_CHANNELS:
        valid_lines.append(f"\n{GROUP_SEPARATOR}")
        # ä¿®å¤æ ¸å¿ƒï¼šç”¨ä¸‰é‡å¼•å·åŒ…è£¹å¤šè¡Œf-stringï¼Œæ”¯æŒæ¢è¡Œä¸”è¯­æ³•åˆæ³•
        bottom_stat = f"""# ================================= æ±‡æ€»ç»Ÿè®¡ =================================
# ç”Ÿæˆæ—¶é—´    ï¼š{update_time}
# æ€»æœ‰æ•ˆé¢‘é“  ï¼š{len(channel_sources_map)} ä¸ª
# æ€»æœ‰æ•ˆæºæ•°  ï¼š{total_valid_source} ä¸ª
# åˆ†ç»„æ˜ç»†    ï¼šå¤®è§†é¢‘é“{len(grouped_channels['ğŸ“º å¤®è§†é¢‘é“'])}ä¸ª | å«è§†é¢‘é“{len(grouped_channels['ğŸ“¡ å«è§†é¢‘é“'])}ä¸ª | åœ°æ–¹é¢‘é“{len(grouped_channels['ğŸ™ï¸ åœ°æ–¹é¢‘é“'])}ä¸ª | å…¶ä»–é¢‘é“{len(grouped_channels['ğŸ¬ å…¶ä»–é¢‘é“'])}ä¸ª
# ç¼“å­˜è¯´æ˜    ï¼šå·²ä¿å­˜{len(verified_urls)}ä¸ªæœ‰æ•ˆæºåˆ°æœ¬åœ°ç¼“å­˜ï¼Œä¸‹æ¬¡è¿è¡Œæ— éœ€é‡å¤éªŒè¯
# ================================================================================"""
        valid_lines.append(bottom_stat)

    # å®¹é”™é€»è¾‘ï¼ˆä¼˜åŒ–ï¼Œä¿ç•™ç¾åŒ–æ ¼å¼ï¼‰
    valid_channel_count = len(channel_sources_map)
    if valid_channel_count < MIN_VALID_CHANNELS:
        print(f"\nâš ï¸  æœ‰æ•ˆé¢‘é“({valid_channel_count})ä½äºé˜ˆå€¼({MIN_VALID_CHANNELS})ï¼Œç”ŸæˆåŸºç¡€ç¾åŒ–æ–‡ä»¶")
        output_path = Path(OUTPUT_FILE)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        error_content = f"""#EXTM3U x-tvg-url="https://iptv-org.github.io/epg/guides/cn/tv.cctv.com.epg.xml"
# ================================= IPTVç›´æ’­æºä¿¡æ¯ =================================
# ç”Ÿæˆæ—¶é—´    ï¼š{update_time}
# ç”ŸæˆçŠ¶æ€    ï¼šæœ‰æ•ˆé¢‘é“æ•°ä¸è¶³ï¼ˆä»…{valid_channel_count}ä¸ªï¼‰ï¼Œå»ºè®®ç¨åé‡è¯•
# é‡è¯•å»ºè®®    ï¼šæ£€æŸ¥ç½‘ç»œ/ç­‰å¾…æ•°æ®æºæ›´æ–°/é™ä½MIN_VALID_CHANNELSé˜ˆå€¼
# ================================================================================

# ğŸ“¢ ç³»ç»Ÿä¿¡æ¯ï¼ˆå…±1é¡¹ï¼‰
#EXTINF:-1 group-title='ğŸ“¢ ç³»ç»Ÿä¿¡æ¯',ç”Ÿæˆå¤±è´¥æé†’
# æœ‰æ•ˆé¢‘é“æ•°ä½äºé˜ˆå€¼{MIN_VALID_CHANNELS}ï¼Œè¯·ç¨åé‡æ–°è¿è¡Œè„šæœ¬
#"""
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(error_content)
        return False

    # å†™å…¥ç¾åŒ–åçš„M3U8æ–‡ä»¶
    try:
        output_path = Path(OUTPUT_FILE)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            f.write("\n".join(valid_lines))
    except Exception as e:
        print(f"âŒ å†™å…¥è¾“å‡ºæ–‡ä»¶å¤±è´¥ï¼š{str(e)[:50]}")
        return False

    # æœ€ç»ˆç»Ÿè®¡
    print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡ï¼šéªŒè¯ {len(task_list)} æºï¼Œæœ‰æ•ˆ {total_valid_source} æºï¼Œæœ‰æ•ˆé¢‘é“ {valid_channel_count} ä¸ª")
    for group_name, channels in grouped_channels.items():
        print(f"   ğŸ“‹ {group_name}ï¼š{len(channels)} é¢‘é“")
    print(f"âœ… ç¾åŒ–ç‰ˆM3U8ç”Ÿæˆå®Œæˆï¼š{OUTPUT_FILE}ï¼ˆè·¯å¾„ï¼š{Path(OUTPUT_FILE).absolute()}ï¼‰")
    return True

# -------------------------- ä¸»ç¨‹åº --------------------------
if __name__ == "__main__":
    start_time = time.time()
    print("========== å¹¶è¡ŒåŒ–IPTVæºæŠ“å–ï¼ˆç¼“å­˜+åçˆ¬+ç¾åŒ–è¾“å‡ºç‰ˆï¼‰ ==========")
    load_verified_cache()
    raw_data = fetch_raw_iptv_data_parallel(IPTV_SOURCE_URLS)
    if raw_data:
        generate_m3u8_parallel(raw_data)
    else:
        print("\nâŒ æœªæŠ“å–åˆ°ä»»ä½•åŸå§‹æ•°æ®ï¼Œæ— æ³•ç”ŸæˆM3U8æ–‡ä»¶")
    save_verified_cache()
    total_time = time.time() - start_time
    print(f"\nâ±ï¸  æ€»è¿è¡Œæ—¶é—´ï¼š{total_time:.2f} ç§’ï¼ˆçº¦ {total_time/60:.1f} åˆ†é’Ÿï¼‰")
    print("========== æŠ“å–å®Œæˆï¼Œç¼“å­˜å·²ä¿å­˜ï¼Œç¾åŒ–ç‰ˆM3U8ç”ŸæˆæˆåŠŸ ==========")
