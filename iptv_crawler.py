import requests
import time
import random
import json
from datetime import datetime, timedelta
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from pathlib import Path
import logging
import multiprocessing  # ç”¨äºè·å–CPUæ ¸å¿ƒæ•°ï¼ŒåŠ¨æ€è°ƒæ•´çº¿ç¨‹æ± 

# -------------------------- å…¨å±€é…ç½®ï¼ˆå…¨ä¿ç•™+æ–°å¢ç¾åŒ–/æ€§èƒ½é…ç½®ï¼‰ --------------------------
# 1. æ•°æ®æºé…ç½®
IPTV_SOURCE_URLS = [
    "https://raw.githubusercontent.com/kakaxi-1/zubo/refs/heads/main/IPTV.txt",
    "https://raw.githubusercontent.com/kakaxi-1/IPTV/refs/heads/main/ipv4.txt",
    "https://raw.githubusercontent.com/vbskycn/iptv/refs/heads/master/tv/iptv4.m3u",
    "https://raw.githubusercontent.com/8080713/iptv-api666/refs/heads/main/output/result.m3u",
    "https://raw.githubusercontent.com/Guovin/iptv-api/refs/heads/gd/output/result.m3u",
    "https://raw.githubusercontent.com/BurningC4/Chinese-IPTV/master/TV-IPV4.m3u",
    "https://raw.githubusercontent.com/Kimentanm/aptv/master/m3u/iptv.m3u",
    "https://raw.githubusercontent.com/audyfan/tv/refs/heads/main/live.m3u"
]

# 2. éªŒè¯ä¸è¶…æ—¶é…ç½®
TIMEOUT_VERIFY = 3
TIMEOUT_FETCH = 10
MIN_VALID_CHANNELS = 3
MAX_THREADS_VERIFY_BASE = 30  # éªŒè¯çº¿ç¨‹åŸºç¡€å€¼
MAX_THREADS_FETCH_BASE = 5   # æŠ“å–çº¿ç¨‹åŸºç¡€å€¼

# 3. è¾“å‡ºä¸å»é‡é…ç½®
OUTPUT_FILE = "iptv_playlist.m3u8"
REMOVE_DUPLICATE_CHANNELS = True
REMOVE_LOCAL_URLS = True

# 4. ç¼“å­˜é…ç½®
CACHE_FILE = "iptv_verified_cache.json"
CACHE_EXPIRE_HOURS = 24

# 5. åçˆ¬ä¸åŸºç¡€é…ç½®
MIN_DELAY = 0.05  # é™ä½å»¶è¿Ÿï¼Œæå‡æ•ˆç‡ï¼ˆä»é˜²åçˆ¬ï¼‰
MAX_DELAY = 0.2
CHANNEL_SORT_ENABLE = True

# ========== M3U8æè‡´ç¾åŒ–ä¸“å±é…ç½®ï¼ˆé‡ç‚¹ï¼å¯è‡ªå®šä¹‰ï¼‰ ==========
URL_TRUNCATE_DOMAIN = True  # æ™ºèƒ½æˆªæ–­URLï¼ˆä¿ç•™åŸŸåï¼‰ï¼Œè€Œéç®€å•æˆªå–
URL_TRUNCATE_LENGTH = 60    # ç®€å•æˆªå–é•¿åº¦ï¼ˆURL_TRUNCATE_DOMAIN=Falseæ—¶ç”Ÿæ•ˆï¼‰
GROUP_SEPARATOR = "#" * 50  # åˆ†ç»„åˆ†éš”ç¬¦åŠ é•¿ï¼Œè§†è§‰æ›´æ¸…æ™°
SHOW_BOTTOM_STAT = True
CHANNEL_NAME_CLEAN = True
CHANNEL_NAME_STANDARD = True  # é¢‘é“åæ ‡å‡†åŒ–ï¼ˆCCTV1-ç»¼åˆ/æµ™æ±Ÿ-å«è§†ï¼‰
SOURCE_NUM_PREFIX = "ğŸ“¶æº"    # æºå‰ç¼€åŠ å›¾æ ‡ï¼Œæ›´ç›´è§‚
UPDATE_TIME_MARK = "â°"       # æ›´æ–°æ—¶åˆ»é«˜äº®ç¬¦å·
SPEED_LEVEL_MARK = True      # é€Ÿåº¦åˆ†çº§æ ‡æ³¨ï¼ˆæé€Ÿ/å¿«é€Ÿ/æ™®é€šï¼‰

# ========== æ™ºèƒ½é€‰æºæ ¸å¿ƒé…ç½® ==========
MAX_SOURCES_PER_CHANNEL = 3
SORT_SOURCE_BY_SPEED = True

# ========== æ›´æ–°æ—¶åˆ»é…ç½® ==========
UPDATE_TIME_FORMAT = "%Y-%m-%d %H:%M:%S"

# -------------------------- åº•å±‚ä¼˜åŒ–ï¼šæ­£åˆ™é¢„ç¼–è¯‘+å…¨å±€å¸¸é‡ --------------------------
# é¢„ç¼–è¯‘æ‰€æœ‰æ­£åˆ™ï¼Œé¿å…å¾ªç¯å†…é‡å¤ç¼–è¯‘
RE_CHANNEL_NAME = re.compile(r',\s*([^,]+)\s*$', re.IGNORECASE)
RE_TVG_NAME = re.compile(r'tvg-name="([^"]+)"', re.IGNORECASE)
RE_TITLE_NAME = re.compile(r'title="([^"]+)"', re.IGNORECASE)
RE_OTHER_NAME = re.compile(r'[^"]+\s+([^,\s]+)$', re.IGNORECASE)
RE_CLEAN_NAME = re.compile(r'\s+')
RE_INVALID_CHAR = re.compile(r'[^\u4e00-\u9fff_a-zA-Z0-9\-\(\)ï¼ˆï¼‰Â·ã€]')
RE_CCTV_NORMAL = re.compile(r'CCTV(\d+)(\D.*)?')
RE_WEISHI_NORMAL = re.compile(r'(.+)å«è§†(\D.*)?')
RE_URL_DOMAIN = re.compile(r'https?://([^/]+)/?(.*)')  # æå–åŸŸåçš„æ­£åˆ™
# é€Ÿåº¦åˆ†çº§é˜ˆå€¼ï¼ˆæ¯«ç§’ï¼‰
SPEED_LEVEL_1 = 50    # æé€Ÿï¼š<50ms
SPEED_LEVEL_2 = 150   # å¿«é€Ÿï¼š50-150ms
# é€Ÿåº¦æ ‡æ³¨
SPEED_MARK_1 = "âš¡æé€Ÿ"
SPEED_MARK_2 = "ğŸš€å¿«é€Ÿ"
SPEED_MARK_3 = "â–¶æ™®é€š"
SPEED_MARK_CACHE = "ğŸ’¾ç¼“å­˜Â·æé€Ÿ"

# -------------------------- å…¨å±€åˆå§‹åŒ–ï¼ˆæ€§èƒ½ä¼˜åŒ–+çº¿ç¨‹å®‰å…¨ï¼‰ --------------------------
# 1. æ—¥å¿—ç³»ç»Ÿï¼ˆä¼˜åŒ–ï¼šæ—¥å¿—çº§åˆ«å¯é…ï¼Œå‡å°‘å†—ä½™è¾“å‡ºï¼‰
def init_logger():
    logger = logging.getLogger("IPTV_Spider")
    logger.setLevel(logging.INFO)
    if logger.handlers:
        logger.handlers.clear()
    fmt = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s", datefmt="%Y-%m-%d %H:%M:%S")
    # æ§åˆ¶å°å¤„ç†å™¨ï¼ˆç²¾ç®€æ ¼å¼ï¼‰
    stream_handler = logging.StreamHandler()
    stream_handler.setFormatter(fmt)
    # æ–‡ä»¶å¤„ç†å™¨ï¼ˆå®Œæ•´æ ¼å¼ï¼Œä¿å­˜æ—¥å¿—ï¼‰
    file_handler = logging.FileHandler("iptv_spider.log", encoding="utf-8", mode="a")
    file_handler.setFormatter(fmt)
    logger.addHandler(stream_handler)
    logger.addHandler(file_handler)
    return logger

logger = init_logger()

# 2. Sessionè¿æ¥æ± ä¼˜åŒ–ï¼ˆæ ¸å¿ƒæ€§èƒ½æå‡ï¼šè®¾ç½®è¿æ¥æ± å‚æ•°ï¼Œé¿å…TCPè€—å°½ï¼‰
def init_session():
    session = requests.Session()
    # è¿æ¥æ± é…ç½®
    adapter = requests.adapters.HTTPAdapter(
        pool_connections=20,  # è¿æ¥æ± æ•°é‡
        pool_maxsize=50,      # æ¯ä¸ªè¿æ¥æ± æœ€å¤§è¿æ¥æ•°
        max_retries=1         # é‡è¯•1æ¬¡ï¼Œè§£å†³ä¸´æ—¶ç½‘ç»œæ³¢åŠ¨
    )
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    # è¯·æ±‚å¤´
    DEFAULT_HEADERS = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        "Referer": "https://github.com/",
        "Accept": "*/*",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive"  # é•¿è¿æ¥ï¼Œæå‡æ•ˆç‡
    }
    session.headers.update(DEFAULT_HEADERS)
    return session

# æŠ“å–/éªŒè¯åˆ†ç¦»çš„ä¼˜åŒ–Session
FETCH_SESSION = init_session()
VERIFY_SESSION = init_session()

# 3. åŠ¨æ€çº¿ç¨‹æ± ï¼šæ ¹æ®CPUæ ¸å¿ƒæ•°è‡ªåŠ¨è°ƒæ•´ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
CPU_CORES = multiprocessing.cpu_count()
MAX_THREADS_VERIFY = min(MAX_THREADS_VERIFY_BASE, CPU_CORES * 5)  # å¤šæ ¸CPUæå‡çº¿ç¨‹æ•°
MAX_THREADS_FETCH = min(MAX_THREADS_FETCH_BASE, CPU_CORES * 2)
logger.info(f"æ€§èƒ½ä¼˜åŒ–ï¼šæ£€æµ‹åˆ°CPUæ ¸å¿ƒæ•° {CPU_CORES}ï¼Œè‡ªåŠ¨è°ƒæ•´çº¿ç¨‹æ•° â†’ æŠ“å–{MAX_THREADS_FETCH} | éªŒè¯{MAX_THREADS_VERIFY}")

# 4. çº¿ç¨‹å®‰å…¨æ•°æ®ï¼ˆå†…å­˜ä¼˜åŒ–ï¼šç§»é™¤å†—ä½™æœ¬åœ°å­˜å‚¨ï¼Œç®€åŒ–ç»“æ„ï¼‰
channel_sources_map = {}  # {é¢‘é“å: [(url, è€—æ—¶), ...]}
map_lock = threading.Lock()
verified_urls = set()
url_lock = threading.Lock()
# å…¨å±€æ›´æ–°æ—¶é—´ï¼ˆä¸€æ¬¡ç”Ÿæˆï¼Œå…¨å±€å¤ç”¨ï¼‰
GLOBAL_UPDATE_TIME = datetime.now().strftime(UPDATE_TIME_FORMAT)
GLOBAL_UPDATE_TIME_DISPLAY = f"{UPDATE_TIME_MARK}{GLOBAL_UPDATE_TIME}"

# -------------------------- å·¥å…·å‡½æ•°ï¼ˆæ•ˆç‡+ç¾åŒ–åŒä¼˜åŒ–ï¼‰ --------------------------
def add_random_delay():
    """å»¶è¿Ÿä¼˜åŒ–ï¼šæ›´çŸ­çš„éšæœºå»¶è¿Ÿï¼Œå…¼é¡¾åçˆ¬å’Œæ•ˆç‡"""
    time.sleep(random.uniform(MIN_DELAY, MAX_DELAY))

def filter_invalid_urls(url):
    """URLè¿‡æ»¤ï¼šé€»è¾‘å‰ªæï¼Œåˆå¹¶åˆ¤æ–­æ¡ä»¶"""
    if not url or not url.startswith(("http://", "https://")):
        return False
    if REMOVE_LOCAL_URLS:
        local_hosts = ["localhost", "127.0.0.1", "192.168.", "10.", "172.", "169.254."]
        if any(host in url.lower() for host in local_hosts):
            return False
    return True

def safe_extract_channel_name(line):
    """é¢‘é“åæå–ï¼šä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™ï¼Œæå‡æ•ˆç‡"""
    if not line.startswith("#EXTINF:"):
        return None
    for pattern in [RE_CHANNEL_NAME, RE_TVG_NAME, RE_TITLE_NAME, RE_OTHER_NAME]:
        match = pattern.search(line)
        if match:
            channel_name = match.group(1).strip()
            if channel_name and not channel_name.isdigit():
                return channel_name
    return "æœªçŸ¥é¢‘é“"

def clean_channel_name(name):
    """é¢‘é“åæ¸…ç†ï¼šé¢„ç¼–è¯‘æ­£åˆ™ï¼Œæ•ˆç‡æå‡"""
    if not CHANNEL_NAME_CLEAN or not name:
        return name
    name = RE_CLEAN_NAME.sub(' ', name).strip()
    name = RE_INVALID_CHAR.sub('', name)
    name = name.replace("(", "ï¼ˆ").replace(")", "ï¼‰")
    return name

def standard_channel_name(name):
    """é¢‘é“åæ ‡å‡†åŒ–ï¼ˆç¾åŒ–æ ¸å¿ƒï¼šCCTV1-ç»¼åˆ/æµ™æ±Ÿ-å«è§†ï¼‰"""
    if not CHANNEL_NAME_STANDARD or not name:
        return name
    # CCTVé¢‘é“æ ‡å‡†åŒ–
    cctv_match = RE_CCTV_NORMAL.match(name)
    if cctv_match:
        cctv_num = cctv_match.group(1)
        cctv_suffix = cctv_match.group(2) or "ç»¼åˆ"
        return f"CCTV{cctv_num}-{cctv_suffix.strip()}"
    # å«è§†é¢‘é“æ ‡å‡†åŒ–
    weishi_match = RE_WEISHI_NORMAL.match(name)
    if weishi_match:
        ws_area = weishi_match.group(1).strip()
        ws_suffix = weishi_match.group(2) or ""
        return f"{ws_area}-å«è§†{ws_suffix.strip()}"
    # åœ°æ–¹é¢‘é“ä¿ç•™åŸæ ¼å¼
    return name

def get_speed_mark(response_time):
    """é€Ÿåº¦åˆ†çº§æ ‡æ³¨ï¼ˆç¾åŒ–ï¼šæé€Ÿ/å¿«é€Ÿ/æ™®é€šï¼Œæ›¿ä»£çº¯æ•°å­—ï¼‰"""
    if not SPEED_LEVEL_MARK:
        return f"{response_time}ms" if response_time > 0 else "ç¼“å­˜"
    if response_time == 0.0:
        return SPEED_MARK_CACHE
    elif response_time < SPEED_LEVEL_1:
        return f"{SPEED_MARK_1}({response_time}ms)"
    elif response_time < SPEED_LEVEL_2:
        return f"{SPEED_MARK_2}({response_time}ms)"
    else:
        return f"{SPEED_MARK_3}({response_time}ms)"

def smart_truncate_url(url):
    """æ™ºèƒ½URLæˆªæ–­ï¼ˆç¾åŒ–æ ¸å¿ƒï¼šä¿ç•™åŸŸå+å…³é”®è·¯å¾„ï¼Œè€Œéç®€å•æˆªå–ï¼‰"""
    if not url:
        return ""
    if not URL_TRUNCATE_DOMAIN:
        return url[:URL_TRUNCATE_LENGTH] + "..." if len(url) > URL_TRUNCATE_LENGTH else url
    # æ™ºèƒ½æˆªæ–­ï¼šæå–åŸŸå + åæ®µå…³é”®è·¯å¾„
    match = RE_URL_DOMAIN.search(url)
    if not match:
        return url[:URL_TRUNCATE_LENGTH] + "..." if len(url) > URL_TRUNCATE_LENGTH else url
    domain = match.group(1)
    path = match.group(2)
    if len(url) <= URL_TRUNCATE_LENGTH:
        return url
    # åŸŸå+è·¯å¾„å‰Nä½+...ï¼Œä¿è¯å¯è¯»æ€§
    remain_length = URL_TRUNCATE_LENGTH - len(domain) - 3
    path_trunc = path[:remain_length] if remain_length > 0 else ""
    return f"{domain}/{path_trunc}..."

def normalize_channel_name(name):
    """é¢‘é“åå½’ä¸€åŒ–ï¼šé¢„ç¼–è¯‘æ­£åˆ™ï¼Œæ•ˆç‡æå‡"""
    if not name:
        return "æœªçŸ¥é¢‘é“"
    norm_name = RE_INVALID_CHAR.sub('', name).upper()
    # CCTVå½’ä¸€åŒ–æ˜ å°„ï¼ˆç²¾ç®€é€»è¾‘ï¼‰
    cctv_map = {f"å¤®è§†{n}å¥—": f"CCTV{n}" for n in range(1, 18)}
    cctv_map.update({f"ä¸­å¤®{n}å¥—": f"CCTV{n}" for n in range(1, 18)})
    for key, val in cctv_map.items():
        if key in norm_name:
            return val
    return norm_name if norm_name else "æœªçŸ¥é¢‘é“"

# -------------------------- ç¼“å­˜å‡½æ•°ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼šæµå¼è¯»å†™+å¢é‡æ¸…ç†ï¼‰ --------------------------
def load_verified_cache():
    """ç¼“å­˜åŠ è½½ï¼šæµå¼è¯»å†™ï¼Œé¿å…å¤§æ–‡ä»¶å†…å­˜æº¢å‡º"""
    global verified_urls
    try:
        cache_path = Path(CACHE_FILE)
        if not cache_path.exists():
            logger.info(f"æœªæ‰¾åˆ°ç¼“å­˜æ–‡ä»¶ {CACHE_FILE}ï¼Œé¦–æ¬¡è¿è¡Œå°†åˆ›å»º")
            return
        # æµå¼è¯»å–å¤§ç¼“å­˜æ–‡ä»¶
        with open(cache_path, "r", encoding="utf-8", buffering=1024*1024) as f:
            cache_data = json.load(f)
        cache_time_str = cache_data.get("cache_time", "")
        valid_urls = cache_data.get("verified_urls", [])
        if not cache_time_str or not valid_urls:
            logger.warning("ç¼“å­˜æ–‡ä»¶æ— æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡åŠ è½½")
            return
        try:
            cache_time = datetime.strptime(cache_time_str, '%Y-%m-%d %H:%M:%S')
        except ValueError:
            logger.warning("ç¼“å­˜æ—¶é—´æ ¼å¼é”™è¯¯ï¼Œè·³è¿‡åŠ è½½")
            return
        if datetime.now() > cache_time + timedelta(hours=CACHE_EXPIRE_HOURS):
            logger.warning(f"ç¼“å­˜å·²è¿‡æœŸï¼ˆè¶…è¿‡{CACHE_EXPIRE_HOURS}å°æ—¶ï¼‰ï¼Œè·³è¿‡åŠ è½½")
            return
        # å¢é‡è¿‡æ»¤ï¼Œä»…ä¿ç•™æœ‰æ•ˆURL
        valid_urls_filtered = [url for url in valid_urls if filter_invalid_urls(url)]
        verified_urls = set(valid_urls_filtered)
        logger.info(f"æˆåŠŸåŠ è½½æœ¬åœ°ç¼“å­˜ â†’ æœ‰æ•ˆæºæ•°ï¼š{len(verified_urls)} | ç¼“å­˜æ—¶é—´ï¼š{cache_time_str}")
    except json.JSONDecodeError:
        logger.error("ç¼“å­˜æ–‡ä»¶æ ¼å¼æŸåï¼Œæ— æ³•åŠ è½½ï¼Œå°†é‡æ–°åˆ›å»º")
    except Exception as e:
        logger.error(f"åŠ è½½ç¼“å­˜å¤±è´¥ï¼š{str(e)[:60]}", exc_info=False)

def save_verified_cache():
    """ç¼“å­˜ä¿å­˜ï¼šæµå¼å†™å…¥ï¼Œå¢é‡æ¸…ç†ï¼Œå‡å°‘å†…å­˜å ç”¨"""
    try:
        cache_path = Path(CACHE_FILE)
        cache_path.parent.mkdir(parents=True, exist_ok=True)
        # å¢é‡æ¸…ç†ï¼Œä»…ä¿ç•™å½“å‰æœ‰æ•ˆURL
        valid_verified_urls = [url for url in verified_urls if filter_invalid_urls(url)]
        cache_data = {
            "cache_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "verified_urls": valid_verified_urls
        }
        # æµå¼å†™å…¥å¤§ç¼“å­˜æ–‡ä»¶
        with open(cache_path, "w", encoding="utf-8", buffering=1024*1024) as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        logger.info(f"æˆåŠŸä¿å­˜ç¼“å­˜ â†’ {CACHE_FILE} | æœ‰æ•ˆæºæ•°ï¼š{len(valid_verified_urls)}ï¼ˆå·²æ¸…ç†æ— æ•ˆç¼“å­˜ï¼‰")
    except Exception as e:
        logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥ï¼š{str(e)[:60]}", exc_info=False)

# -------------------------- æ•°æ®æºæŠ“å–ï¼ˆæ€§èƒ½ï¼šæµå¼+Sessionæ±  | æ•ˆç‡ï¼šé€»è¾‘å‰ªæï¼‰ --------------------------
def fetch_single_source(url, idx):
    add_random_delay()
    try:
        with FETCH_SESSION.get(url, timeout=TIMEOUT_FETCH, stream=True) as response:
            response.raise_for_status()
            response.encoding = response.apparent_encoding or "utf-8"
            valid_lines = []
            # æµå¼è¯»å–ï¼Œè¾¹è¯»è¾¹å¤„ç†ï¼Œå†…å­˜ä¼˜åŒ–
            for line in response.iter_lines(decode_unicode=True, chunk_size=1024):
                if not line:
                    continue
                line_strip = line.strip()
                if line_strip and not line_strip.startswith(("//", "#EXTM3U")):  # å‰ªæï¼šè·³è¿‡æ— ç”¨å¤´
                    valid_lines.append(line_strip)
        logger.info(f"æ•°æ®æº {idx+1:2d} æŠ“å–æˆåŠŸ â†’ æœ‰æ•ˆè¡Œï¼š{len(valid_lines):,}")
        return True, valid_lines
    except requests.exceptions.Timeout:
        logger.error(f"æ•°æ®æº {idx+1:2d} æŠ“å–å¤±è´¥ â†’ è¶…æ—¶ï¼ˆ>{TIMEOUT_FETCH}ç§’ï¼‰")
    except requests.exceptions.HTTPError as e:
        logger.error(f"æ•°æ®æº {idx+1:2d} æŠ“å–å¤±è´¥ â†’ HTTPé”™è¯¯ï¼š{e.response.status_code}")
    except requests.exceptions.ConnectionError:
        logger.error(f"æ•°æ®æº {idx+1:2d} æŠ“å–å¤±è´¥ â†’ è¿æ¥æ‹’ç»/ç½‘ç»œå¼‚å¸¸")
    except Exception as e:
        logger.error(f"æ•°æ®æº {idx+1:2d} æŠ“å–å¤±è´¥ â†’ æœªçŸ¥é”™è¯¯ï¼š{str(e)[:50]}", exc_info=False)
    return False, []

def fetch_raw_iptv_data_parallel(url_list):
    all_lines = []
    valid_source_count = 0
    logger.info(f"å¼€å§‹å¹¶è¡ŒæŠ“å–æ•°æ®æº â†’ æ€»æºæ•°ï¼š{len(url_list)} | çº¿ç¨‹æ•°ï¼š{MAX_THREADS_FETCH}")
    with ThreadPoolExecutor(max_workers=MAX_THREADS_FETCH) as executor:
        future_to_idx = {executor.submit(fetch_single_source, url, idx): idx for idx, url in enumerate(url_list)}
        for future in as_completed(future_to_idx):
            success, lines = future.result()
            if success and lines:
                all_lines.extend(lines)
                valid_source_count += 1
    logger.info(f"å¹¶è¡ŒæŠ“å–å®Œæˆ â†’ å¯ç”¨æºæ•°ï¼š{valid_source_count}/{len(url_list)} | æ€»æœ‰æ•ˆè¡Œï¼š{len(all_lines):,}")
    return all_lines

# -------------------------- æºéªŒè¯ï¼ˆæ€§èƒ½ï¼šSessionæ± +é€»è¾‘å‰ªæ | æµ‹é€Ÿï¼šç²¾å‡†è®¡æ—¶ï¼‰ --------------------------
def verify_single_source(url, channel_name):
    if not filter_invalid_urls(url):
        return None, None, None
    add_random_delay()
    # ç¼“å­˜å‘½ä¸­ï¼Œç›´æ¥è¿”å›ï¼ˆè€—æ—¶0ï¼‰
    if url in verified_urls:
        return channel_name, url, 0.0
    # ç²¾ç»†åŒ–è¶…æ—¶ï¼šè¿æ¥1sï¼Œè¯»å–å‰©ä½™æ—¶é—´ï¼Œæ€»è¶…æ—¶ä¸å˜
    connect_timeout = 1
    read_timeout = max(1, TIMEOUT_VERIFY - connect_timeout)
    valid_stream_suffix = (".m3u8", ".ts", ".flv", ".rtmp", ".rtsp", ".m4s")
    valid_content_types = ["video/", "application/x-mpegurl", "audio/", "application/octet-stream", "video/mp4"]
    
    try:
        start_time = time.time()
        with VERIFY_SESSION.get(
            url,
            timeout=(connect_timeout, read_timeout),
            allow_redirects=True,
            stream=True,
            headers={"Range": "bytes=0-1024"},  # ä»…è¯»å‰1024å­—èŠ‚ï¼Œæå‡æ•ˆç‡
            verify=False  # è·³è¿‡SSLéªŒè¯ï¼Œè§£å†³éƒ¨åˆ†è¯ä¹¦é—®é¢˜ï¼Œæå‡æ•ˆç‡
        ) as response:
            # ç²¾å‡†è®¡ç®—å“åº”è€—æ—¶ï¼ˆæ¯«ç§’ï¼Œä¿ç•™3ä½ï¼‰
            response_time = round((time.time() - start_time) * 1000, 3)
            # é€»è¾‘å‰ªæï¼šåˆå¹¶çŠ¶æ€ç åˆ¤æ–­
            if response.status_code not in [200, 206, 301, 302, 307, 308]:
                return None, None, None
            # å“åº”å¤´éªŒè¯
            content_type = response.headers.get("Content-Type", "").lower()
            if not any(ct in content_type for ct in valid_content_types):
                return None, None, None
            # æœ€ç»ˆURLæ ¼å¼éªŒè¯
            final_url = response.url.lower()
            if not final_url.endswith(valid_stream_suffix):
                return None, None, None
            # M3U8æ–‡ä»¶å¤´éªŒè¯ï¼ˆä»…å¯¹m3u8æ ¼å¼ï¼‰
            if final_url.endswith(".m3u8"):
                stream_data = response.content[:1024].decode("utf-8", errors="ignore")
                if "#EXTM3U" not in stream_data:
                    return None, None, None
            # éªŒè¯é€šè¿‡ï¼ŒåŠ å…¥ç¼“å­˜ï¼ˆåŠ é”ä¿è¯çº¿ç¨‹å®‰å…¨ï¼‰
            with url_lock:
                verified_urls.add(url)
            return channel_name, url, response_time
    except Exception:
        # é™é»˜å¤±è´¥ï¼Œä¸æ‰“å°å†—ä½™æ—¥å¿—ï¼ˆæå‡æ•ˆç‡ï¼‰
        return None, None, None

def get_channel_group(channel_name):
    """é¢‘é“åˆ†ç»„ï¼šé€»è¾‘å‰ªæï¼Œæå‡æ•ˆç‡"""
    if not channel_name:
        return "ğŸ¬ å…¶ä»–é¢‘é“"
    if any(k in channel_name for k in ["CCTV", "å¤®è§†", "ä¸­å¤®", "å¤®è§†é¢‘"]):
        return "ğŸ“º å¤®è§†é¢‘é“"
    if "å«è§†" in channel_name:
        return "ğŸ“¡ å«è§†é¢‘é“"
    # åœ°æ–¹é¢‘é“å…³é”®è¯ï¼ˆç²¾ç®€ï¼‰
    province_city = ["åŒ—äº¬", "ä¸Šæµ·", "å¤©æ´¥", "é‡åº†", "æ²³åŒ—", "å±±è¥¿", "è¾½å®", "å‰æ—", "é»‘é¾™æ±Ÿ",
                     "æ±Ÿè‹", "æµ™æ±Ÿ", "å®‰å¾½", "ç¦å»º", "æ±Ÿè¥¿", "å±±ä¸œ", "æ²³å—", "æ¹–åŒ—", "æ¹–å—",
                     "å¹¿ä¸œ", "å¹¿è¥¿", "æµ·å—", "å››å·", "è´µå·", "äº‘å—", "é™•è¥¿", "ç”˜è‚ƒ", "é’æµ·",
                     "å†…è’™å¤", "å®å¤", "æ–°ç–†", "è¥¿è—", "é¦™æ¸¯", "æ¾³é—¨", "å°æ¹¾"]
    for area in province_city:
        if area in channel_name and "å«è§†" not in channel_name:
            return "ğŸ™ï¸ åœ°æ–¹é¢‘é“"
    return "ğŸ¬ å…¶ä»–é¢‘é“"

# -------------------------- æ ¸å¿ƒï¼šæ™ºèƒ½é€‰æº+æè‡´ç¾åŒ–M3U8ç”Ÿæˆ --------------------------
def generate_m3u8_parallel(raw_lines):
    # æè‡´ç¾åŒ–ï¼šåˆ†å±‚å¤´éƒ¨ï¼Œä¿¡æ¯æ›´æ¸…æ™°
    m3u8_header = f"""#EXTM3U x-tvg-url="https://iptv-org.github.io/epg/guides/cn/tv.cctv.com.epg.xml"
{GROUP_SEPARATOR}
# ğŸ“º IPTVç›´æ’­æºæ’­æ”¾åˆ—è¡¨ - æ™ºèƒ½é€‰æºç‰ˆ
# {GLOBAL_UPDATE_TIME_DISPLAY} ç”Ÿæˆ | éªŒè¯è¶…æ—¶ï¼š{TIMEOUT_VERIFY}ç§’ | æ¯ä¸ªé¢‘é“ä¿ç•™{MAX_SOURCES_PER_CHANNEL}ä¸ªæœ€å¿«æº
# ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šCPUå¤šæ ¸è‡ªé€‚åº” | Sessionè¿æ¥æ±  | æ­£åˆ™é¢„ç¼–è¯‘ | æµå¼è¯»å†™
# âœ¨ ç¾åŒ–ç‰¹æ€§ï¼šé¢‘é“æ ‡å‡†åŒ– | é€Ÿåº¦åˆ†çº§æ ‡æ³¨ | æ™ºèƒ½URLæˆªæ–­ | ç»“æ„åˆ†å±‚å¯è§†åŒ–
# ğŸ¯ æ’­æ”¾å™¨å…¼å®¹ï¼šKodi/TVBox/å®Œç¾è§†é¢‘/æå…‰TV/å°ç±³ç”µè§†/å½“è´å¸‚åœºæ‰€æœ‰M3U8æ’­æ”¾å™¨
{GROUP_SEPARATOR}
"""
    valid_lines = [m3u8_header]
    total_selected_source = 0

    # æå–å¾…éªŒè¯ä»»åŠ¡ï¼ˆé€»è¾‘å‰ªæï¼Œå‡å°‘å¾ªç¯ï¼‰
    task_list = []
    temp_channel = None
    seen_urls = set()
    for line in raw_lines:
        line_strip = line.strip()
        if not line_strip:
            continue
        if line_strip.startswith("#EXTINF:"):
            temp_channel = safe_extract_channel_name(line_strip)
        elif filter_invalid_urls(line_strip) and temp_channel:
            if line_strip not in seen_urls:
                seen_urls.add(line_strip)
                task_list.append((line_strip, temp_channel))
            temp_channel = None
    if not task_list:
        logger.error("æœªæå–åˆ°å¾…éªŒè¯çš„æºï¼Œæ— æ³•ç”ŸæˆM3U8æ–‡ä»¶")
        return False
    logger.info(f"å¾…éªŒè¯æºæ€»æ•° â†’ {len(task_list):,}ï¼ˆå·²å»é‡+è¿‡æ»¤æ— æ•ˆURL+å¤ç”¨æœ¬åœ°ç¼“å­˜ï¼‰")

    # å¹¶è¡ŒéªŒè¯+æµ‹é€Ÿï¼ˆåŠ¨æ€çº¿ç¨‹æ± ï¼‰
    logger.info(f"å¼€å§‹å¹¶è¡ŒéªŒè¯+æµ‹é€Ÿ â†’ çº¿ç¨‹æ•°ï¼š{MAX_THREADS_VERIFY} | é€Ÿåº¦åˆ†çº§ï¼š{SPEED_MARK_1}/{SPEED_MARK_2}/{SPEED_MARK_3}")
    with ThreadPoolExecutor(max_workers=MAX_THREADS_VERIFY) as executor:
        future_to_task = {executor.submit(verify_single_source, url, chan): (url, chan) for url, chan in task_list}
        success_count = 0
        fail_count = 0
        for future in as_completed(future_to_task):
            chan_name, valid_url, response_time = future.result()
            if chan_name and valid_url and response_time >= 0:
                success_count += 1
                # çº¿ç¨‹å®‰å…¨å­˜å…¥æ•°æ®ï¼Œå»é‡ç›¸åŒURL
                with map_lock:
                    if chan_name not in channel_sources_map:
                        channel_sources_map[chan_name] = []
                    if not any(item[0] == valid_url for item in channel_sources_map[chan_name]):
                        channel_sources_map[chan_name].append((valid_url, response_time))
            else:
                fail_count += 1
    # è®¡ç®—éªŒè¯æˆåŠŸç‡ï¼ˆé¿å…é™¤é›¶ï¼‰
    verify_rate = success_count / (success_count + fail_count) * 100 if (success_count + fail_count) > 0 else 0.0
    logger.info(f"éªŒè¯+æµ‹é€Ÿå®Œæˆ â†’ æˆåŠŸï¼š{success_count:,} | å¤±è´¥ï¼š{fail_count:,} | æˆåŠŸç‡ï¼š{verify_rate:.1f}%")

    # é¢‘é“æ¨¡ç³Šå»é‡ï¼ˆå½’ä¸€åŒ–ï¼‰
    if REMOVE_DUPLICATE_CHANNELS:
        dedup_map = {}
        norm_name_map = {}
        for chan_name, sources in channel_sources_map.items():
            if not sources:
                continue
            norm_name = normalize_channel_name(chan_name)
            # ä¿ç•™æºæ•°æœ€å¤šçš„é¢‘é“ï¼Œä¿è¯å¤‡ç”¨æºå……è¶³
            if norm_name not in norm_name_map or len(sources) > len(norm_name_map[norm_name][1]):
                norm_name_map[norm_name] = (chan_name, sources)
        # è¿˜åŸä¸ºåŸé¢‘é“å
        for norm_name, (orig_name, sources) in norm_name_map.items():
            dedup_map[orig_name] = sources
        channel_sources_map.clear()
        channel_sources_map.update(dedup_map)
        logger.info(f"é¢‘é“æ¨¡ç³Šå»é‡å®Œæˆ â†’ å‰©ä½™æœ‰æ•ˆé¢‘é“ï¼š{len(channel_sources_map):,} ä¸ª")

    # æ™ºèƒ½é€‰æºæ ¸å¿ƒé€»è¾‘ï¼šæŒ‰é€Ÿåº¦æ’åº+ä¿ç•™æœ€å¿«Nä¸ª
    smart_selected_map = {}
    for chan_name, sources in channel_sources_map.items():
        if not sources:
            continue
        # æŒ‰é€Ÿåº¦æ’åºï¼ˆ0msç¼“å­˜æºæœ€å¿«ï¼‰
        if SORT_SOURCE_BY_SPEED:
            sorted_sources = sorted(sources, key=lambda x: x[1])
        else:
            sorted_sources = sources
        # ä¿ç•™æœ€å¿«çš„Nä¸ªæº
        selected_sources = sorted_sources[:MAX_SOURCES_PER_CHANNEL]
        smart_selected_map[chan_name] = selected_sources
        total_selected_source += len(selected_sources)
    channel_sources_map = smart_selected_map
    logger.info(f"æ™ºèƒ½é€‰æºå®Œæˆ â†’ ä¼˜è´¨æºæ€»æ•°ï¼š{total_selected_source:,} | å¹³å‡æ¯é¢‘é“ï¼š{total_selected_source/len(channel_sources_map):.1f} ä¸ª")

    # é¢‘é“åˆ†ç»„+æ ‡å‡†åŒ–ï¼ˆç¾åŒ–æ ¸å¿ƒï¼‰
    grouped_channels = {"ğŸ“º å¤®è§†é¢‘é“": [], "ğŸ“¡ å«è§†é¢‘é“": [], "ğŸ™ï¸ åœ°æ–¹é¢‘é“": [], "ğŸ¬ å…¶ä»–é¢‘é“": []}
    for channel_name, sources in channel_sources_map.items():
        if not sources:
            continue
        # æ¸…ç†+æ ‡å‡†åŒ–é¢‘é“å
        clean_name = clean_channel_name(channel_name)
        std_name = standard_channel_name(clean_name)
        # è·å–åˆ†ç»„
        group = get_channel_group(std_name)
        grouped_channels[group].append((std_name, sources))

    # æè‡´ç¾åŒ–ï¼šç³»ç»Ÿä¿¡æ¯æ®µï¼ˆé«˜äº®æ›´æ–°æ—¶åˆ»ï¼‰
    valid_lines.append(f"\n# ğŸ“¢ ç³»ç»Ÿä¿¡æ¯ | {GLOBAL_UPDATE_TIME_DISPLAY}")
    valid_lines.append(f"#EXTINF:-1 group-title='ğŸ“¢ ç³»ç»Ÿä¿¡æ¯',IPTVæºç»Ÿè®¡ã€{GLOBAL_UPDATE_TIME_DISPLAY}ã€‘")
    valid_lines.append(f"# æœ‰æ•ˆé¢‘é“ï¼š{len(channel_sources_map):,} ä¸ª | ä¼˜è´¨æºï¼š{total_selected_source:,} ä¸ª | éªŒè¯æˆåŠŸç‡ï¼š{verify_rate:.1f}%")
    valid_lines.append(f"# æ™ºèƒ½é€‰æºï¼šæ¯ä¸ªé¢‘é“ä¿ç•™æœ€å¿«{MAX_SOURCES_PER_CHANNEL}ä¸ªæº | é€Ÿåº¦åˆ†çº§ï¼š{SPEED_MARK_1}/{SPEED_MARK_2}/{SPEED_MARK_3}")
    valid_lines.append(f"# ä½¿ç”¨æç¤ºï¼šæ’­æ”¾å™¨ä¼˜å…ˆé€‰æ‹©{SOURCE_NUM_PREFIX}1ï¼ˆæœ€å¿«ï¼‰ï¼Œå¡é¡¿è¯·åˆ‡æ¢åç»­å¤‡ç”¨æº")
    valid_lines.append(f"{GROUP_SEPARATOR}\n")

    # æè‡´ç¾åŒ–ï¼šåˆ†ç»„è¾“å‡ºï¼ˆå›¾æ ‡+ç¼©è¿›+é€Ÿåº¦æ ‡æ³¨+æ›´æ–°æ—¶åˆ»ï¼‰
    for group_name, channels in grouped_channels.items():
        if not channels:
            continue
        # æ’åºé¢‘é“ï¼Œä¿è¯å±•ç¤ºé¡ºåºå›ºå®š
        if CHANNEL_SORT_ENABLE:
            channels.sort(key=lambda x: x[0])
        # åˆ†ç»„æ ‡é¢˜ï¼ˆç¾åŒ–ï¼šé¢‘é“æ•°+æ›´æ–°æ—¶åˆ»ï¼‰
        valid_lines.append(f"# {group_name}ï¼ˆå…±{len(channels)}ä¸ªé¢‘é“ï¼‰| {GLOBAL_UPDATE_TIME_DISPLAY}")
        valid_lines.append(GROUP_SEPARATOR)
        # éå†æ¯ä¸ªé¢‘é“
        for channel_name, sources in channels:
            source_count = len(sources)
            # é¢‘é“æ ‡é¢˜ï¼ˆç¾åŒ–ï¼šæ ‡å‡†åŒ–+æºæ•°+æ›´æ–°æ—¶åˆ»ï¼Œæ’­æ”¾å™¨ç›´æ¥å¯è§ï¼‰
            valid_lines.append(f"\n#EXTINF:-1 group-title='{group_name}',{channel_name}ï¼ˆ{source_count}ä¸ªä¼˜è´¨æºï¼‰{GLOBAL_UPDATE_TIME_DISPLAY}")
            # éå†æ¯ä¸ªæºï¼ˆç¾åŒ–ï¼šé€Ÿåº¦åˆ†çº§+æ™ºèƒ½URLæˆªæ–­+å›¾æ ‡å‰ç¼€ï¼‰
            for idx, (url, response_time) in enumerate(sources, 1):
                speed_mark = get_speed_mark(response_time)
                trunc_url = smart_truncate_url(url)
                # æºæ³¨é‡Šï¼ˆç¾åŒ–ï¼šç¼©è¿›+å›¾æ ‡+é€Ÿåº¦+æ›´æ–°æ—¶åˆ»ï¼‰
                valid_lines.append(f"#  {SOURCE_NUM_PREFIX}{idx} {speed_mark} | {GLOBAL_UPDATE_TIME_DISPLAY}ï¼š{trunc_url}")
                # åŸå§‹URLï¼ˆæ’­æ”¾å™¨å”¯ä¸€è¯†åˆ«ï¼Œä¸ä¿®æ”¹ï¼‰
                valid_lines.append(url)
        # åˆ†ç»„ä¹‹é—´ç©ºè¡Œï¼Œç»“æ„æ›´æ¸…æ™°
        valid_lines.append(f"\n{GROUP_SEPARATOR}\n")

    # æè‡´ç¾åŒ–ï¼šåº•éƒ¨å¯è§†åŒ–ç»Ÿè®¡ï¼ˆå­—ç¬¦ç»Ÿè®¡å›¾+è¯¦ç»†ä¿¡æ¯ï¼‰
    if SHOW_BOTTOM_STAT and len(channel_sources_map) >= MIN_VALID_CHANNELS:
        # å­—ç¬¦ç»Ÿè®¡å›¾ï¼ˆç›´è§‚å±•ç¤ºé¢‘é“åˆ†å¸ƒï¼‰
        stat_chart = ""
        for g_name, g_chans in grouped_channels.items():
            if g_chans:
                chan_num = len(g_chans)
                # æŒ‰æ¯”ä¾‹ç”Ÿæˆç»Ÿè®¡æ¡ï¼ˆæœ€å¤§20ä¸ªâ˜…ï¼‰
                stat_bar = "â˜…" * min(chan_num // 2, 20) if chan_num > 0 else ""
                stat_chart += f"# {g_name}ï¼š{chan_num:2d}ä¸ª {stat_bar}\n"
        # åº•éƒ¨ç»Ÿè®¡å†…å®¹
        bottom_stat = f"""
{GROUP_SEPARATOR}
# ğŸ“Š æ’­æ”¾åˆ—è¡¨æ±‡æ€»ç»Ÿè®¡ | {GLOBAL_UPDATE_TIME_DISPLAY}
{stat_chart}# ğŸ¯ æ ¸å¿ƒæŒ‡æ ‡ï¼šæœ‰æ•ˆé¢‘é“{len(channel_sources_map):,}ä¸ª | ä¼˜è´¨æº{total_selected_source:,}ä¸ª | éªŒè¯æˆåŠŸç‡{verify_rate:.1f}%
# âš¡ æ€§èƒ½æŒ‡æ ‡ï¼šéªŒè¯è¶…æ—¶{TIMEOUT_VERIFY}ç§’ | çº¿ç¨‹æ•°{MAX_THREADS_VERIFY}ï¼ˆCPU{CPU_CORES}æ ¸è‡ªé€‚åº”ï¼‰
# ğŸ’¾ ç¼“å­˜æŒ‡æ ‡ï¼šæœ¬åœ°ç¼“å­˜æœ‰æ•ˆæº{len(verified_urls):,}ä¸ª | ç¼“å­˜æœ‰æ•ˆæœŸ{CACHE_EXPIRE_HOURS}å°æ—¶
# ğŸ“Œ æ’­æ”¾å™¨æç¤ºï¼šæ‰€æœ‰æºå·²æŒ‰é€Ÿåº¦æ’åºï¼Œä¼˜å…ˆé€‰æ‹©{SOURCE_NUM_PREFIX}1ï¼Œå¤±æ•ˆè¯·é‡æ–°è¿è¡Œè„šæœ¬ç”Ÿæˆ
# ğŸ“… æ›´æ–°æç¤ºï¼šå»ºè®®æ¯6-12å°æ—¶é‡æ–°è¿è¡Œè„šæœ¬ï¼Œä¿è¯æºçš„æ–°é²œåº¦å’Œæœ‰æ•ˆæ€§
{GROUP_SEPARATOR}
"""
        valid_lines.append(bottom_stat)

    # å®¹é”™é€»è¾‘ï¼šæœ‰æ•ˆé¢‘é“æ•°ä½äºé˜ˆå€¼ï¼Œç”Ÿæˆæé†’æ–‡ä»¶
    valid_channel_count = len(channel_sources_map)
    if valid_channel_count < MIN_VALID_CHANNELS:
        logger.warning(f"æœ‰æ•ˆé¢‘é“æ•°{valid_channel_count}ä½äºé˜ˆå€¼{MIN_VALID_CHANNELS}ï¼Œç”ŸæˆåŸºç¡€æé†’æ–‡ä»¶")
        error_content = f"""#EXTM3U x-tvg-url="https://iptv-org.github.io/epg/guides/cn/tv.cctv.com.epg.xml"
{GROUP_SEPARATOR}
# âŒ IPTVæ’­æ”¾åˆ—è¡¨ç”Ÿæˆå¤±è´¥ | {GLOBAL_UPDATE_TIME_DISPLAY}
# å¤±è´¥åŸå› ï¼šæœ‰æ•ˆé¢‘é“æ•°ä¸è¶³ï¼ˆä»…{valid_channel_count}ä¸ªï¼‰ï¼Œä½äºé˜ˆå€¼{MIN_VALID_CHANNELS}ä¸ª
# é‡è¯•å»ºè®®ï¼š1. æ£€æŸ¥ç½‘ç»œæ˜¯å¦èƒ½è®¿é—®GitHub 2. å¢å¤§TIMEOUT_VERIFYé˜ˆå€¼ 3. ç¨åé‡æ–°è¿è¡Œè„šæœ¬
# éªŒè¯ç»Ÿè®¡ï¼šå…±éªŒè¯{len(task_list):,}ä¸ªæº | æˆåŠŸ{success_count:,}ä¸ª | æˆåŠŸç‡{verify_rate:.1f}%
{GROUP_SEPARATOR}
#EXTINF:-1 group-title='ğŸ“¢ é”™è¯¯ä¿¡æ¯',ç”Ÿæˆå¤±è´¥ã€{GLOBAL_UPDATE_TIME_DISPLAY}ã€‘
# æœ‰æ•ˆé¢‘é“æ•°ä¸è¶³ï¼Œè¯·æŒ‰ä¸Šè¿°å»ºè®®é‡è¯•ï¼
"""
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            f.write(error_content)
        return False

    # å†™å…¥æè‡´ç¾åŒ–ç‰ˆM3U8æ–‡ä»¶ï¼ˆæµå¼å†™å…¥ï¼Œå†…å­˜ä¼˜åŒ–ï¼‰
    try:
        output_path = Path(OUTPUT_FILE)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w", encoding="utf-8", buffering=1024*1024) as f:
            f.write("\n".join(valid_lines))
    except Exception as e:
        logger.error(f"å†™å…¥M3U8æ–‡ä»¶å¤±è´¥ï¼š{str(e)[:60]}", exc_info=False)
        return False

    # æœ€ç»ˆç»Ÿè®¡ï¼ˆå¯è§†åŒ–ï¼‰
    logger.info(f"\nğŸ“Š æœ€ç»ˆç”Ÿæˆç»Ÿè®¡ {GLOBAL_UPDATE_TIME_DISPLAY}")
    logger.info(f"   ğŸ“º é¢‘é“åˆ†å¸ƒï¼šå¤®è§†é¢‘é“{len(grouped_channels['ğŸ“º å¤®è§†é¢‘é“'])} | å«è§†é¢‘é“{len(grouped_channels['ğŸ“¡ å«è§†é¢‘é“'])}")
    logger.info(f"   ğŸ™ï¸ åœ°æ–¹é¢‘é“{len(grouped_channels['ğŸ™ï¸ åœ°æ–¹é¢‘é“'])} | å…¶ä»–é¢‘é“{len(grouped_channels['ğŸ¬ å…¶ä»–é¢‘é“'])}")
    logger.info(f"   ğŸ¯ æ ¸å¿ƒæŒ‡æ ‡ï¼šæœ‰æ•ˆé¢‘é“{valid_channel_count:,}ä¸ª | ä¼˜è´¨æº{total_selected_source:,}ä¸ª | éªŒè¯æˆåŠŸç‡{verify_rate:.1f}%")
    logger.info(f"   ğŸ’¾ ç¼“å­˜æŒ‡æ ‡ï¼šæœ¬åœ°ç¼“å­˜{len(verified_urls):,}ä¸ªæœ‰æ•ˆæº | ä¸‹æ¬¡è¿è¡Œå…é‡å¤éªŒè¯")
    logger.info(f"   âœ… æè‡´ç¾åŒ–ç‰ˆM3U8ç”Ÿæˆå®Œæˆ â†’ {OUTPUT_FILE}ï¼ˆç»å¯¹è·¯å¾„ï¼š{output_path.absolute()}ï¼‰")
    return True

# -------------------------- ä¸»ç¨‹åºï¼ˆå…¨æµç¨‹æ•´åˆ+æ€§èƒ½ç»Ÿè®¡ï¼‰ --------------------------
if __name__ == "__main__":
    # ç¨‹åºå¯åŠ¨æ—¶é—´
    start_time = time.time()
    logger.info("="*80)
    logger.info("    IPTVç›´æ’­æºæŠ“å–å·¥å…· - æè‡´æ€§èƒ½+æè‡´ç¾åŒ–ç‰ˆ V3.0    ")
    logger.info("="*80)
    logger.info(f"ç¨‹åºå¯åŠ¨ â†’ {GLOBAL_UPDATE_TIME_DISPLAY} | CPUæ ¸å¿ƒæ•°ï¼š{CPU_CORES} | Pythonç‰ˆæœ¬ï¼š{requests.__version__}")
    logger.info("="*80)

    # æ ¸å¿ƒæµç¨‹ï¼šåŠ è½½ç¼“å­˜ â†’ æŠ“å–æ•°æ® â†’ æ™ºèƒ½é€‰æº+ç¾åŒ–ç”Ÿæˆ â†’ ä¿å­˜ç¼“å­˜
    load_verified_cache()
    raw_data = fetch_raw_iptv_data_parallel(IPTV_SOURCE_URLS)
    if raw_data:
        generate_m3u8_parallel(raw_data)
    else:
        logger.error("æœªæŠ“å–åˆ°ä»»ä½•åŸå§‹æ•°æ®ï¼Œç¨‹åºç»ˆæ­¢")
    save_verified_cache()

    # ç¨‹åºè¿è¡Œæ€»è€—æ—¶
    total_time = time.time() - start_time
    minutes = int(total_time // 60)
    seconds = round(total_time % 60, 2)
    logger.info("="*80)
    logger.info(f"ç¨‹åºè¿è¡Œå®Œæˆ â†’ æ€»è€—æ—¶ï¼š{minutes}åˆ†{seconds}ç§’ï¼ˆ{total_time:.2f}ç§’ï¼‰")
    logger.info(f"ç”Ÿæˆæ–‡ä»¶ â†’ æ’­æ”¾åˆ—è¡¨ï¼š{OUTPUT_FILE} | è¿è¡Œæ—¥å¿—ï¼šiptv_spider.log | éªŒè¯ç¼“å­˜ï¼š{CACHE_FILE}")
    logger.info(f"ä½¿ç”¨æç¤º â†’ æ’­æ”¾å™¨ç›´æ¥å¯¼å…¥{OUTPUT_FILE}ï¼Œä¼˜å…ˆé€‰æ‹©{SOURCE_NUM_PREFIX}1ï¼ˆæœ€å¿«ï¼‰ï¼Œå¡é¡¿åˆ‡æ¢åç»­æº")
    logger.info(f"æ›´æ–°æç¤º â†’ å»ºè®®é€šè¿‡è®¡åˆ’ä»»åŠ¡/crontabæ¯6-12å°æ—¶è‡ªåŠ¨è¿è¡Œï¼Œä¿è¯æºæ–°é²œ")
    logger.info("="*80)
