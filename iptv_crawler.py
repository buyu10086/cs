import requests
import time
import random
import json
from datetime import datetime, timedelta, timezone
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# -------------------------- é…ç½®ä¼˜åŒ–ï¼ˆæ–°å¢ç¼“å­˜+åçˆ¬é…ç½®ï¼‰ --------------------------
IPTV_SOURCE_URLS = [
    "https://raw.githubusercontent.com/TianmuTNT/iptv/refs/heads/main/iptv.m3u",
    "https://raw.githubusercontent.com/kakaxi-1/zubo/refs/heads/main/IPTV.txt",
    "https://raw.githubusercontent.com/vbskycn/iptv/refs/heads/master/tv/iptv4.m3u",
    "https://raw.githubusercontent.com/mytv-android/China-TV-Live-M3U8/refs/heads/main/webview.m3u"
]
# æ ¸å¿ƒä¼˜åŒ–ï¼šç¼©çŸ­éªŒè¯è¶…æ—¶æ—¶é—´
TIMEOUT = 3
OUTPUT_FILE = "iptv_playlist.m3u8"
REMOVE_DUPLICATE_CHANNELS = True
MIN_VALID_SOURCES = 3
# å¹¶è¡Œçº¿ç¨‹æ•°
MAX_THREADS = 30

# æ–°å¢ä¼˜åŒ–1ï¼šç¼“å­˜é…ç½®
CACHE_FILE = "iptv_verified_cache.json"
# ç¼“å­˜æœ‰æ•ˆæœŸï¼š24å°æ—¶ï¼ˆé¿å…æ—§ç¼“å­˜å¤±æ•ˆï¼Œå¯è°ƒæ•´ï¼‰
CACHE_EXPIRE_HOURS = 24

# æ–°å¢ä¼˜åŒ–2ï¼šåçˆ¬éšæœºå»¶è¿Ÿé…ç½®ï¼ˆ0.1-0.5ç§’ï¼Œä¸å½±å“å¹¶è¡Œæ•ˆç‡ï¼‰
MIN_DELAY = 0.1
MAX_DELAY = 0.5

# çº¿ç¨‹å®‰å…¨ï¼šé”ä¿æŠ¤å…±äº«æ•°æ®
channel_sources_map = {}
map_lock = threading.Lock()
verified_urls = set()
url_lock = threading.Lock()

# -------------------------- æ–°å¢ï¼šç¼“å­˜ç›¸å…³æ ¸å¿ƒå‡½æ•° --------------------------
def load_verified_cache():
    """åŠ è½½æœ¬åœ°å·²éªŒè¯æºçš„ç¼“å­˜ï¼ˆå¸¦è¿‡æœŸåˆ¤æ–­ï¼‰"""
    global verified_urls
    try:
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            cache_data = json.load(f)
        
        # éªŒè¯ç¼“å­˜æ˜¯å¦è¿‡æœŸ
        cache_time_str = cache_data.get("cache_time", "")
        if not cache_time_str:
            print("âš ï¸  ç¼“å­˜æ–‡ä»¶æ— æ—¶é—´æˆ³ï¼Œè·³è¿‡åŠ è½½")
            return
        
        cache_time = datetime.strptime(cache_time_str, '%Y-%m-%d %H:%M:%S')
        expire_time = cache_time + timedelta(hours=CACHE_EXPIRE_HOURS)
        current_time = datetime.now()
        
        if current_time > expire_time:
            print(f"âš ï¸  ç¼“å­˜å·²è¿‡æœŸï¼ˆè¶…è¿‡{CACHE_EXPIRE_HOURS}å°æ—¶ï¼‰ï¼Œè·³è¿‡åŠ è½½")
            return
        
        # åŠ è½½æœ‰æ•ˆç¼“å­˜
        valid_urls = cache_data.get("verified_urls", [])
        verified_urls = set(valid_urls)
        print(f"âœ… æˆåŠŸåŠ è½½æœ¬åœ°ç¼“å­˜ï¼Œå…± {len(verified_urls)} ä¸ªå·²éªŒè¯æºï¼ˆç¼“å­˜æ—¶é—´ï¼š{cache_time_str}ï¼‰")
    
    except FileNotFoundError:
        print(f"â„¹ï¸  æœªæ‰¾åˆ°ç¼“å­˜æ–‡ä»¶ {CACHE_FILE}ï¼Œå°†åœ¨è¿è¡Œååˆ›å»º")
    except json.JSONDecodeError:
        print(f"âš ï¸  ç¼“å­˜æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œæ— æ³•åŠ è½½")
    except Exception as e:
        print(f"âš ï¸  åŠ è½½ç¼“å­˜å¤±è´¥ï¼š{str(e)[:50]}")

def save_verified_cache():
    """ä¿å­˜å½“å‰å·²éªŒè¯æºåˆ°æœ¬åœ°ç¼“å­˜"""
    try:
        # ç¼“å­˜æ—¶é—´ä¹Ÿä½¿ç”¨åŒ—äº¬æ—¶åŒº
        tz_beijing = timezone(timedelta(hours=8))
        cache_time = datetime.now(tz_beijing).strftime('%Y-%m-%d %H:%M:%S')
        cache_data = {
            "cache_time": cache_time,
            "verified_urls": list(verified_urls)
        }
        
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æˆåŠŸä¿å­˜ç¼“å­˜åˆ° {CACHE_FILE}ï¼Œå…± {len(verified_urls)} ä¸ªå·²éªŒè¯æº")
    except Exception as e:
        print(f"âŒ ä¿å­˜ç¼“å­˜å¤±è´¥ï¼š{str(e)[:50]}")

# -------------------------- æ–°å¢ï¼šåçˆ¬å»¶è¿Ÿå·¥å…·å‡½æ•° --------------------------
def add_random_delay():
    """æ·»åŠ éšæœºåçˆ¬å»¶è¿Ÿï¼ˆä¸å½±å“æ•´ä½“å¹¶è¡Œæ•ˆç‡ï¼‰"""
    delay = random.uniform(MIN_DELAY, MAX_DELAY)
    time.sleep(delay)

# -------------------------- å¹¶è¡ŒåŒ–æ ¸å¿ƒå‡½æ•°ï¼ˆæ•´åˆç¼“å­˜+åçˆ¬å»¶è¿Ÿï¼‰ --------------------------
def fetch_single_source(url, idx):
    """å¹¶è¡ŒæŠ“å–å•ä¸ªæ•°æ®æºï¼Œè¿”å›(æ˜¯å¦æˆåŠŸ, æœ‰æ•ˆè¡Œåˆ—è¡¨)ï¼ˆæ–°å¢åçˆ¬å»¶è¿Ÿï¼‰"""
    # æ–°å¢ï¼šè¯·æ±‚å‰æ·»åŠ éšæœºå»¶è¿Ÿï¼Œé¿å…åçˆ¬
    add_random_delay()
    
    try:
        response = requests.get(
            url,
            timeout=10,  # æ•°æ®æºæŠ“å–è¶…æ—¶å¯ç¨é•¿
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Referer": "https://github.com/",
                "Accept": "*/*"
            }
        )
        response.raise_for_status()
        # æå‰è¿‡æ»¤æ— æ•ˆè¡Œï¼šç©ºè¡Œã€æ³¨é‡Šè¡Œ
        lines = [line.strip() for line in response.text.splitlines() if line.strip() and not line.startswith("//")]
        print(f"âœ… æ•°æ®æº {idx+1} æŠ“å–æˆåŠŸï¼Œæœ‰æ•ˆè¡Œ {len(lines)}")
        return True, lines
    except Exception as e:
        print(f"âŒ æ•°æ®æº {idx+1} æŠ“å–å¤±è´¥ï¼š{str(e)[:50]}")
        return False, []

def fetch_raw_iptv_data_parallel(url_list):
    """å¹¶è¡ŒæŠ“å–æ‰€æœ‰æ•°æ®æº"""
    all_lines = []
    valid_source_count = 0
    # çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œ
    with ThreadPoolExecutor(max_workers=min(MAX_THREADS, len(url_list))) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_idx = {executor.submit(fetch_single_source, url, idx): idx for idx, url in enumerate(url_list)}
        # æŒ‰å®Œæˆé¡ºåºè·å–ç»“æœ
        for future in as_completed(future_to_idx):
            success, lines = future.result()
            if success and lines:
                all_lines.extend(lines)
                valid_source_count += 1
    print(f"\nğŸ“Š å¹¶è¡ŒæŠ“å–å®Œæˆï¼šå°è¯• {len(url_list)} æºï¼Œå¯ç”¨ {valid_source_count} æº")
    return all_lines

def extract_channel_name(line):
    """ä»m3uæ³¨é‡Šè¡Œæå–é¢‘é“åç§°"""
    if line.startswith("#EXTINF:"):
        match = re.search(r',([^,]+)$', line)
        if not match:
            match = re.search(r'tvg-name="([^"]+)"', line)
        if match:
            return match.group(1).strip()
    return None

def verify_single_source(url, channel_name):
    """éªŒè¯å•ä¸ªæºæ˜¯å¦å¯ç”¨ï¼Œè¿”å›(é¢‘é“å, æœ‰æ•ˆurl)ï¼ˆæ–°å¢åçˆ¬å»¶è¿Ÿ+ç¼“å­˜å¤ç”¨ï¼‰"""
    # æ–°å¢ï¼šè¯·æ±‚å‰æ·»åŠ éšæœºå»¶è¿Ÿï¼Œé¿å…åçˆ¬
    add_random_delay()
    
    if not url.startswith(("http://", "https://")):
        return None, None
    
    # å¤ç”¨ç¼“å­˜ï¼šé¿å…é‡å¤éªŒè¯ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
    with url_lock:
        if url in verified_urls:
            return channel_name, url
    
    try:
        response = requests.get(
            url,
            timeout=TIMEOUT,
            allow_redirects=True,
            headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36"},
            stream=True  # åªè¯·æ±‚å¤´ï¼Œä¸ä¸‹è½½å†…å®¹
        )
        if response.status_code in [200, 206, 301, 302, 307, 308]:
            with url_lock:
                verified_urls.add(url)
            return channel_name, url
    except:
        pass
    return None, None

def get_channel_group(channel_name):
    """é¢‘é“åˆ†ç»„é€»è¾‘ï¼ˆä¿ç•™åŸæœ‰åŠŸèƒ½ï¼‰"""
    if not channel_name:
        return "ğŸ¬ å…¶ä»–é¢‘é“"
    cctv_keywords = ["CCTV", "å¤®è§†", "ä¸­å¤®", "å¤®è§†é¢‘", "CCTV-", "ä¸­è§†"]
    if any(keyword in channel_name for keyword in cctv_keywords):
        return "ğŸ“º å¤®è§†é¢‘é“"
    if "å«è§†" in channel_name:
        return "ğŸ“¡ å«è§†é¢‘é“"
    province_city = ["åŒ—äº¬", "ä¸Šæµ·", "å¤©æ´¥", "é‡åº†", "æ²³åŒ—", "å±±è¥¿", "è¾½å®", "å‰æ—", "é»‘é¾™æ±Ÿ",
                     "æ±Ÿè‹", "æµ™æ±Ÿ", "å®‰å¾½", "ç¦å»º", "æ±Ÿè¥¿", "å±±ä¸œ", "æ²³å—", "æ¹–åŒ—", "æ¹–å—",
                     "å¹¿ä¸œ", "å¹¿è¥¿", "æµ·å—", "å››å·", "è´µå·", "äº‘å—", "é™•è¥¿", "ç”˜è‚ƒ", "é’æµ·",
                     "å†…è’™å¤", "å®å¤", "æ–°ç–†", "è¥¿è—", "é¦™æ¸¯", "æ¾³é—¨", "å°æ¹¾",
                     "å¹¿å·", "æ·±åœ³", "æ­å·", "å—äº¬", "æˆéƒ½", "æ­¦æ±‰", "è¥¿å®‰", "éƒ‘å·", "é’å²›"]
    for area in province_city:
        if area in channel_name and "å«è§†" not in channel_name:
            return "ğŸ™ï¸ åœ°æ–¹é¢‘é“"
    return "ğŸ¬ å…¶ä»–é¢‘é“"

def generate_m3u8_parallel(raw_lines):
    """å¹¶è¡ŒéªŒè¯æº+ç”Ÿæˆm3u8æ–‡ä»¶"""
    # å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨åŒ—äº¬æ—¶åŒºï¼ˆUTC+8ï¼‰ç”Ÿæˆæ—¶é—´
    tz_beijing = timezone(timedelta(hours=8))
    update_time = datetime.now(tz_beijing).strftime('%Y-%m-%d %H:%M:%S')
    
    # é¦–è¡Œç›´æ¥æ˜¾ç¤ºåŒ—äº¬æ—¶åˆ»ï¼ˆEXTM3Uä¸ºm3u8æ ‡å‡†é¦–è¡Œï¼Œæ·»åŠ åŒ—äº¬æ—¶é—´æ³¨é‡Šï¼‰
    m3u8_header = f"""#EXTM3U x-tvg-url="https://iptv-org.github.io/epg/guides/cn/tv.cctv.com.epg.xml"  # ç”Ÿæˆæ—¶é—´ï¼ˆåŒ—äº¬æ—¶åŒºï¼‰ï¼š{update_time}
# æ›´æ–°æ—¶é—´ï¼ˆåŒ—äº¬æ—¶åˆ»ï¼‰ï¼š{update_time}
# æ”¯æŒå¤šæºåˆ‡æ¢+é¢‘é“åˆ†ç»„+æœ¬åœ°ç¼“å­˜ä¼˜åŒ–
"""
    valid_lines = [m3u8_header]
    valid_lines.append(f"#EXTINF:-1 group-title='ğŸ“¢ ç³»ç»Ÿä¿¡æ¯',ğŸ“… ç›´æ’­æºæ›´æ–°æ—¶é—´ï¼ˆåŒ—äº¬æ—¶åˆ»ï¼‰ï¼š{update_time}")
    valid_lines.append("#")

    # ç¬¬ä¸€æ­¥ï¼šæå–æ‰€æœ‰å¾…éªŒè¯çš„(é¢‘é“å, url)å¯¹
    task_list = []
    temp_channel = None
    for line in raw_lines:
        line = line.strip()
        if not line:
            continue
        if line.startswith("#EXTINF:"):
            temp_channel = extract_channel_name(line)
        elif line.startswith(("http://", "https://")) and temp_channel:
            task_list.append((line, temp_channel))
            temp_channel = None
    print(f"\nğŸ” å¾…éªŒè¯æºæ€»æ•°ï¼š{len(task_list)}ï¼ˆå·²å¤ç”¨æœ¬åœ°ç¼“å­˜ï¼Œæ— éœ€é‡å¤éªŒè¯æœ‰æ•ˆæºï¼‰")

    # ç¬¬äºŒæ­¥ï¼šå¹¶è¡ŒéªŒè¯æ‰€æœ‰æº
    total_valid = 0
    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
        future_to_task = {executor.submit(verify_single_source, url, chan): (url, chan) for url, chan in task_list}
        for future in as_completed(future_to_task):
            chan_name, valid_url = future.result()
            if chan_name and valid_url:
                total_valid += 1
                # çº¿ç¨‹å®‰å…¨åœ°æ›´æ–°é¢‘é“-æºæ˜ å°„
                with map_lock:
                    if chan_name not in channel_sources_map:
                        channel_sources_map[chan_name] = []
                    if valid_url not in channel_sources_map[chan_name]:
                        channel_sources_map[chan_name].append(valid_url)

    # ç¬¬ä¸‰æ­¥ï¼šæŒ‰åˆ†ç»„ç”Ÿæˆæ–‡ä»¶
    grouped_channels = {"ğŸ“º å¤®è§†é¢‘é“": [], "ğŸ“¡ å«è§†é¢‘é“": [], "ğŸ™ï¸ åœ°æ–¹é¢‘é“": [], "ğŸ¬ å…¶ä»–é¢‘é“": []}
    for channel_name, sources in channel_sources_map.items():
        if sources:
            group = get_channel_group(channel_name)
            # å»é‡ï¼ˆå¦‚æœå¼€å¯å»é‡é…ç½®ï¼‰
            if REMOVE_DUPLICATE_CHANNELS:
                sources = list(dict.fromkeys(sources))
            # åªä¿ç•™æœ€å°‘æœ‰æ•ˆæºæ•°é‡
            if len(sources) >= MIN_VALID_SOURCES:
                sources = sources[:MIN_VALID_SOURCES]
            grouped_channels[group].append((channel_name, sources))

    # ç¬¬å››æ­¥ï¼šå†™å…¥æ–‡ä»¶
    for group, channels in grouped_channels.items():
        if channels:
            valid_lines.append(f"\n# {group}")
            for channel_name, sources in sorted(channels):
                for idx, url in enumerate(sources):
                    # ç”Ÿæˆå¸¦åˆ†ç»„çš„EXTINFè¡Œ
                    extinf_line = f"#EXTINF:-1 group-title='{group}',{channel_name}ï¼ˆæº{idx+1}ï¼‰"
                    valid_lines.append(extinf_line)
                    valid_lines.append(url)

    # æœ€ç»ˆå†™å…¥æ–‡ä»¶
    try:
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            f.write("\n".join(valid_lines))
        print(f"\nâœ… M3U8æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼ä¿å­˜è·¯å¾„ï¼š{OUTPUT_FILE}")
        print(f"ğŸ“Œ ç”Ÿæˆæ—¶åˆ»ï¼ˆåŒ—äº¬æ—¶åŒºï¼‰ï¼š{update_time}")
    except Exception as e:
        print(f"\nâŒ å†™å…¥M3U8æ–‡ä»¶å¤±è´¥ï¼š{str(e)}")

# -------------------------- ä¸»å‡½æ•° --------------------------
def main():
    """ä¸»æ‰§è¡Œæµç¨‹"""
    print("ğŸš€ å¼€å§‹IPTVæºçˆ¬å–ä¸éªŒè¯æµç¨‹...")
    start_time = time.time()
    
    # 1. åŠ è½½ç¼“å­˜
    load_verified_cache()
    
    # 2. å¹¶è¡ŒæŠ“å–åŸå§‹æ•°æ®
    raw_lines = fetch_raw_iptv_data_parallel(IPTV_SOURCE_URLS)
    if not raw_lines:
        print("âŒ æœªæŠ“å–åˆ°ä»»ä½•IPTVæºæ•°æ®ï¼Œç¨‹åºé€€å‡º")
        return
    
    # 3. å¹¶è¡ŒéªŒè¯+ç”Ÿæˆm3u8
    generate_m3u8_parallel(raw_lines)
    
    # 4. ä¿å­˜ç¼“å­˜
    save_verified_cache()
    
    # ç»Ÿè®¡è€—æ—¶
    total_time = round(time.time() - start_time, 2)
    print(f"\nğŸ‰ å…¨éƒ¨æµç¨‹å®Œæˆï¼æ€»è€—æ—¶ï¼š{total_time} ç§’")

if __name__ == "__main__":
    main()
