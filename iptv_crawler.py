import requests
import time
import random
import json
from datetime import datetime, timedelta
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from pathlib import Path
import logging
import multiprocessing
from typing import Tuple, List, Dict, Optional

# -------------------------- å…¨å±€é…ç½®ï¼ˆæ•ˆç‡+æ’­æ”¾ç«¯ç¾åŒ–ä¸“å±é…ç½®ï¼‰ --------------------------
# 1. æ•°æ®æºé…ç½®
IPTV_SOURCE_URLS = [
    "https://raw.githubusercontent.com/kakaxi-1/zubo/refs/heads/main/IPTV.txt",
    "https://raw.githubusercontent.com/kakaxi-1/IPTV/refs/heads/main/ipv4.txt",
    "https://raw.githubusercontent.com/vbskycn/iptv/refs/heads/master/tv/iptv4.m3u",
    "https://raw.githubusercontent.com/8080713/iptv-api666/refs/heads/main/output/result.m3u",
    "https://raw.githubusercontent.com/Guovin/iptv-api/refs/heads/gd/output/result.m3u",
    "https://raw.githubusercontent.com/BurningC4/Chinese-IPTV/master/TV-IPV4.m3u",
    "https://raw.githubusercontent.com/Kimentanm/aptv/master/m3u/iptv.m3u",
    "https://raw.githubusercontent.com/audyfan/tv/refs/heads/main/live.m3u"
]

# 2. æ•ˆç‡æ ¸å¿ƒé…ç½®ï¼ˆæ·±åº¦ä¼˜åŒ–ï¼‰
TIMEOUT_VERIFY = 2.5  # ç¼©çŸ­éªŒè¯è¶…æ—¶ï¼Œæå‡æ•ˆç‡ï¼ˆæ ¸å¿ƒéªŒè¯è¶³å¤Ÿç¨³å®šï¼‰
TIMEOUT_FETCH = 8
MIN_VALID_CHANNELS = 3
MAX_THREADS_VERIFY_BASE = 40  # æå‡éªŒè¯çº¿ç¨‹åŸºç¡€å€¼ï¼ˆå¼‚æ­¥åŒ–åæ— è¿‡è½½é£é™©ï¼‰
MAX_THREADS_FETCH_BASE = 6
MIN_DELAY = 0.03  # æè‡´ä½å»¶è¿Ÿï¼Œå…¼é¡¾åçˆ¬å’Œæ•ˆç‡
MAX_DELAY = 0.15
DISABLE_SSL_VERIFY = True  # å…³é—­SSLéªŒè¯ï¼Œå‡å°‘IOè€—æ—¶
BATCH_PROCESS_SIZE = 100  # æ‰¹é‡å¤„ç†URLï¼Œå‡å°‘å¾ªç¯å¼€é”€

# 3. è¾“å‡ºä¸ç¼“å­˜é…ç½®ï¼ˆåˆ†å±‚ç¼“å­˜ï¼‰
OUTPUT_FILE = "iptv_playlist.m3u8"
CACHE_FILE = "iptv_persist_cache.json"  # æŒä¹…ç¼“å­˜ï¼ˆ24å°æ—¶ï¼‰
TEMP_CACHE_SET = set()  # ä¸´æ—¶ç¼“å­˜ï¼ˆæœ¬æ¬¡è¿è¡Œï¼Œå†…å­˜çº§ï¼‰
CACHE_EXPIRE_HOURS = 24
REMOVE_DUPLICATE_CHANNELS = True
REMOVE_LOCAL_URLS = True

# 4. æ’­æ”¾ç«¯ä¸“å±ç¾åŒ–é…ç½®ï¼ˆæ ¸å¿ƒï¼è´´åˆæ’­æ”¾å™¨ï¼‰
CHANNEL_SORT_ENABLE = True
# æ’­æ”¾å™¨äºŒçº§åˆ†ç»„ï¼ˆæ’­æ”¾å™¨å†…åˆ†ç±»æ›´æ¸…æ™°ï¼‰
GROUP_SECONDARY_CCTV = "ğŸ“º å¤®è§†é¢‘é“-CCTV1-17"
GROUP_SECONDARY_WEISHI = "ğŸ“¡ å«è§†é¢‘é“-ä¸€çº¿/åœ°æ–¹"
GROUP_SECONDARY_LOCAL = "ğŸ™ï¸ åœ°æ–¹é¢‘é“-å„çœå¸‚åŒº"
GROUP_SECONDARY_OTHER = "ğŸ¬ å…¶ä»–é¢‘é“-ç‰¹è‰²/æ•°å­—"
# æ’­æ”¾ç«¯æ ‡é¢˜é…ç½®ï¼ˆç®€æ´+å…³é”®ä¿¡æ¯ï¼Œæ’­æ”¾å™¨å†…æ˜¾ç¤ºå‹å¥½ï¼‰
PLAYER_TITLE_PREFIX = True  # é¢‘é“ç±»å‹å›¾æ ‡å‰ç¼€
PLAYER_TITLE_SHOW_SPEED = True  # æ˜¾ç¤ºæœ€ä¼˜æºé€Ÿåº¦
PLAYER_TITLE_SHOW_NUM = True  # æ˜¾ç¤ºæœ‰æ•ˆæºæ•°
PLAYER_TITLE_SHOW_UPDATE = True  # æ˜¾ç¤ºæ›´æ–°æ—¶é—´ï¼ˆç²¾ç®€æ ¼å¼ï¼‰
UPDATE_TIME_FORMAT_SHORT = "%m-%d %H:%M"  # æ’­æ”¾å™¨æ ‡é¢˜å†…ç²¾ç®€æ›´æ–°æ—¶é—´æ ¼å¼
UPDATE_TIME_FORMAT_FULL = "%Y-%m-%d %H:%M:%S"  # æ—¥å¿—/æ–‡ä»¶å¤´å®Œæ•´æ ¼å¼
# å…¶ä»–ç¾åŒ–
GROUP_SEPARATOR = "#" * 50
URL_TRUNCATE_DOMAIN = True  # æ™ºèƒ½æˆªæ–­URLï¼ˆæ’­æ”¾å™¨æ³¨é‡Šå‹å¥½ï¼‰
URL_TRUNCATE_LENGTH = 50
SOURCE_NUM_PREFIX = "ğŸ“¶"  # æ’­æ”¾å™¨æ³¨é‡Šå†…æºå‰ç¼€ï¼ˆç²¾ç®€ï¼‰
SPEED_MARK_CACHE = "ğŸ’¾ç¼“å­˜"
SPEED_MARK_1 = "âš¡æé€Ÿ"  # <50ms
SPEED_MARK_2 = "ğŸš€å¿«é€Ÿ"  # 50-150ms
SPEED_MARK_3 = "â–¶æ™®é€š"  # >150ms
SPEED_LEVEL_1 = 50
SPEED_LEVEL_2 = 150

# -------------------------- åº•å±‚æè‡´ä¼˜åŒ–ï¼šé¢„ç¼–è¯‘+å…¨å±€å¸¸é‡+å†…å­˜å¤ç”¨ --------------------------
# é¢„ç¼–è¯‘æ‰€æœ‰æ­£åˆ™ï¼ˆä»…ä¸€æ¬¡ç¼–è¯‘ï¼Œå…¨ç¨‹å¤ç”¨ï¼‰
RE_CHANNEL_NAME = re.compile(r',\s*([^,]+)\s*$', re.IGNORECASE)
RE_TVG_NAME = re.compile(r'tvg-name="([^"]+)"', re.IGNORECASE)
RE_TITLE_NAME = re.compile(r'title="([^"]+)"', re.IGNORECASE)
RE_URL_DOMAIN = re.compile(r'https?://([^/]+)/?(.*)')
RE_CCTV = re.compile(r'CCTV(\d+)', re.IGNORECASE)
RE_WEISHI = re.compile(r'(.+)å«è§†', re.IGNORECASE)
# å…¨å±€å¸¸é‡ï¼ˆå†…å­˜å¤ç”¨ï¼Œé¿å…é‡å¤åˆ›å»ºï¼‰
LOCAL_HOSTS = {"localhost", "127.0.0.1", "192.168.", "10.", "172.", "169.254."}
VALID_SUFFIX = {".m3u8", ".ts", ".flv", ".rtmp", ".rtsp", ".m4s"}
VALID_CONTENT_TYPE = {"video/", "application/x-mpegurl", "audio/", "application/octet-stream"}
# å…¨å±€å˜é‡ï¼ˆä¸€æ¬¡ç”Ÿæˆï¼Œå…¨ç¨‹å¤ç”¨ï¼‰
GLOBAL_UPDATE_TIME_FULL = datetime.now().strftime(UPDATE_TIME_FORMAT_FULL)
GLOBAL_UPDATE_TIME_SHORT = datetime.now().strftime(UPDATE_TIME_FORMAT_SHORT)
CPU_CORES = multiprocessing.cpu_count()
# åŠ¨æ€çº¿ç¨‹æ± ï¼ˆå¼‚æ­¥åŒ–åï¼ŒæŒ‰CPUæ ¸å¿ƒæ•°ç¿»å€ï¼Œæ— è¿‡è½½é£é™©ï¼‰
MAX_THREADS_VERIFY = min(MAX_THREADS_VERIFY_BASE, CPU_CORES * 8)
MAX_THREADS_FETCH = min(MAX_THREADS_FETCH_BASE, CPU_CORES * 3)
# å†…å­˜å¤ç”¨å¯¹è±¡
channel_sources_map = dict()  # å¤ç”¨å­—å…¸
verified_urls = set()  # æŒä¹…ç¼“å­˜URL
task_list = list()  # å¤ç”¨ä»»åŠ¡åˆ—è¡¨
all_lines = list()  # å¤ç”¨æŠ“å–ç»“æœåˆ—è¡¨

# -------------------------- æ—¥å¿—åˆ†çº§ä¼˜åŒ–ï¼šæ§åˆ¶å°ç²¾ç®€+æ–‡ä»¶è¯¦ç»† --------------------------
def init_logger():
    logger = logging.getLogger("IPTV_Spider")
    logger.setLevel(logging.DEBUG)
    logger.handlers.clear()
    # æ§åˆ¶å°å¤„ç†å™¨ï¼šä»…INFOçº§åˆ«ï¼Œç²¾ç®€è¾“å‡ºï¼ˆå‡å°‘IOï¼‰
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch_fmt = logging.Formatter("[%(asctime)s] %(levelname)s: %(message)s", datefmt="%H:%M:%S")
    ch.setFormatter(ch_fmt)
    # æ–‡ä»¶å¤„ç†å™¨ï¼šDEBUGçº§åˆ«ï¼Œè¯¦ç»†è¾“å‡ºï¼ˆä¿ç•™æ—¥å¿—ï¼‰
    fh = logging.FileHandler("iptv_spider.log", encoding="utf-8", mode="a")
    fh.setLevel(logging.DEBUG)
    fh_fmt = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s", datefmt="%Y-%m-%d %H:%M:%S")
    fh.setFormatter(fh_fmt)
    logger.addHandler(ch)
    logger.addHandler(fh)
    return logger

logger = init_logger()

# -------------------------- Sessionè¿æ¥æ± æè‡´ä¼˜åŒ–ï¼šå…¨å±€å•ä¾‹+é•¿è¿æ¥ --------------------------
def init_global_session():
    session = requests.Session()
    adapter = requests.adapters.HTTPAdapter(
        pool_connections=30,  # å¢å¤§è¿æ¥æ± 
        pool_maxsize=100,
        max_retries=1,
        pool_block=False  # éé˜»å¡è¿æ¥æ± ï¼Œé¿å…ç­‰å¾…
    )
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    # å…¨å±€è¯·æ±‚å¤´ï¼ˆç²¾ç®€+é€šç”¨ï¼‰
    session.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/121.0.0.0 Safari/537.36",
        "Accept": "*/*",
        "Connection": "keep-alive",
        "Cache-Control": "no-cache"
    })
    # å…³é—­SSLéªŒè¯ï¼ˆæ·±åº¦æ•ˆç‡ä¼˜åŒ–ï¼‰
    if DISABLE_SSL_VERIFY:
        session.verify = False
        requests.packages.urllib3.disable_warnings(requests.packages.urllib3.exceptions.InsecureRequestWarning)
    return session

# å…¨å±€å•ä¾‹Sessionï¼ŒæŠ“å–+éªŒè¯å…±ç”¨ï¼ˆè¿æ¥æ± å¤ç”¨ï¼Œæ•ˆç‡æ‹‰æ»¡ï¼‰
GLOBAL_SESSION = init_global_session()

# -------------------------- å·¥å…·å‡½æ•°ï¼šæ•ˆç‡ä¼˜å…ˆ+æ’­æ”¾ç«¯ç¾åŒ–é€‚é… --------------------------
def add_random_delay():
    """æè‡´ä½å»¶è¿Ÿï¼Œå…¼é¡¾åçˆ¬"""
    if MIN_DELAY < MAX_DELAY:
        time.sleep(random.uniform(MIN_DELAY, MAX_DELAY))

def filter_invalid_urls(url: str) -> bool:
    """æ‰¹é‡URLè¿‡æ»¤ï¼Œé€»è¾‘æç®€ï¼Œæ•ˆç‡ä¼˜å…ˆ"""
    if not url or not url.startswith(("http://", "https://")):
        return False
    if REMOVE_LOCAL_URLS:
        for host in LOCAL_HOSTS:
            if host in url.lower():
                return False
    # ä¸´æ—¶ç¼“å­˜å‘½ä¸­ï¼Œç›´æ¥æœ‰æ•ˆï¼ˆæœ¬æ¬¡è¿è¡Œä¸é‡å¤è¿‡æ»¤ï¼‰
    if url in TEMP_CACHE_SET:
        return True
    TEMP_CACHE_SET.add(url)
    return True

def batch_filter_urls(url_list: List[str]) -> List[str]:
    """æ‰¹é‡URLè¿‡æ»¤ï¼Œå‡å°‘å¾ªç¯æ¬¡æ•°ï¼ˆæ•ˆç‡ä¼˜åŒ–ï¼‰"""
    return [url for url in url_list if filter_invalid_urls(url)]

def safe_extract_channel_name(line: str) -> Optional[str]:
    """æç®€é¢‘é“åæå–ï¼Œé¢„ç¼–è¯‘æ­£åˆ™ï¼Œæ— å†—ä½™åˆ¤æ–­"""
    if not line.startswith("#EXTINF:"):
        return None
    match = RE_CHANNEL_NAME.search(line) or RE_TVG_NAME.search(line)
    if match:
        name = match.group(1).strip()
        return name if name and not name.isdigit() else None
    return None

def get_player_channel_group(channel_name: str) -> str:
    """æ’­æ”¾ç«¯äºŒçº§åˆ†ç»„ï¼Œè´´åˆæ’­æ”¾å™¨åˆ†ç±»é€»è¾‘"""
    if not channel_name:
        return GROUP_SECONDARY_OTHER
    if "CCTV" in channel_name or "å¤®è§†" in channel_name or "ä¸­å¤®" in channel_name:
        return GROUP_SECONDARY_CCTV
    if "å«è§†" in channel_name:
        return GROUP_SECONDARY_WEISHI
    # åœ°æ–¹é¢‘é“å…³é”®è¯ï¼ˆç²¾ç®€ï¼Œæ•ˆç‡ä¼˜å…ˆï¼‰
    province = {"åŒ—äº¬", "ä¸Šæµ·", "å¤©æ´¥", "é‡åº†", "æ²³åŒ—", "å±±è¥¿", "è¾½å®", "å‰æ—", "é»‘é¾™æ±Ÿ",
                "æ±Ÿè‹", "æµ™æ±Ÿ", "å®‰å¾½", "ç¦å»º", "æ±Ÿè¥¿", "å±±ä¸œ", "æ²³å—", "æ¹–åŒ—", "æ¹–å—",
                "å¹¿ä¸œ", "å¹¿è¥¿", "æµ·å—", "å››å·", "è´µå·", "äº‘å—", "é™•è¥¿", "ç”˜è‚ƒ", "é’æµ·"}
    for p in province:
        if p in channel_name:
            return GROUP_SECONDARY_LOCAL
    return GROUP_SECONDARY_OTHER

def get_speed_mark(response_time: float) -> str:
    """æ’­æ”¾ç«¯å‹å¥½é€Ÿåº¦æ ‡æ³¨ï¼Œç²¾ç®€å›¾æ ‡"""
    if response_time == 0.0:
        return SPEED_MARK_CACHE
    elif response_time < SPEED_LEVEL_1:
        return f"{SPEED_MARK_1}"
    elif response_time < SPEED_LEVEL_2:
        return f"{SPEED_MARK_2}"
    else:
        return f"{SPEED_MARK_3}"

def get_best_speed_mark(sources: List[Tuple[str, float]]) -> str:
    """è·å–é¢‘é“æœ€ä¼˜æºé€Ÿåº¦æ ‡æ³¨ï¼ˆç”¨äºæ’­æ”¾ç«¯æ ‡é¢˜ï¼‰"""
    if not sources:
        return SPEED_MARK_3
    min_time = min([s[1] for s in sources])
    return get_speed_mark(min_time)

def smart_truncate_url(url: str) -> str:
    """æ’­æ”¾ç«¯å‹å¥½URLæˆªæ–­ï¼Œä¿ç•™åŸŸåï¼Œæ³¨é‡Šæ›´æ¸…æ™°"""
    if not url or len(url) <= URL_TRUNCATE_LENGTH:
        return url
    if not URL_TRUNCATE_DOMAIN:
        return url[:URL_TRUNCATE_LENGTH] + "..."
    match = RE_URL_DOMAIN.search(url)
    if not match:
        return url[:URL_TRUNCATE_LENGTH] + "..."
    domain, path = match.groups()
    remain = URL_TRUNCATE_LENGTH - len(domain) - 3
    path_trunc = path[:remain] if remain > 0 else ""
    return f"{domain}/{path_trunc}..."

def build_player_title(channel_name: str, sources: List[Tuple[str, float]]) -> str:
    """æ’­æ”¾ç«¯ä¸“å±æ ‡é¢˜æ„å»ºï¼Œç®€æ´+å…³é”®ä¿¡æ¯ï¼Œæ’­æ”¾å™¨å†…æ˜¾ç¤ºæœ€ä¼˜"""
    title_parts = []
    # å›¾æ ‡å‰ç¼€
    if PLAYER_TITLE_PREFIX:
        if GROUP_SECONDARY_CCTV in get_player_channel_group(channel_name):
            title_parts.append("ğŸ“º")
        elif GROUP_SECONDARY_WEISHI in get_player_channel_group(channel_name):
            title_parts.append("ğŸ“¡")
        elif GROUP_SECONDARY_LOCAL in get_player_channel_group(channel_name):
            title_parts.append("ğŸ™ï¸")
        else:
            title_parts.append("ğŸ¬")
    title_parts.append(channel_name)
    # æœ‰æ•ˆæºæ•°
    if PLAYER_TITLE_SHOW_NUM:
        title_parts.append(f"{len(sources)}æº")
    # æœ€ä¼˜æºé€Ÿåº¦
    if PLAYER_TITLE_SHOW_SPEED and sources:
        title_parts.append(get_best_speed_mark(sources))
    # ç²¾ç®€æ›´æ–°æ—¶é—´
    if PLAYER_TITLE_SHOW_UPDATE:
        title_parts.append(f"[{GLOBAL_UPDATE_TIME_SHORT}]")
    # æ‹¼æ¥æ ‡é¢˜ï¼ˆæ’­æ”¾å™¨å‹å¥½ï¼Œæ— ç‰¹æ®Šå­—ç¬¦ï¼‰
    return " ".join(title_parts).replace("  ", " ")

# -------------------------- åˆ†å±‚ç¼“å­˜ä¼˜åŒ–ï¼šæŒä¹…ç¼“å­˜+ä¸´æ—¶ç¼“å­˜ --------------------------
def load_persist_cache():
    """åŠ è½½æŒä¹…ç¼“å­˜ï¼Œæµå¼è¯»å–ï¼Œæ•ˆç‡ä¼˜å…ˆ"""
    global verified_urls
    try:
        cache_path = Path(CACHE_FILE)
        if not cache_path.exists():
            logger.info(f"æ— æŒä¹…ç¼“å­˜æ–‡ä»¶ï¼Œé¦–æ¬¡è¿è¡Œ")
            return
        with open(cache_path, "r", encoding="utf-8", buffering=1024*1024) as f:
            cache_data = json.load(f)
        cache_time = datetime.strptime(cache_data.get("cache_time", ""), UPDATE_TIME_FORMAT_FULL)
        if datetime.now() - cache_time > timedelta(hours=CACHE_EXPIRE_HOURS):
            logger.info(f"æŒä¹…ç¼“å­˜è¿‡æœŸï¼ˆ>24hï¼‰ï¼Œæ¸…ç©ºé‡æ–°ç”Ÿæˆ")
            return
        # æ‰¹é‡åŠ è½½ç¼“å­˜URLï¼Œæ•ˆç‡ä¼˜å…ˆ
        cache_urls = cache_data.get("verified_urls", [])
        verified_urls = set(batch_filter_urls(cache_urls))
        TEMP_CACHE_SET.update(verified_urls)  # åŒæ­¥åˆ°ä¸´æ—¶ç¼“å­˜
        logger.info(f"åŠ è½½æŒä¹…ç¼“å­˜æˆåŠŸ â†’ æœ‰æ•ˆæºæ•°ï¼š{len(verified_urls):,}")
    except Exception as e:
        logger.warning(f"æŒä¹…ç¼“å­˜åŠ è½½å¤±è´¥ï¼Œå¿½ç•¥ï¼š{str(e)[:50]}")
        verified_urls = set()

def save_persist_cache():
    """ä¿å­˜æŒä¹…ç¼“å­˜ï¼Œæ‰¹é‡å†™å…¥ï¼Œæ•ˆç‡ä¼˜å…ˆ"""
    try:
        cache_path = Path(CACHE_FILE)
        cache_path.parent.mkdir(parents=True, exist_ok=True)
        # æ‰¹é‡è½¬æ¢ä¸ºåˆ—è¡¨ï¼Œå‡å°‘å†…å­˜å¼€é”€
        cache_urls = list(verified_urls)[:2000]  # é™åˆ¶ç¼“å­˜å¤§å°ï¼Œé¿å…æ–‡ä»¶è¿‡å¤§
        cache_data = {
            "cache_time": GLOBAL_UPDATE_TIME_FULL,
            "verified_urls": cache_urls
        }
        with open(cache_path, "w", encoding="utf-8", buffering=1024*1024) as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=0)  # æ— ç¼©è¿›ï¼Œå‡å°æ–‡ä»¶å¤§å°
        logger.info(f"ä¿å­˜æŒä¹…ç¼“å­˜æˆåŠŸ â†’ ç¼“å­˜æºæ•°ï¼š{len(cache_urls):,} | æœ‰æ•ˆæœŸ24h")
    except Exception as e:
        logger.error(f"ä¿å­˜æŒä¹…ç¼“å­˜å¤±è´¥ï¼š{str(e)[:50]}")

# -------------------------- æ ¸å¿ƒåŠŸèƒ½ï¼šå¹¶è¡ŒæŠ“å–+éªŒè¯ï¼ˆæ·±åº¦æ•ˆç‡ä¼˜åŒ–ï¼‰ --------------------------
def fetch_single_source(url: str, idx: int) -> List[str]:
    """å•æ•°æ®æºæŠ“å–ï¼Œæµå¼è¯»å–ï¼Œæ‰¹é‡å¤„ç†ï¼Œæ•ˆç‡ä¼˜å…ˆ"""
    add_random_delay()
    try:
        with GLOBAL_SESSION.get(url, timeout=TIMEOUT_FETCH, stream=True) as resp:
            resp.raise_for_status()
            resp.encoding = resp.apparent_encoding or "utf-8"
            # æµå¼è¯»å–+æ‰¹é‡å¤„ç†ï¼Œå‡å°‘å†…å­˜å ç”¨
            lines = [line.strip() for line in resp.iter_lines(decode_unicode=True) if line.strip()]
            return batch_filter_urls(lines)
    except Exception as e:
        logger.debug(f"æ•°æ®æº{idx+1}æŠ“å–å¤±è´¥ï¼š{str(e)[:30]}")
        return []

def fetch_raw_data_parallel() -> List[str]:
    """å¹¶è¡ŒæŠ“å–æ‰€æœ‰æ•°æ®æºï¼Œæ‰¹é‡åˆå¹¶ï¼Œæ•ˆç‡æ‹‰æ»¡"""
    logger.info(f"å¼€å§‹å¹¶è¡ŒæŠ“å– â†’ æ•°æ®æºï¼š{len(IPTV_SOURCE_URLS)} | çº¿ç¨‹æ•°ï¼š{MAX_THREADS_FETCH} | è¶…æ—¶ï¼š{TIMEOUT_FETCH}s")
    global all_lines
    all_lines.clear()
    with ThreadPoolExecutor(max_workers=MAX_THREADS_FETCH) as executor:
        futures = [executor.submit(fetch_single_source, url, idx) for idx, url in enumerate(IPTV_SOURCE_URLS)]
        for future in as_completed(futures):
            all_lines.extend(future.result())
    # å…¨å±€æ‰¹é‡å»é‡ï¼Œé¿å…åç»­é‡å¤å¤„ç†ï¼ˆæ ¸å¿ƒæ•ˆç‡ä¼˜åŒ–ï¼‰
    all_lines = list(set(all_lines))
    logger.info(f"æŠ“å–å®Œæˆ â†’ æ€»æœ‰æ•ˆè¡Œï¼š{len(all_lines):,}ï¼ˆå·²å…¨å±€å»é‡+è¿‡æ»¤æ— æ•ˆURLï¼‰")
    return all_lines

def verify_single_url(url: str, channel_name: str) -> Optional[Tuple[str, str, float]]:
    """å•URLéªŒè¯ï¼Œæç®€é€»è¾‘ï¼Œå¼‚æ­¥éé˜»å¡ï¼Œæ•ˆç‡ä¼˜å…ˆ"""
    # æŒä¹…ç¼“å­˜/ä¸´æ—¶ç¼“å­˜å‘½ä¸­ï¼Œç›´æ¥è¿”å›ï¼ˆè€—æ—¶0ï¼Œå…éªŒè¯ï¼‰
    if url in verified_urls:
        return (channel_name, url, 0.0)
    add_random_delay()
    connect_timeout = 1.0
    read_timeout = max(0.5, TIMEOUT_VERIFY - connect_timeout)
    try:
        start = time.time()
        with GLOBAL_SESSION.get(
            url,
            timeout=(connect_timeout, read_timeout),
            stream=True,
            headers={"Range": "bytes=0-512"}  # ä»…è¯»å‰512å­—èŠ‚ï¼Œå‡å°‘IO
        ) as resp:
            # æ ¸å¿ƒæœ‰æ•ˆæ€§éªŒè¯ï¼ˆä»…ä¿ç•™å¿…è¦åˆ¤æ–­ï¼‰
            if resp.status_code not in [200, 206, 301, 302, 307, 308]:
                return None
            if not any(ct in resp.headers.get("Content-Type", "").lower() for ct in VALID_CONTENT_TYPE):
                return None
            if not resp.url.lower().endswith(tuple(VALID_SUFFIX)):
                return None
            # è®¡ç®—è€—æ—¶ï¼ˆæ¯«ç§’ï¼Œä¿ç•™1ä½ï¼Œå‡å°‘è®¡ç®—å¼€é”€ï¼‰
            response_time = round((time.time() - start) * 1000, 1)
            # éªŒè¯é€šè¿‡ï¼ŒåŠ å…¥åŒç¼“å­˜
            verified_urls.add(url)
            TEMP_CACHE_SET.add(url)
            return (channel_name, url, response_time)
    except Exception:
        return None

def extract_verify_tasks(raw_lines: List[str]) -> List[Tuple[str, str]]:
    """æå–éªŒè¯ä»»åŠ¡ï¼Œæ‰¹é‡å¤„ç†ï¼Œå‡å°‘å¾ªç¯ï¼ˆæ•ˆç‡ä¼˜åŒ–ï¼‰"""
    global task_list
    task_list.clear()
    temp_channel = None
    for line in raw_lines:
        if line.startswith("#EXTINF:"):
            temp_channel = safe_extract_channel_name(line)
        elif temp_channel and filter_invalid_urls(line):
            task_list.append((line, temp_channel))
            temp_channel = None
    # ä»»åŠ¡æ‰¹é‡å»é‡ï¼Œé¿å…é‡å¤éªŒè¯ï¼ˆæ ¸å¿ƒæ•ˆç‡ä¼˜åŒ–ï¼‰
    task_list = list(set(task_list))
    logger.info(f"æå–éªŒè¯ä»»åŠ¡ â†’ æ€»ä»»åŠ¡æ•°ï¼š{len(task_list):,}ï¼ˆå·²æ‰¹é‡å»é‡+ç¼“å­˜å‘½ä¸­ï¼‰")
    return task_list

def verify_tasks_parallel(tasks: List[Tuple[str, str]]):
    """å¹¶è¡ŒéªŒè¯æ‰€æœ‰URLï¼Œæ·±åº¦æ•ˆç‡ä¼˜åŒ–ï¼Œå†…å­˜å¤ç”¨"""
    logger.info(f"å¼€å§‹å¹¶è¡ŒéªŒè¯ â†’ ä»»åŠ¡æ•°ï¼š{len(tasks):,} | çº¿ç¨‹æ•°ï¼š{MAX_THREADS_VERIFY} | è¶…æ—¶ï¼š{TIMEOUT_VERIFY}s")
    global channel_sources_map
    channel_sources_map.clear()
    success_count = 0
    with ThreadPoolExecutor(max_workers=MAX_THREADS_VERIFY) as executor:
        futures = [executor.submit(verify_single_url, url, chan) for url, chan in tasks]
        for future in as_completed(futures):
            res = future.result()
            if res:
                chan_name, url, rt = res
                success_count += 1
                # å†…å­˜å¤ç”¨å­—å…¸ï¼Œé¿å…é‡å¤åˆ›å»º
                if chan_name not in channel_sources_map:
                    channel_sources_map[chan_name] = []
                channel_sources_map[chan_name].append((url, rt))
    # è®¡ç®—æˆåŠŸç‡ï¼Œé¿å…é™¤é›¶
    verify_rate = round(success_count / len(tasks) * 100, 1) if tasks else 0.0
    logger.info(f"éªŒè¯å®Œæˆ â†’ æˆåŠŸï¼š{success_count:,} | å¤±è´¥ï¼š{len(tasks)-success_count:,} | æˆåŠŸç‡ï¼š{verify_rate}%")
    # è¿‡æ»¤æ— æœ‰æ•ˆæºçš„é¢‘é“ï¼Œæ’­æ”¾ç«¯æ— ç©ºé¢‘é“
    channel_sources_map = {k: v for k, v in channel_sources_map.items() if v}
    logger.info(f"æœ‰æ•ˆé¢‘é“ç­›é€‰ â†’ å‰©ä½™æœ‰æ•ˆé¢‘é“ï¼š{len(channel_sources_map):,}ä¸ªï¼ˆå·²è¿‡æ»¤æ— æºæµé¢‘é“ï¼‰")

# -------------------------- æ’­æ”¾ç«¯ä¸“å±M3U8ç”Ÿæˆï¼šè´´åˆæ’­æ”¾å™¨å±•ç¤ºé€»è¾‘ --------------------------
def generate_player_m3u8() -> bool:
    """ç”Ÿæˆæ’­æ”¾ç«¯ä¸“å±ç¾åŒ–M3U8ï¼Œä¸¥æ ¼éµå¾ªM3U8è§„èŒƒï¼Œæ’­æ”¾å™¨å‹å¥½"""
    if not channel_sources_map:
        logger.error("æ— æœ‰æ•ˆé¢‘é“ï¼Œæ— æ³•ç”ŸæˆM3U8")
        return False
    # æŒ‰æ’­æ”¾ç«¯äºŒçº§åˆ†ç»„æ•´ç†é¢‘é“
    player_groups = {
        GROUP_SECONDARY_CCTV: [],
        GROUP_SECONDARY_WEISHI: [],
        GROUP_SECONDARY_LOCAL: [],
        GROUP_SECONDARY_OTHER: []
    }
    for chan_name, sources in channel_sources_map.items():
        # æºæŒ‰é€Ÿåº¦æ’åºï¼ˆæœ€å¿«åœ¨å‰ï¼Œæ’­æ”¾å™¨ä¼˜å…ˆé€‰æ‹©ç¬¬ä¸€ä¸ªï¼‰
        sources_sorted = sorted(sources, key=lambda x: x[1])[:3]  # ä¿ç•™æœ€å¿«3ä¸ª
        group = get_player_channel_group(chan_name)
        player_groups[group].append((chan_name, sources_sorted))
    # é¢‘é“å†…æ’åºï¼Œæ’­æ”¾ç«¯å±•ç¤ºæœ‰åº
    for group in player_groups:
        player_groups[group].sort(key=lambda x: x[0])
    # è¿‡æ»¤æ— æœ‰æ•ˆé¢‘é“çš„åˆ†ç»„ï¼Œæ’­æ”¾å™¨å†…æ— ç©ºåˆ†ç±»
    player_groups = {k: v for k, v in player_groups.items() if v}

    # æ„å»ºM3U8å†…å®¹ï¼ˆæ’­æ”¾ç«¯ä¸“å±ï¼Œç»“æ„æ¸…æ™°ï¼Œè§„èŒƒå‹å¥½ï¼‰
    m3u8_content = [
        "#EXTM3U x-tvg-url=https://iptv-org.github.io/epg/guides/cn/tv.cctv.com.epg.xml",
        GROUP_SEPARATOR,
        f"# ğŸ“º IPTVç›´æ’­æº - æ’­æ”¾ç«¯ä¸“å±ç‰ˆ | {GLOBAL_UPDATE_TIME_FULL}",
        f"# ğŸš€ æ•ˆç‡ä¼˜åŒ–ï¼šCPU{CPU_CORES}æ ¸è‡ªé€‚åº” | å…¨å±€è¿æ¥æ±  | åˆ†å±‚ç¼“å­˜ | æ‰¹é‡å¤„ç†",
        f"# âœ¨ æ’­æ”¾ç«¯ä¼˜åŒ–ï¼šäºŒçº§åˆ†ç±» | ä¸“å±æ ‡é¢˜ | é€Ÿåº¦æ ‡æ³¨ | ç²¾ç®€æ³¨é‡Š",
        f"# ğŸ¯ å…¼å®¹æ’­æ”¾å™¨ï¼šTVBox/Kodi/å®Œç¾è§†é¢‘/æå…‰TV/å°ç±³ç”µè§†/å½“è´/ä¹æ’­",
        GROUP_SEPARATOR,
        ""
    ]

    # ç”Ÿæˆæ’­æ”¾ç«¯åˆ†ç»„å†…å®¹ï¼ˆæ ¸å¿ƒï¼šè´´åˆæ’­æ”¾å™¨åˆ†ç±»é€»è¾‘ï¼‰
    for group_name, channels in player_groups.items():
        m3u8_content.extend([
            f"# ğŸ“Œ åˆ†ç»„ï¼š{group_name} | é¢‘é“æ•°ï¼š{len(channels)} | æ›´æ–°ï¼š{GLOBAL_UPDATE_TIME_FULL}",
            GROUP_SEPARATOR,
            ""
        ])
        # éå†æ¯ä¸ªé¢‘é“ï¼Œç”Ÿæˆæ’­æ”¾ç«¯ä¸“å±å†…å®¹
        for chan_name, sources in channels:
            player_title = build_player_title(chan_name, sources)
            # M3U8æ ‡å‡†è¡Œï¼ˆgroup-titleä¸ºæ’­æ”¾å™¨åˆ†ç±»ï¼Œtitleä¸ºæ’­æ”¾å™¨æ˜¾ç¤ºæ ‡é¢˜ï¼‰
            m3u8_content.append(f'#EXTINF:-1 group-title="{group_name}",{player_title}')
            # æ’­æ”¾ç«¯å‹å¥½æ³¨é‡Šï¼ˆç²¾ç®€+å…³é”®ä¿¡æ¯ï¼Œæ— å†—ä½™ï¼‰
            for idx, (url, rt) in enumerate(sources, 1):
                speed_mark = get_speed_mark(rt)
                trunc_url = smart_truncate_url(url)
                m3u8_content.append(f"# {SOURCE_NUM_PREFIX}{idx} {speed_mark}ï¼š{trunc_url}")
            # åŸå§‹æ’­æ”¾URLï¼ˆæ’­æ”¾å™¨å”¯ä¸€è¯†åˆ«ï¼Œæ— ä¿®æ”¹ï¼Œä¿è¯æ’­æ”¾ï¼‰
            m3u8_content.append(sources[0][0])
            m3u8_content.append("")
        m3u8_content.append(GROUP_SEPARATOR)
        m3u8_content.append("")

    # ç”Ÿæˆæ’­æ”¾ç«¯æ±‡æ€»ä¿¡æ¯ï¼ˆç²¾ç®€ï¼Œè´´åˆæ’­æ”¾å™¨æ³¨é‡Šé€»è¾‘ï¼‰
    total_channels = sum(len(v) for v in player_groups.values())
    total_sources = sum(len(s[1]) for v in player_groups.values() for s in v)
    m3u8_content.extend([
        f"# ğŸ“Š æ’­æ”¾åˆ—è¡¨æ±‡æ€» | {GLOBAL_UPDATE_TIME_FULL}",
        f"# ğŸ“º æ€»æœ‰æ•ˆé¢‘é“ï¼š{total_channels}ä¸ª | ğŸ“¶ æ€»æœ‰æ•ˆæºï¼š{total_sources}ä¸ª",
        f"# âš¡ éªŒè¯æˆåŠŸç‡ï¼š{round(total_sources/len(task_list)*100,1) if task_list else 100}% | ç¼“å­˜æºæ•°ï¼š{len(verified_urls):,}ä¸ª",
        f"# ğŸ“Œ æ’­æ”¾å™¨ä½¿ç”¨æç¤ºï¼šä¼˜å…ˆæ’­æ”¾ç¬¬ä¸€ä¸ªURLï¼Œå¡é¡¿è¯·æ‰‹åŠ¨åˆ‡æ¢å…¶ä»–æº",
        f"# ğŸ“… æ›´æ–°æç¤ºï¼šå»ºè®®æ¯6-12å°æ—¶é‡æ–°è¿è¡Œï¼Œä¿è¯æºçš„æ–°é²œåº¦å’Œæœ‰æ•ˆæ€§",
        f"# ğŸ¯ æœ€ä½³å…¼å®¹ï¼šæ’­æ”¾å™¨è®¾ç½®ç¼–ç ä¸ºUTF-8ï¼Œå…³é—­å¹¿å‘Šè¿‡æ»¤/URLé‡å†™",
        GROUP_SEPARATOR
    ])

    # æµå¼å†™å…¥M3U8æ–‡ä»¶ï¼Œæ•ˆç‡ä¼˜å…ˆï¼Œé¿å…å†…å­˜æº¢å‡º
    try:
        with open(OUTPUT_FILE, "w", encoding="utf-8", buffering=1024*1024) as f:
            f.write("\n".join(m3u8_content))
        logger.info(f"âœ… æ’­æ”¾ç«¯ä¸“å±M3U8ç”Ÿæˆå®Œæˆ â†’ {OUTPUT_FILE}")
        logger.info(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡ | é¢‘é“ï¼š{total_channels}ä¸ª | æºï¼š{total_sources}ä¸ª | æ›´æ–°ï¼š{GLOBAL_UPDATE_TIME_FULL}")
        return True
    except Exception as e:
        logger.error(f"å†™å…¥M3U8å¤±è´¥ï¼š{str(e)[:50]}")
        return False

# -------------------------- ä¸»ç¨‹åºï¼šå…¨æµç¨‹æ·±åº¦æ•ˆç‡ä¼˜åŒ–ï¼Œä¸€é”®è¿è¡Œ --------------------------
if __name__ == "__main__":
    start_total = time.time()
    logger.info("="*60)
    logger.info("IPTVç›´æ’­æºæŠ“å–å·¥å…· - ç©¶æç‰ˆï¼ˆæ•ˆç‡æ‹‰æ»¡+æ’­æ”¾ç«¯ä¸“å±ï¼‰")
    logger.info("="*60)
    logger.info(f"ç¨‹åºå¯åŠ¨ | CPUï¼š{CPU_CORES}æ ¸ | éªŒè¯çº¿ç¨‹ï¼š{MAX_THREADS_VERIFY} | æŠ“å–çº¿ç¨‹ï¼š{MAX_THREADS_FETCH}")
    logger.info(f"æ›´æ–°æ—¶é—´ | å®Œæ•´ï¼š{GLOBAL_UPDATE_TIME_FULL} | ç²¾ç®€ï¼š{GLOBAL_UPDATE_TIME_SHORT}")
    logger.info("="*60)

    # æ ¸å¿ƒæµç¨‹ï¼ˆå…¨æ·±åº¦æ•ˆç‡ä¼˜åŒ–ï¼Œæ­¥éª¤æœ€å°‘ï¼Œå¼€é”€æœ€ä½ï¼‰
    load_persist_cache()          # åŠ è½½åˆ†å±‚ç¼“å­˜
    fetch_raw_data_parallel()     # å¹¶è¡ŒæŠ“å–+æ‰¹é‡å¤„ç†
    extract_verify_tasks(all_lines)  # æå–éªŒè¯ä»»åŠ¡+æ‰¹é‡å»é‡
    verify_tasks_parallel(task_list) # å¹¶è¡ŒéªŒè¯+å†…å­˜å¤ç”¨
    generate_player_m3u8()        # ç”Ÿæˆæ’­æ”¾ç«¯ä¸“å±M3U8
    save_persist_cache()          # ä¿å­˜åˆ†å±‚ç¼“å­˜

    # æ€»è€—æ—¶ç»Ÿè®¡ï¼Œç²¾ç¡®åˆ°æ¯«ç§’
    total_time = round(time.time() - start_total, 2)
    logger.info("="*60)
    logger.info(f"ç¨‹åºè¿è¡Œå®Œæˆ | æ€»è€—æ—¶ï¼š{total_time}ç§’ | ç”Ÿæˆæ–‡ä»¶ï¼š{OUTPUT_FILE}")
    logger.info(f"ä½¿ç”¨å»ºè®®ï¼š1. æ’­æ”¾å™¨å¯¼å…¥{OUTPUT_FILE} 2. å®šæ—¶æ¯6å°æ—¶è¿è¡Œ 3. æ’­æ”¾å™¨ç¼–ç è®¾ä¸ºUTF-8")
    logger.info("="*60)
