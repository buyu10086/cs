import requests
import time
import random
import json
from datetime import datetime, timedelta
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
import logging
import multiprocessing
from typing import Tuple, List, Dict, Optional

# -------------------------- å…¨å±€é…ç½®ï¼ˆä¼˜åŒ–è‡ªåŠ¨é€‰æº+ä¿ç•™3ä¸ªæºï¼‰ --------------------------
# 1. æ•°æ®æºé…ç½®ï¼ˆå…¨é‡å«è§†é¢‘é“ï¼‰
IPTV_SOURCE_URLS = [
    "https://raw.githubusercontent.com/kakaxi-1/zubo/refs/heads/main/IPTV.txt",
    "https://raw.githubusercontent.com/kakaxi-1/IPTV/refs/heads/main/ipv4.txt",
    "https://raw.githubusercontent.com/vbskycn/iptv/refs/heads/master/tv/iptv4.m3u",
    "https://raw.githubusercontent.com/8080713/iptv-api666/refs/heads/main/output/result.m3u",
    "https://raw.githubusercontent.com/Guovin/iptv-api/refs/heads/gd/output/result.m3u",
    "https://raw.githubusercontent.com/BurningC4/Chinese-IPTV/master/TV-IPV4.m3u",
    "https://raw.githubusercontent.com/Kimentanm/aptv/master/m3u/iptv.m3u",
    "https://raw.githubusercontent.com/audyfan/tv/refs/heads/main/live.m3u",
    # å«è§†é¢‘é“ä¸“å±æ•°æ®æº
    "https://raw.githubusercontent.com/iptv-org/iptv/master/streams/cn.m3u",
    "https://raw.githubusercontent.com/zhouweitong123/IPTV/main/IPTV/å«è§†.m3u",
    "https://raw.githubusercontent.com/chenfenping/iptv/main/tv/m3u8/weishi.m3u",
    "https://raw.githubusercontent.com/yangzongzhuan/IPTV/master/m3u/weishi.m3u",
    "https://raw.githubusercontent.com/linkease/iptv/main/playlist/weishi.m3u"
]

# 2. æ•ˆç‡æ ¸å¿ƒé…ç½®
TIMEOUT_VERIFY = 3.5
TIMEOUT_FETCH = 12
MIN_VALID_CHANNELS = 1
MAX_THREADS_VERIFY_BASE = 25
MAX_THREADS_FETCH_BASE = 6
MIN_DELAY = 0.15
MAX_DELAY = 0.4
DISABLE_SSL_VERIFY = True
BATCH_PROCESS_SIZE = 50

# 3. è¾“å‡ºä¸ç¼“å­˜é…ç½®
OUTPUT_FILE = "iptv_playlist.m3u8"
CACHE_FILE = "iptv_persist_cache.json"
TEMP_CACHE_SET = set()
CACHE_EXPIRE_HOURS = 24
REMOVE_DUPLICATE_CHANNELS = False
REMOVE_LOCAL_URLS = True

# 4. è‡ªåŠ¨é€‰æº+ä¿ç•™3ä¸ªæºé…ç½®ï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼‰
AUTO_SELECT_SOURCE = True  # å¼€å¯è‡ªåŠ¨é€‰æœ€ä¼˜æºï¼ˆé»˜è®¤å¼€å¯ï¼‰
TOTAL_SOURCES_PER_CHANNEL = 3  # æ¯ä¸ªé¢‘é“ä¿ç•™3ä¸ªæºï¼ˆ1ä¸ªæœ€ä¼˜+2ä¸ªå¤‡ç”¨ï¼‰
SELECT_SPEED_THRESHOLD = 30  # é€Ÿåº¦å·®å€¼é˜ˆå€¼ï¼ˆmsï¼‰ï¼Œä½äºæ­¤å€¼æ—¶ä¼˜å…ˆé€‰.m3u8æ ¼å¼
PREFER_M3U8 = True  # ä¼˜å…ˆé€‰æ‹©.m3u8æ ¼å¼æºï¼ˆç¨³å®šæ€§æ›´é«˜ï¼‰

# 5. æ’åº+æ’­æ”¾ç«¯é…ç½®
CHANNEL_SORT_ENABLE = True
CCTV_SORT_ENABLE = True
WEISHI_SORT_ENABLE = True
LOCAL_SORT_ENABLE = True
FEATURE_SORT_ENABLE = True
DIGITAL_SORT_ENABLE = True

# åˆ†ç»„é…ç½®
GROUP_SECONDARY_CCTV = "ğŸ“º å¤®è§†é¢‘é“-CCTV1-17"
GROUP_SECONDARY_WEISHI = "ğŸ“¡ å«è§†é¢‘é“-ä¸€çº¿/åœ°æ–¹ï¼ˆå…¨é‡ï¼‰"
GROUP_SECONDARY_LOCAL = "ğŸ™ï¸ åœ°æ–¹é¢‘é“-å„çœå¸‚åŒº"
GROUP_SECONDARY_FEATURE = "ğŸ¬ ç‰¹è‰²é¢‘é“-ç”µå½±/ä½“è‚²/å°‘å„¿"
GROUP_SECONDARY_DIGITAL = "ğŸ”¢ æ•°å­—é¢‘é“-æŒ‰æ•°å­—æ’åº"
GROUP_SECONDARY_OTHER = "ğŸŒ€ å…¶ä»–é¢‘é“-ç»¼åˆ"

# æ’­æ”¾ç«¯ç¾åŒ–é…ç½®
PLAYER_TITLE_PREFIX = True
PLAYER_TITLE_SHOW_SPEED = True  # æ˜¾ç¤ºæœ€ä¼˜æºé€Ÿåº¦
PLAYER_TITLE_SHOW_NUM = True    # æ˜¾ç¤ºä¿ç•™æºæ•°ï¼ˆ3ä¸ªï¼‰
PLAYER_TITLE_SHOW_UPDATE = True
UPDATE_TIME_FORMAT_SHORT = "%m-%d %H:%M"
UPDATE_TIME_FORMAT_FULL = "%Y-%m-%d %H:%M:%S"
GROUP_SEPARATOR = "#" * 50
URL_TRUNCATE_DOMAIN = True
URL_TRUNCATE_LENGTH = 50
SOURCE_NUM_PREFIX = "ğŸ“¶"
SPEED_MARK_CACHE = "ğŸ’¾ç¼“å­˜Â·æé€Ÿ"
SPEED_MARK_1 = "âš¡æé€Ÿ"
SPEED_MARK_2 = "ğŸš€å¿«é€Ÿ"
SPEED_MARK_3 = "â–¶æ™®é€š"
SPEED_LEVEL_1 = 50
SPEED_LEVEL_2 = 150

# -------------------------- æ’åºæ ¸å¿ƒé…ç½® --------------------------
TOP_WEISHI = [
    "æ¹–å—å«è§†", "æµ™æ±Ÿå«è§†", "æ±Ÿè‹å«è§†", "ä¸œæ–¹å«è§†", "åŒ—äº¬å«è§†", "å®‰å¾½å«è§†", "å±±ä¸œå«è§†", "å¹¿ä¸œå«è§†",
    "æ·±åœ³å«è§†", "å¤©æ´¥å«è§†", "å››å·å«è§†", "æ¹–åŒ—å«è§†", "æ²³å—å«è§†", "æ±Ÿè¥¿å«è§†", "äº‘å—å«è§†", "è´µå·å«è§†"
]
ALL_PROVINCE_WEISHI = [
    "åŒ—äº¬å«è§†", "å¤©æ´¥å«è§†", "æ²³åŒ—å«è§†", "å±±è¥¿å«è§†", "å†…è’™å¤å«è§†", "è¾½å®å«è§†", "å‰æ—å«è§†", "é»‘é¾™æ±Ÿå«è§†",
    "ä¸Šæµ·å«è§†", "æ±Ÿè‹å«è§†", "æµ™æ±Ÿå«è§†", "å®‰å¾½å«è§†", "ç¦å»ºå«è§†", "æ±Ÿè¥¿å«è§†", "å±±ä¸œå«è§†", "æ²³å—å«è§†",
    "æ¹–åŒ—å«è§†", "æ¹–å—å«è§†", "å¹¿ä¸œå«è§†", "å¹¿è¥¿å«è§†", "æµ·å—å«è§†", "é‡åº†å«è§†", "å››å·å«è§†", "è´µå·å«è§†",
    "äº‘å—å«è§†", "è¥¿è—å«è§†", "é™•è¥¿å«è§†", "ç”˜è‚ƒå«è§†", "é’æµ·å«è§†", "å®å¤å«è§†", "æ–°ç–†å«è§†", "å°æ¹¾å«è§†",
    "é¦™æ¸¯å«è§†", "æ¾³é—¨å«è§†", "æ·±åœ³å«è§†", "å¦é—¨å«è§†", "é’å²›å«è§†"
]
DIRECT_CITIES = ["åŒ—äº¬", "ä¸Šæµ·", "å¤©æ´¥", "é‡åº†"]
PROVINCE_PINYIN_ORDER = [
    "å®‰å¾½", "ç¦å»º", "ç”˜è‚ƒ", "å¹¿ä¸œ", "å¹¿è¥¿", "è´µå·", "æµ·å—", "æ²³åŒ—", "æ²³å—", "é»‘é¾™æ±Ÿ",
    "æ¹–åŒ—", "æ¹–å—", "å‰æ—", "æ±Ÿè‹", "æ±Ÿè¥¿", "è¾½å®", "å†…è’™å¤", "å®å¤", "é’æµ·", "å±±ä¸œ",
    "å±±è¥¿", "é™•è¥¿", "ä¸Šæµ·", "å››å·", "å°æ¹¾", "å¤©æ´¥", "è¥¿è—", "æ–°ç–†", "äº‘å—", "æµ™æ±Ÿ",
    "é‡åº†", "åŒ—äº¬"
]
FEATURE_TYPE_ORDER = [
    ("ç”µå½±", ["ç”µå½±", "å½±é™¢", "å½±è§†"]),
    ("ä½“è‚²", ["ä½“è‚²", "èµ›äº‹", "å¥¥è¿", "è¶³çƒ", "ç¯®çƒ"]),
    ("å°‘å„¿", ["å°‘å„¿", "å¡é€š", "åŠ¨ç”»", "å®è´"]),
    ("è´¢ç»", ["è´¢ç»", "è‚¡å¸‚", "é‡‘è", "ç†è´¢"]),
    ("ç»¼è‰º", ["ç»¼è‰º", "å¨±ä¹", "é€‰ç§€", "æ™šä¼š"]),
    ("æ–°é—»", ["æ–°é—»", "èµ„è®¯", "æ—¶äº‹"]),
    ("çºªå½•ç‰‡", ["çºªå½•ç‰‡", "çºªå®", "çºªå½•"]),
    ("éŸ³ä¹", ["éŸ³ä¹", "æ­Œæ›²", "MTV"])
]

# -------------------------- åº•å±‚ä¼˜åŒ–ï¼šæ­£åˆ™+å…¨å±€å˜é‡ --------------------------
RE_CHANNEL_NAME = re.compile(r',\s*([^,]+)\s*$', re.IGNORECASE)
RE_TVG_NAME = re.compile(r'tvg-name="([^"]+)"', re.IGNORECASE)
RE_TITLE_NAME = re.compile(r'title="([^"]+)"', re.IGNORECASE)
RE_OTHER_NAME = re.compile(r'([^\s]+)$', re.IGNORECASE)
RE_URL_DOMAIN = re.compile(r'https?://([^/]+)/?(.*)')
RE_CCTV_NUMBER = re.compile(r'CCTV(\d+)', re.IGNORECASE)
RE_DIGITAL_NUMBER = re.compile(r'^(\d+)(é¢‘é“|å°)?$', re.IGNORECASE)
RE_WEISHI_SUFFIX = re.compile(r'(å«è§†|å«è§†é¢‘é“|å«è§†HD|å«è§†é«˜æ¸…|å«è§†-é«˜æ¸…)', re.IGNORECASE)
RE_M3U8_SUFFIX = re.compile(r'\.m3u8$', re.IGNORECASE)  # åŒ¹é….m3u8æ ¼å¼
LOCAL_HOSTS = {"localhost", "127.0.0.1", "192.168.", "10.", "172.", "169.254."}
VALID_SUFFIX = {".m3u8", ".ts", ".flv", ".rtmp", ".rtsp", ".m4s", ".mp4"}
VALID_CONTENT_TYPE = {"video/", "application/x-mpegurl", "audio/", "application/octet-stream", "video/mp4"}

# å…¨å±€å˜é‡
GLOBAL_UPDATE_TIME_FULL = datetime.now().strftime(UPDATE_TIME_FORMAT_FULL)
GLOBAL_UPDATE_TIME_SHORT = datetime.now().strftime(UPDATE_TIME_FORMAT_SHORT)
CPU_CORES = multiprocessing.cpu_count()
MAX_THREADS_VERIFY = min(MAX_THREADS_VERIFY_BASE, CPU_CORES * 4)
MAX_THREADS_FETCH = min(MAX_THREADS_FETCH_BASE, CPU_CORES * 2)
channel_sources_map = dict()
verified_urls = set()
task_list = list()
all_lines = list()

# -------------------------- æ—¥å¿—åˆå§‹åŒ– --------------------------
def init_logger():
    logger = logging.getLogger("IPTV_Spider")
    logger.setLevel(logging.DEBUG)
    logger.handlers.clear()
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch_fmt = logging.Formatter("[%(asctime)s] %(levelname)s: %(message)s", datefmt="%H:%M:%S")
    ch.setFormatter(ch_fmt)
    fh = logging.FileHandler("iptv_spider.log", encoding="utf-8", mode="a")
    fh.setLevel(logging.DEBUG)
    fh_fmt = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s", datefmt="%Y-%m-%d %H:%M:%S")
    fh.setFormatter(fh_fmt)
    logger.addHandler(ch)
    logger.addHandler(fh)
    return logger

logger = init_logger()

# -------------------------- Sessionåˆå§‹åŒ– --------------------------
def init_global_session():
    session = requests.Session()
    adapter = requests.adapters.HTTPAdapter(
        pool_connections=25,
        pool_maxsize=60,
        max_retries=3,
        pool_block=False
    )
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    session.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/121.0.0.0 Safari/537.36",
        "Accept": "*/*",
        "Connection": "keep-alive",
        "Cache-Control": "no-cache"
    })
    if DISABLE_SSL_VERIFY:
        session.verify = False
        requests.packages.urllib3.disable_warnings(requests.packages.urllib3.exceptions.InsecureRequestWarning)
    return session

GLOBAL_SESSION = init_global_session()

# -------------------------- å·¥å…·å‡½æ•°ï¼ˆæ ¸å¿ƒï¼šè‡ªåŠ¨é€‰æœ€ä¼˜æº+ä¿ç•™3ä¸ªæºï¼‰ --------------------------
def add_random_delay():
    time.sleep(random.uniform(MIN_DELAY, MAX_DELAY))

def filter_invalid_urls(url: str) -> bool:
    if not url or not url.startswith(("http://", "https://")):
        return False
    if REMOVE_LOCAL_URLS:
        for host in LOCAL_HOSTS:
            if host in url.lower():
                return False
    if url in TEMP_CACHE_SET:
        return True
    TEMP_CACHE_SET.add(url)
    return True

def safe_extract_channel_name(line: str) -> Optional[str]:
    if not line.startswith("#EXTINF:"):
        return None
    match = RE_CHANNEL_NAME.search(line) or RE_TVG_NAME.search(line) or RE_TITLE_NAME.search(line) or RE_OTHER_NAME.search(line)
    if match:
        name = match.group(1).strip()
        return name if name else "æœªçŸ¥é¢‘é“"
    return "æœªçŸ¥é¢‘é“"

def is_weishi_channel(channel_name: str) -> bool:
    if not channel_name:
        return False
    if RE_WEISHI_SUFFIX.search(channel_name):
        return True
    for weishi in ALL_PROVINCE_WEISHI:
        if weishi in channel_name and "å«è§†" in channel_name:
            return True
    for province in PROVINCE_PINYIN_ORDER:
        if province in channel_name and any(suffix in channel_name for suffix in ["å«è§†", "å«è§†é¢‘é“"]):
            return True
    return False

def get_channel_subgroup(channel_name: str) -> str:
    if DIGITAL_SORT_ENABLE and RE_DIGITAL_NUMBER.match(channel_name):
        return GROUP_SECONDARY_DIGITAL
    if is_weishi_channel(channel_name):
        return GROUP_SECONDARY_WEISHI
    if FEATURE_SORT_ENABLE:
        for feature_type, keywords in FEATURE_TYPE_ORDER:
            if any(keyword in channel_name for keyword in keywords):
                return GROUP_SECONDARY_FEATURE
    if "CCTV" in channel_name or "å¤®è§†" in channel_name or "ä¸­å¤®" in channel_name:
        return GROUP_SECONDARY_CCTV
    for area in DIRECT_CITIES + PROVINCE_PINYIN_ORDER:
        if area in channel_name and not is_weishi_channel(channel_name):
            return GROUP_SECONDARY_LOCAL
    return GROUP_SECONDARY_OTHER

def select_best_sources(sources: List[Tuple[str, float]]) -> List[Tuple[str, float]]:
    """æ ¸å¿ƒä¼˜åŒ–ï¼šè‡ªåŠ¨é€‰æ‹©1ä¸ªæœ€ä¼˜æº+2ä¸ªå¤‡ç”¨æºï¼Œå…±3ä¸ªæºï¼ˆå®Œæ•´é€»è¾‘ï¼Œæ— è¯­æ³•æ¼æ´ï¼‰"""
    if not sources:
        logger.debug("é€‰æºå¤±è´¥ï¼šæ— æœ‰æ•ˆæ’­æ”¾æº")
        return []
    
    # 1. å…ˆæŒ‰å“åº”æ—¶é—´å‡åºæ’åºï¼ˆæœ€å¿«åœ¨å‰ï¼Œä¸ºé€‰æºæ‰“åŸºç¡€ï¼‰
    sorted_sources = sorted(sources, key=lambda x: x[1])
    best_sources = []
    
    if AUTO_SELECT_SOURCE:
        # 2. ç¼“å­˜æºä¼˜å…ˆï¼ˆå“åº”æ—¶é—´0msï¼Œç›´æ¥ä½œä¸ºæœ€ä¼˜æºï¼‰
        cache_source = next((s for s in sorted_sources if s[1] == 0.0), None)
        if cache_source:
            best_sources.append(cache_source)
            # æ’é™¤å·²é€‰çš„ç¼“å­˜æºï¼Œå‰©ä½™æºç”¨äºé€‰å¤‡ç”¨
            remaining_sources = [s for s in sorted_sources if s[0] != cache_source[0]]
            logger.debug(f"é€‰æºæˆåŠŸï¼šå‘½ä¸­ç¼“å­˜æœ€ä¼˜æºï¼Œå¼€å§‹ç­›é€‰å¤‡ç”¨æº")
        else:
            # 3. æ— ç¼“å­˜æºï¼ŒæŒ‰ã€Œé€Ÿåº¦+æ ¼å¼ã€é€‰æ‹©æœ€ä¼˜æº
            primary_source = sorted_sources[0]
            
            # é€Ÿåº¦ç›¸è¿‘æ—¶ï¼Œä¼˜å…ˆé€‰æ‹©.m3u8æ ¼å¼ï¼ˆç¨³å®šæ€§æ›´é«˜ï¼‰
            if PREFER_M3U8 and len(sorted_sources) >= 2:
                first_speed = sorted_sources[0][1]
                second_speed = sorted_sources[1][1]
                # é€Ÿåº¦å·®å€¼ä½äºé˜ˆå€¼ï¼Œä¸”ç¬¬äºŒä¸ªæºæ˜¯.m3u8æ ¼å¼
                if (second_speed - first_speed) <= SELECT_SPEED_THRESHOLD and RE_M3U8_SUFFIX.search(sorted_sources[1][0]):
                    primary_source = sorted_sources[1]
                    logger.debug(f"é€‰æºæˆåŠŸï¼šé€Ÿåº¦ç›¸è¿‘ï¼Œä¼˜å…ˆé€‰æ‹©.m3u8æ ¼å¼ä½œä¸ºæœ€ä¼˜æº")
            
            best_sources.append(primary_source)
            # æ’é™¤å·²é€‰çš„æœ€ä¼˜æºï¼Œå‰©ä½™æºç”¨äºé€‰å¤‡ç”¨
            remaining_sources = [s for s in sorted_sources if s[0] != primary_source[0]]
            logger.debug(f"é€‰æºæˆåŠŸï¼šç­›é€‰å‡ºéç¼“å­˜æœ€ä¼˜æºï¼Œå“åº”æ—¶é—´{primary_source[1]}ms")
        
        # 4. ä»å‰©ä½™æºä¸­ç­›é€‰2ä¸ªå¤‡ç”¨æºï¼ˆæŒ‰é€Ÿåº¦æ’åºï¼Œæœ€å¤šè¡¥å¤Ÿ3ä¸ªæºï¼‰
        backup_count = TOTAL_SOURCES_PER_CHANNEL - len(best_sources)
        backup_sources = remaining_sources[:backup_count]
        best_sources.extend(backup_sources)
        
        # 5. å»é‡å¤‡ç”¨æºï¼ˆé¿å…URLé‡å¤ï¼‰
        unique_best_sources = []
        seen_urls = set()
        for s in best_sources:
            if s[0] not in seen_urls:
                seen_urls.add(s[0])
                unique_best_sources.append(s)
        best_sources = unique_best_sources[:TOTAL_SOURCES_PER_CHANNEL]
    
    else:
        # å…³é—­è‡ªåŠ¨é€‰æºæ—¶ï¼Œç›´æ¥å–å‰3ä¸ªæœ€å¿«çš„æº
        best_sources = sorted_sources[:TOTAL_SOURCES_PER_CHANNEL]
    
    # 6. ç¡®ä¿è¿”å›ç»“æœä¸è¶…è¿‡è®¾å®šçš„æºæ•°ï¼Œä¸”æ— ç©ºå€¼
    final_sources = best_sources[:TOTAL_SOURCES_PER_CHANNEL]
    logger.debug(f"é€‰æºå®Œæˆï¼šè¯¥é¢‘é“å…±ä¿ç•™{len(final_sources)}ä¸ªæœ‰æ•ˆæ’­æ”¾æº")
    return final_sources

# -------------------------- è¾…åŠ©å·¥å…·å‡½æ•°ï¼ˆå®Œæ•´ï¼‰ --------------------------
def get_speed_mark(response_time: float) -> str:
    if response_time == 0.0:
        return SPEED_MARK_CACHE
    elif response_time < SPEED_LEVEL_1:
        return f"{SPEED_MARK_1}"
    elif response_time < SPEED_LEVEL_2:
        return f"{SPEED_MARK_2}"
    else:
        return f"{SPEED_MARK_3}"

def get_best_speed_mark(sources: List[Tuple[str, float]]) -> str:
    if not sources:
        return SPEED_MARK_3
    min_time = min([s[1] for s in sources])
    return get_speed_mark(min_time)

def smart_truncate_url(url: str) -> str:
    if not url or len(url) <= URL_TRUNCATE_LENGTH:
        return url
    if not URL_TRUNCATE_DOMAIN:
        return url[:URL_TRUNCATE_LENGTH] + "..."
    match = RE_URL_DOMAIN.search(url)
    if not match:
        return url[:URL_TRUNCATE_LENGTH] + "..."
    domain, path = match.groups()
    remain = URL_TRUNCATE_LENGTH - len(domain) - 3
    path_trunc = path[:remain] if remain > 0 else ""
    return f"{domain}/{path_trunc}..."

def build_player_title(channel_name: str, sources: List[Tuple[str, float]]) -> str:
    title_parts = []
    if PLAYER_TITLE_PREFIX:
        subgroup = get_channel_subgroup(channel_name)
        if subgroup == GROUP_SECONDARY_CCTV:
            title_parts.append("ğŸ“º")
        elif subgroup == GROUP_SECONDARY_WEISHI:
            title_parts.append("ğŸ“¡")
        elif subgroup == GROUP_SECONDARY_LOCAL:
            title_parts.append("ğŸ™ï¸")
        elif subgroup == GROUP_SECONDARY_FEATURE:
            title_parts.append("ğŸ¬")
        elif subgroup == GROUP_SECONDARY_DIGITAL:
            title_parts.append("ğŸ”¢")
        else:
            title_parts.append("ğŸŒ€")
    title_parts.append(channel_name)
    if PLAYER_TITLE_SHOW_NUM and sources:
        title_parts.append(f"{len(sources)}æº")
    if PLAYER_TITLE_SHOW_SPEED and sources:
        title_parts.append(get_best_speed_mark(sources))
    if PLAYER_TITLE_SHOW_UPDATE:
        title_parts.append(f"[{GLOBAL_UPDATE_TIME_SHORT}]")
    return " ".join(title_parts).replace("  ", " ")

# -------------------------- ç¼“å­˜å‡½æ•°ï¼ˆå®Œæ•´ï¼‰ --------------------------
def load_persist_cache():
    global verified_urls
    try:
        cache_path = Path(CACHE_FILE)
        if not cache_path.exists():
            logger.info(f"æ— æŒä¹…ç¼“å­˜æ–‡ä»¶ï¼Œé¦–æ¬¡è¿è¡Œ")
            return
        with open(cache_path, "r", encoding="utf-8", buffering=1024*1024) as f:
            cache_data = json.load(f)
        cache_time = datetime.strptime(cache_data.get("cache_time", ""), UPDATE_TIME_FORMAT_FULL)
        if datetime.now() - cache_time > timedelta(hours=CACHE_EXPIRE_HOURS):
            logger.info(f"æŒä¹…ç¼“å­˜è¿‡æœŸï¼ˆè¶…è¿‡{CACHE_EXPIRE_HOURS}å°æ—¶ï¼‰ï¼Œæ¸…ç©ºé‡æ–°ç”Ÿæˆ")
            return
        cache_urls = cache_data.get("verified_urls", [])
        verified_urls = set([url for url in cache_urls if filter_invalid_urls(url)])
        TEMP_CACHE_SET.update(verified_urls)
        logger.info(f"åŠ è½½æŒä¹…ç¼“å­˜æˆåŠŸ â†’ æœ‰æ•ˆç¼“å­˜æºæ•°ï¼š{len(verified_urls):,}")
    except Exception as e:
        logger.warning(f"æŒä¹…ç¼“å­˜åŠ è½½å¤±è´¥ï¼š{str(e)[:50]}")
        verified_urls = set()

def save_persist_cache():
    try:
        cache_path = Path(CACHE_FILE)
        cache_path.parent.mkdir(parents=True, exist_ok=True)
        cache_urls = list(verified_urls)[:3000]  # æ‰©å¤§ç¼“å­˜å®¹é‡ï¼Œä¿ç•™æ›´å¤šå«è§†é¢‘æº
        cache_data = {
            "cache_time": GLOBAL_UPDATE_TIME_FULL,
            "verified_urls": cache_urls
        }
        with open(cache_path, "w", encoding="utf-8", buffering=1024*1024) as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=0)
        logger.info(f"ä¿å­˜æŒä¹…ç¼“å­˜æˆåŠŸ â†’ ç¼“å­˜æºæ•°ï¼š{len(cache_urls):,}")
    except Exception as e:
        logger.error(f"ä¿å­˜æŒä¹…ç¼“å­˜å¤±è´¥ï¼š{str(e)[:50]}")

# -------------------------- æ ¸å¿ƒåŠŸèƒ½ï¼ˆæŠ“å–+éªŒè¯ï¼Œå®Œæ•´ï¼‰ --------------------------
def fetch_single_source(url: str, idx: int) -> List[str]:
    add_random_delay()
    try:
        with GLOBAL_SESSION.get(url, timeout=TIMEOUT_FETCH, stream=True) as resp:
            resp.raise_for_status()
            resp.encoding = resp.apparent_encoding or "utf-8"
            lines = [line.strip() for line in resp.iter_lines(decode_unicode=True) if line.strip()]
            logger.debug(f"æ•°æ®æº{idx+1}æŠ“å–å®Œæˆ â†’ æœ‰æ•ˆè¡Œï¼š{len(lines)}")
            return lines
    except Exception as e:
        logger.debug(f"æ•°æ®æº{idx+1}æŠ“å–å¤±è´¥ï¼š{str(e)[:30]}")
        return []

def fetch_raw_data_parallel() -> List[str]:
    logger.info(f"å¼€å§‹å¹¶è¡ŒæŠ“å– â†’ æ•°æ®æºï¼š{len(IPTV_SOURCE_URLS)} | çº¿ç¨‹æ•°ï¼š{MAX_THREADS_FETCH} | è¶…æ—¶ï¼š{TIMEOUT_FETCH}s")
    global all_lines
    all_lines.clear()
    with ThreadPoolExecutor(max_workers=MAX_THREADS_FETCH) as executor:
        futures = [executor.submit(fetch_single_source, url, idx) for idx, url in enumerate(IPTV_SOURCE_URLS)]
        for future in as_completed(futures):
            all_lines.extend(future.result())
    logger.info(f"æŠ“å–å®Œæˆ â†’ æ€»æœ‰æ•ˆè¡Œï¼š{len(all_lines):,}ï¼ˆåŒ…å«å¤§é‡å«è§†é¢‘é“æ•°æ®ï¼‰")
    return all_lines

def verify_single_url(url: str, channel_name: str) -> Optional[Tuple[str, str, float]]:
    if url in verified_urls:
        return (channel_name, url, 0.0)
    add_random_delay()
    connect_timeout = 1.5
    read_timeout = max(1.5, TIMEOUT_VERIFY - connect_timeout)
    try:
        start = time.time()
        with GLOBAL_SESSION.get(
            url,
            timeout=(connect_timeout, read_timeout),
            stream=True,
            headers={"Range": "bytes=0-1024"}
        ) as resp:
            if resp.status_code not in [200, 206, 301, 302, 307, 308]:
                return None
            if not any(ct in resp.headers.get("Content-Type", "").lower() for ct in VALID_CONTENT_TYPE):
                return None
            if not resp.url.lower().endswith(tuple(VALID_SUFFIX)):
                return None
            # æ”¾å®½m3u8æ–‡ä»¶å¤´éªŒè¯ï¼Œé€‚é…æ›´å¤šå«è§†é¢‘æº
            if resp.url.lower().endswith(".m3u8"):
                stream_data = resp.content[:1024].decode("utf-8", errors="ignore")
                if "#EXTM3U" not in stream_data and "EXTM3U" not in stream_data:
                    return None
            response_time = round((time.time() - start) * 1000, 1)
            verified_urls.add(url)
            TEMP_CACHE_SET.add(url)
            return (channel_name, url, response_time)
    except Exception:
        return None

def extract_verify_tasks(raw_lines: List[str]) -> List[Tuple[str, str]]:
    global task_list
    task_list.clear()
    temp_channel = None
    for line in raw_lines:
        if line.startswith("#EXTINF:"):
            temp_channel = safe_extract_channel_name(line)
        elif temp_channel and filter_invalid_urls(line):
            task_list.append((line, temp_channel))
            temp_channel = None
    # ä»…å¯¹URLå»é‡ï¼Œä¿ç•™ä¸åŒåç§°çš„å«è§†é¢‘é“ï¼ˆé¿å…ä¸¢å¤±å…¨é‡å«è§†ï¼‰
    unique_urls = set()
    unique_tasks = []
    for url, chan in task_list:
        if url not in unique_urls:
            unique_urls.add(url)
            unique_tasks.append((url, chan))
    task_list = unique_tasks
    logger.info(f"æå–éªŒè¯ä»»åŠ¡ â†’ æ€»ä»»åŠ¡æ•°ï¼š{len(task_list):,}ï¼ˆåŒ…å«å¤§é‡å«è§†é¢‘é“ä»»åŠ¡ï¼‰")
    return task_list

def verify_tasks_parallel(tasks: List[Tuple[str, str]]):
    logger.info(f"å¼€å§‹å¹¶è¡ŒéªŒè¯ â†’ ä»»åŠ¡æ•°ï¼š{len(tasks):,} | çº¿ç¨‹æ•°ï¼š{MAX_THREADS_VERIFY} | è¶…æ—¶ï¼š{TIMEOUT_VERIFY}s")
    global channel_sources_map
    channel_sources_map.clear()
    success_count = 0
    weishi_count = 0  # ç»Ÿè®¡å«è§†é¢‘é“æ•°é‡ï¼Œç”¨äºéªŒè¯æ˜¯å¦æŠ“å–æˆåŠŸ
    
    with ThreadPoolExecutor(max_workers=MAX_THREADS_VERIFY) as executor:
        futures = [executor.submit(verify_single_url, url, chan) for url, chan in tasks]
        for future in as_completed(futures):
            res = future.result()
            if res:
                chan_name, url, rt = res
                success_count += 1
                
                # ç»Ÿè®¡å«è§†é¢‘é“æ•°é‡
                if is_weishi_channel(chan_name):
                    weishi_count += 1
                
                # å­˜å…¥é¢‘é“-æºæ˜ å°„è¡¨
                if chan_name not in channel_sources_map:
                    channel_sources_map[chan_name] = []
                channel_sources_map[chan_name].append((url, rt))
    
    # æ‰“å°éªŒè¯ç»“æœï¼Œæ–¹ä¾¿æ’æŸ¥é—®é¢˜
    verify_rate = round(success_count / len(tasks) * 100, 1) if tasks else 0.0
    logger.info(f"éªŒè¯å®Œæˆ â†’ æˆåŠŸï¼š{success_count:,} | å¤±è´¥ï¼š{len(tasks)-success_count:,} | æˆåŠŸç‡ï¼š{verify_rate}%")
    logger.info(f"å«è§†é¢‘é“ç»Ÿè®¡ â†’ æˆåŠŸéªŒè¯å«è§†é¢‘é“ï¼š{weishi_count} ä¸ªï¼ˆå…¨é‡æ”¶å½•ï¼‰")
    
    # ç­›é€‰æœ‰æœ‰æ•ˆæºçš„é¢‘é“
    channel_sources_map = {k: v for k, v in channel_sources_map.items() if v}
    logger.info(f"æœ‰æ•ˆé¢‘é“ç­›é€‰ â†’ å‰©ä½™æ€»æœ‰æ•ˆé¢‘é“ï¼š{len(channel_sources_map):,}ä¸ª")

# -------------------------- æ ¸å¿ƒï¼šç”Ÿæˆå¸¦3ä¸ªæºçš„m3u8æ–‡ä»¶ï¼ˆå®Œæ•´ï¼Œå¸¦æ›´æ–°æ ¡éªŒï¼‰ --------------------------
def generate_player_m3u8() -> bool:
    if not channel_sources_map:
        logger.error("ç”Ÿæˆå¤±è´¥ï¼šæ— æœ‰æ•ˆé¢‘é“ï¼ˆå¯å°è¯•æ›´æ¢æ•°æ®æºæˆ–æ£€æŸ¥ç½‘ç»œï¼‰")
        return False
    
    # 1. æŒ‰åˆ†ç»„æ•´ç†é¢‘é“ï¼Œå¹¶ä¸ºæ¯ä¸ªé¢‘é“ç­›é€‰3ä¸ªæœ€ä¼˜æº
    player_groups = {
        GROUP_SECONDARY_CCTV: [],
        GROUP_SECONDARY_WEISHI: [],
        GROUP_SECONDARY_LOCAL: [],
        GROUP_SECONDARY_FEATURE: [],
        GROUP_SECONDARY_DIGITAL: [],
        GROUP_SECONDARY_OTHER: []
    }
    
    for chan_name, sources in channel_sources_map.items():
        # è°ƒç”¨é€‰æºå‡½æ•°ï¼Œè·å–1ä¸ªæœ€ä¼˜+2ä¸ªå¤‡ç”¨ï¼Œå…±3ä¸ªæº
        best_3_sources = select_best_sources(sources)
        if not best_3_sources:
            continue
        
        # æŒ‰åˆ†ç»„å½’ç±»
        subgroup = get_channel_subgroup(chan_name)
        player_groups[subgroup].append((chan_name, best_3_sources))
    
    # 2. å„åˆ†ç»„æŒ‰å¯¹åº”è§„åˆ™æ’åº
    for group_name, channels in player_groups.items():
        if channels:
            channels.sort(key=lambda x: get_channel_sort_key(group_name, x[0]))
            # é‡ç‚¹æ‰“å°å«è§†é¢‘é“æ’åºç»“æœï¼Œæ–¹ä¾¿éªŒè¯
            if group_name == GROUP_SECONDARY_WEISHI:
                logger.info(f"å«è§†é¢‘é“æ’åºå®Œæˆ â†’ å‰20ä¸ªï¼š{[chan[0] for chan in channels[:20]]}...")
            else:
                logger.info(f"{group_name}æ’åºå®Œæˆ â†’ å‰10ä¸ªï¼š{[chan[0] for chan in channels[:10]]}...")
    
    # 3. è¿‡æ»¤æ— æœ‰æ•ˆé¢‘é“çš„åˆ†ç»„
    player_groups = {k: v for k, v in player_groups.items() if v}
    if not player_groups:
        logger.error("ç”Ÿæˆå¤±è´¥ï¼šæ— æœ‰æ•ˆåˆ†ç»„é¢‘é“")
        return False
    
    # 4. æ„å»ºm3u8æ–‡ä»¶å†…å®¹ï¼ˆå¸¦æ›´æ–°æ—¶é—´ï¼Œç¡®ä¿æ–‡ä»¶æœ‰å˜åŒ–ï¼‰
    m3u8_content = [
        "#EXTM3U x-tvg-url=https://iptv-org.github.io/epg/guides/cn/tv.cctv.com.epg.xml",
        GROUP_SEPARATOR,
        f"# ğŸ“º IPTVç›´æ’­æº - è‡ªåŠ¨é€‰æœ€ä¼˜+3ä¸ªæ‰‹åŠ¨åˆ‡æ¢æº | æ›´æ–°æ—¶é—´ï¼š{GLOBAL_UPDATE_TIME_FULL}",
        f"# ğŸš€ é€‰æºè§„åˆ™ï¼šç¼“å­˜æºä¼˜å…ˆâ†’é€Ÿåº¦ä¼˜å…ˆâ†’.m3u8æ ¼å¼ä¼˜å…ˆ | æ¯ä¸ªé¢‘é“ä¿ç•™3ä¸ªæœ‰æ•ˆæº",
        f"# ğŸ“¡ å«è§†é¢‘é“ï¼šä¸€çº¿å«è§†+çœçº§å«è§†+åœ°æ–¹å«è§†ï¼ˆå…¨é‡æ”¶å½•ï¼‰",
        f"# ğŸ¯ å…¼å®¹ï¼šTVBox/Kodi/å®Œç¾è§†é¢‘/æå…‰TV",
        GROUP_SEPARATOR,
        ""
    ]
    
    # 5. å†™å…¥å„åˆ†ç»„çš„é¢‘é“å’Œæ’­æ”¾æº
    for group_name, channels in player_groups.items():
        m3u8_content.extend([
            f"# ğŸ“Œ åˆ†ç»„ï¼š{group_name} | é¢‘é“æ•°ï¼š{len(channels)} | æ›´æ–°ï¼š{GLOBAL_UPDATE_TIME_FULL}",
            GROUP_SEPARATOR,
            ""
        ])
        
        for chan_name, best_3_sources in channels:
            # æ„å»ºæ’­æ”¾å™¨æ˜¾ç¤ºæ ‡é¢˜
            player_title = build_player_title(chan_name, best_3_sources)
            m3u8_content.append(f'#EXTINF:-1 group-title="{group_name}",{player_title}')
            
            # å†™å…¥3ä¸ªæºçš„å¤‡æ³¨ï¼ˆæ–¹ä¾¿æŸ¥çœ‹é€Ÿåº¦ï¼‰å’Œå®é™…æ’­æ”¾URLï¼ˆç¬¬ä¸€ä¸ªä¸ºæœ€ä¼˜é»˜è®¤æºï¼‰
            for idx, (url, rt) in enumerate(best_3_sources, 1):
                speed_mark = get_speed_mark(rt)
                trunc_url = smart_truncate_url(url)
                m3u8_content.append(f"# {SOURCE_NUM_PREFIX}{idx} {speed_mark}ï¼š{trunc_url}")
            
            # å†™å…¥é»˜è®¤æ’­æ”¾URLï¼ˆæœ€ä¼˜æºï¼Œæ’­æ”¾å™¨æ‰“å¼€å³æ’­ï¼‰
            m3u8_content.append(best_3_sources[0][0])
            m3u8_content.append("")
        
        m3u8_content.append(GROUP_SEPARATOR)
        m3u8_content.append("")
    
    # 6. å†™å…¥æ±‡æ€»ç»Ÿè®¡ï¼Œç¡®ä¿æ¯æ¬¡ç”Ÿæˆçš„æ–‡ä»¶å†…å®¹ä¸åŒ
    total_channels = sum(len(v) for v in player_groups.values())
    weishi_total = len(player_groups.get(GROUP_SECONDARY_WEISHI, []))
    total_sources = sum(len(s[1]) for v in player_groups.values() for s in v)
    
    m3u8_content.extend([
        f"# ğŸ“Š æ±‡æ€»ç»Ÿè®¡ | {GLOBAL_UPDATE_TIME_FULL}",
        f"# æ€»é¢‘é“æ•°ï¼š{total_channels}ä¸ª | å«è§†é¢‘é“ï¼š{weishi_total}ä¸ª | æ€»æœ‰æ•ˆæºï¼š{total_sources}ä¸ª",
        f"# éªŒè¯æˆåŠŸç‡ï¼š{round(total_sources/len(task_list)*100,1) if task_list else 100}%",
        f"# æç¤ºï¼šé»˜è®¤æ’­æ”¾ç¬¬1ä¸ªæœ€ä¼˜æºï¼Œå¡é¡¿å¯æ‰‹åŠ¨åˆ‡æ¢å…¶ä»–2ä¸ªå¤‡ç”¨æº",
        GROUP_SEPARATOR
    ])
    
    # 7. å†™å…¥æ–‡ä»¶ï¼ˆè¦†ç›–åŸæœ‰æ–‡ä»¶ï¼Œç¡®ä¿æ›´æ–°ï¼‰
    try:
        with open(OUTPUT_FILE, "w", encoding="utf-8", buffering=1024*1024) as f:
            f.write("\n".join(m3u8_content))
        
        # éªŒè¯æ–‡ä»¶æ˜¯å¦ç”ŸæˆæˆåŠŸï¼Œä¸”æœ‰æœ‰æ•ˆå¤§å°
        file_size = Path(OUTPUT_FILE).stat().st_size / 1024
        logger.info(f"âœ… m3u8æ–‡ä»¶ç”Ÿæˆå®Œæˆ â†’ æ–‡ä»¶åï¼š{OUTPUT_FILE} | æ–‡ä»¶å¤§å°ï¼š{file_size:.2f}KB")
        logger.info(f"âœ… æ¯ä¸ªé¢‘é“ä¿ç•™3ä¸ªæ’­æ”¾æºï¼Œé»˜è®¤æ’­æ”¾æœ€ä¼˜æºï¼Œæ”¯æŒæ‰‹åŠ¨åˆ‡æ¢å¤‡ç”¨æº")
        return True
    except Exception as e:
        logger.error(f"ç”Ÿæˆå¤±è´¥ï¼šæ–‡ä»¶å†™å…¥å‡ºé”™ â†’ {str(e)[:50]}")
        return False

# -------------------------- å„ç±»å‹é¢‘é“æ’åºå‡½æ•°ï¼ˆå®Œæ•´ï¼‰ --------------------------
def get_cctv_sort_key(channel_name: str) -> Tuple[int, str]:
    if not CCTV_SORT_ENABLE or "CCTV" not in channel_name.upper():
        return (999, channel_name.upper())
    match = RE_CCTV_NUMBER.search(channel_name.upper())
    return (int(match.group(1)) if match else 999, channel_name.upper())

def get_weishi_sort_key(channel_name: str) -> Tuple[int, str]:
    if not WEISHI_SORT_ENABLE:
        return (999, channel_name.upper())
    # ä¸€çº¿å«è§†ä¼˜å…ˆ
    for idx, top_ws in enumerate(TOP_WEISHI):
        if top_ws in channel_name:
            return (idx, channel_name.upper())
    # å…¶ä»–å«è§†æŒ‰çœä»½æ‹¼éŸ³æ’åº
    for idx, province in enumerate(PROVINCE_PINYIN_ORDER):
        if province in channel_name:
            return (len(TOP_WEISHI) + idx, channel_name.upper())
    # æ— åŒ¹é…çš„å«è§†æ’æœ€å
    return (len(TOP_WEISHI) + len(PROVINCE_PINYIN_ORDER), channel_name.upper())

def get_local_sort_key(channel_name: str) -> Tuple[int, str]:
    if not LOCAL_SORT_ENABLE:
        return (999, channel_name.upper())
    # ç›´è¾–å¸‚ä¼˜å…ˆ
    for idx, city in enumerate(DIRECT_CITIES):
        if city in channel_name:
            return (idx, channel_name.upper())
    # çœä»½æŒ‰æ‹¼éŸ³æ’åº
    for idx, province in enumerate(PROVINCE_PINYIN_ORDER):
        if province in channel_name and province not in DIRECT_CITIES:
            return (len(DIRECT_CITIES) + idx, channel_name.upper())
    # å…¶ä»–åœ°æ–¹é¢‘é“æ’æœ€å
    return (len(DIRECT_CITIES) + len(PROVINCE_PINYIN_ORDER), channel_name.upper())

def get_feature_sort_key(channel_name: str) -> Tuple[int, str]:
    if not FEATURE_SORT_ENABLE:
        return (999, channel_name.upper())
    # æŒ‰ç‰¹è‰²ç±»å‹æ’åº
    for idx, (feature_type, keywords) in enumerate(FEATURE_TYPE_ORDER):
        if any(keyword in channel_name for keyword in keywords):
            return (idx, channel_name.upper())
    # å…¶ä»–ç‰¹è‰²é¢‘é“æ’æœ€å
    return (len(FEATURE_TYPE_ORDER), channel_name.upper())

def get_digital_sort_key(channel_name: str) -> Tuple[int, str]:
    if not DIGITAL_SORT_ENABLE:
        return (999, channel_name.upper())
    match = RE_DIGITAL_NUMBER.match(channel_name)
    return (int(match.group(1)) if match else 999, channel_name.upper())

def get_channel_sort_key(group_name: str, channel_name: str) -> Tuple[int, str]:
    """ç»Ÿä¸€æ’åºå…¥å£ï¼Œæ ¹æ®åˆ†ç»„è°ƒç”¨å¯¹åº”æ’åºå‡½æ•°"""
    if group_name == GROUP_SECONDARY_CCTV:
        return get_cctv_sort_key(channel_name)
    elif group_name == GROUP_SECONDARY_WEISHI:
        return get_weishi_sort_key(channel_name)
    elif group_name == GROUP_SECONDARY_LOCAL:
        return get_local_sort_key(channel_name)
    elif group_name == GROUP_SECONDARY_FEATURE:
        return get_feature_sort_key(channel_name)
    elif group_name == GROUP_SECONDARY_DIGITAL:
        return get_digital_sort_key(channel_name)
    else:
        return (999, channel_name.upper())

# -------------------------- ä¸»ç¨‹åºï¼ˆå®Œæ•´ï¼Œå¸¦æµç¨‹æ ¡éªŒï¼‰ --------------------------
if __name__ == "__main__":
    start_total = time.time()
    logger.info("="*60)
    logger.info("IPTVç›´æ’­æºæŠ“å–å·¥å…· - è‡ªåŠ¨é€‰æœ€ä¼˜+3ä¸ªæ‰‹åŠ¨åˆ‡æ¢æºï¼ˆç»ˆæä¿®å¤ç‰ˆï¼‰")
    logger.info("="*60)
    logger.info(f"å¯åŠ¨é…ç½® | CPUï¼š{CPU_CORES}æ ¸ | éªŒè¯çº¿ç¨‹ï¼š{MAX_THREADS_VERIFY} | æŠ“å–çº¿ç¨‹ï¼š{MAX_THREADS_FETCH}")
    logger.info(f"æ›´æ–°æ—¶é—´ | å®Œæ•´ï¼š{GLOBAL_UPDATE_TIME_FULL} | ç²¾ç®€ï¼š{GLOBAL_UPDATE_TIME_SHORT}")
    logger.info(f"é€‰æºé…ç½® | è‡ªåŠ¨é€‰æœ€ä¼˜ï¼š{AUTO_SELECT_SOURCE} | æ¯ä¸ªé¢‘é“ä¿ç•™æºæ•°ï¼š{TOTAL_SOURCES_PER_CHANNEL}")
    logger.info("="*60)
    
    # æ‰§è¡Œæ ¸å¿ƒæµç¨‹
    load_persist_cache()
    fetch_raw_data_parallel()
    extract_verify_tasks(all_lines)
    verify_tasks_parallel(task_list)
    generate_success = generate_player_m3u8()
    save_persist_cache()
    
    # æ‰“å°æœ€ç»ˆæ‰§è¡Œç»“æœ
    total_time = round(time.time() - start_total, 2)
    logger.info("="*60)
    if generate_success:
        logger.info(f"æ‰§è¡Œå®Œæˆ | æ€»è€—æ—¶ï¼š{total_time}ç§’ | ç”Ÿæˆæ–‡ä»¶ï¼š{OUTPUT_FILE}ï¼ˆå·²æ›´æ–°ï¼‰")
        logger.info(f"æ ¸å¿ƒæ•ˆæœ | è‡ªåŠ¨é€‰æœ€ä¼˜æºï¼Œä¿ç•™3ä¸ªæ‰‹åŠ¨åˆ‡æ¢æºï¼Œm3u8æ–‡ä»¶å†…å®¹å·²æ›´æ–°")
    else:
        logger.error(f"æ‰§è¡Œå¤±è´¥ | æ€»è€—æ—¶ï¼š{total_time}ç§’ | æœªç”Ÿæˆæœ‰æ•ˆm3u8æ–‡ä»¶")
    logger.info("="*60)
