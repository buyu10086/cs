import requests
import time
import random
import json
from datetime import datetime, timedelta
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from pathlib import Path  # æ–°å¢ï¼šå¤„ç†æ–‡ä»¶è·¯å¾„æ›´å®‰å…¨

# -------------------------- å…¨å±€é…ç½®ï¼ˆé›†ä¸­ç®¡ç†ï¼Œæ›´æ˜“ä¿®æ”¹ï¼Œè¡¥å……æ³¨é‡Šï¼‰ --------------------------
# 1. æ•°æ®æºé…ç½®
IPTV_SOURCE_URLS = [
    "https://raw.githubusercontent.com/kakaxi-1/zubo/refs/heads/main/IPTV.txt",
    "https://raw.githubusercontent.com/kakaxi-1/IPTV/refs/heads/main/ipv4.txt",
    "https://raw.githubusercontent.com/vbskycn/iptv/refs/heads/master/tv/iptv4.m3u",
    "https://raw.githubusercontent.com/mytv-android/China-TV-Live-M3U8/refs/heads/main/webview.m3u"
]

# 2. éªŒè¯ä¸è¶…æ—¶é…ç½®
TIMEOUT_VERIFY = 3  # å•ä¸ªç›´æ’­æºéªŒè¯è¶…æ—¶æ—¶é—´ï¼ˆç¼©çŸ­ï¼Œæå‡æ•ˆç‡ï¼‰
TIMEOUT_FETCH = 10  # æ•°æ®æºæ–‡ä»¶æŠ“å–è¶…æ—¶æ—¶é—´ï¼ˆç¨é•¿ï¼Œä¿è¯å®Œæ•´è·å–ï¼‰
MIN_VALID_CHANNELS = 3  # ä¼˜åŒ–ï¼šæ”¹ä¸ºæœ‰æ•ˆé¢‘é“æ•°é˜ˆå€¼ï¼ˆæ›´è´´åˆä¸šåŠ¡é€»è¾‘ï¼‰
MAX_THREADS_VERIFY = 30  # ç›´æ’­æºéªŒè¯çº¿ç¨‹æ•°ï¼ˆå¤§é‡ä»»åŠ¡ï¼Œé«˜çº¿ç¨‹ï¼‰
MAX_THREADS_FETCH = 5   # æ•°æ®æºæŠ“å–çº¿ç¨‹æ•°ï¼ˆå°‘é‡URLï¼Œæ— éœ€é«˜çº¿ç¨‹ï¼Œå‡å°‘èµ„æºæµªè´¹ï¼‰

# 3. è¾“å‡ºä¸å»é‡é…ç½®
OUTPUT_FILE = "iptv_playlist.m3u8"
REMOVE_DUPLICATE_CHANNELS = True  # ç°åœ¨å·²ç”Ÿæ•ˆï¼šå»é™¤é‡å¤é¢‘é“
REMOVE_LOCAL_URLS = True  # æ–°å¢ï¼šè¿‡æ»¤æœ¬åœ°æ— æ•ˆURLï¼ˆlocalhost/127.0.0.1ç­‰ï¼‰

# 4. ç¼“å­˜é…ç½®
CACHE_FILE = "iptv_verified_cache.json"
CACHE_EXPIRE_HOURS = 24  # ç¼“å­˜æœ‰æ•ˆæœŸ24å°æ—¶

# 5. åçˆ¬ä¸æ ¼å¼é…ç½®
MIN_DELAY = 0.1
MAX_DELAY = 0.5
CHANNEL_SORT_ENABLE = True  # æ–°å¢ï¼šé¢‘é“æŒ‰åç§°æ’åºï¼Œç”Ÿæˆæ–‡ä»¶æ›´æ•´æ´

# -------------------------- çº¿ç¨‹å®‰å…¨æ•°æ®ï¼ˆå‡å°‘å…¨å±€å˜é‡ä¾èµ–ï¼Œä¼˜åŒ–é”é€»è¾‘ï¼‰ --------------------------
channel_sources_map = {}
map_lock = threading.Lock()
verified_urls = set()
url_lock = threading.Lock()

# -------------------------- å·¥å…·å‡½æ•°ä¼˜åŒ–ï¼ˆç¼–ç å¤„ç†ã€èµ„æºå…³é—­ã€URLè¿‡æ»¤ï¼‰ --------------------------
def add_random_delay():
    """æ·»åŠ éšæœºåçˆ¬å»¶è¿Ÿï¼ˆä¸å½±å“æ•´ä½“å¹¶è¡Œæ•ˆç‡ï¼‰"""
    delay = random.uniform(MIN_DELAY, MAX_DELAY)
    time.sleep(delay)

def filter_invalid_urls(url):
    """æ–°å¢ï¼šè¿‡æ»¤æ— æ•ˆURLï¼ˆæœ¬åœ°åœ°å€ã€ç©ºå€¼ï¼‰"""
    if not url or not url.startswith(("http://", "https://")):
        return False
    if REMOVE_LOCAL_URLS and any(host in url.lower() for host in ["localhost", "127.0.0.1", "192.168.", "10.", "172."]):
        return False
    return True

def safe_extract_channel_name(line):
    """ä¼˜åŒ–ï¼šå¢å¼ºé¢‘é“åæå–èƒ½åŠ›ï¼Œé€‚é…æ›´å¤šM3Uæ ¼å¼ï¼Œæé«˜æˆåŠŸç‡"""
    if not line.startswith("#EXTINF:"):
        return None
    
    # æ­£åˆ™ä¼˜åŒ–ï¼šä¼˜å…ˆåŒ¹é…æœ€åä¸€ä¸ªé€—å·åå†…å®¹ï¼Œå†åŒ¹é…tvg-nameï¼Œæœ€ååŒ¹é…title
    patterns = [
        r',\s*([^,]+)\s*$',  # ä¼˜å…ˆï¼šåŒ¹é…é€—å·åå†…å®¹ï¼ˆæœ€å¸¸è§æ ¼å¼ï¼‰
        r'tvg-name="([^"]+)"',  # å¤‡é€‰1ï¼šåŒ¹é…tvg-nameå±æ€§
        r'title="([^"]+)"',     # å¤‡é€‰2ï¼šåŒ¹é…titleå±æ€§
        r'[^"]+\s+([^,\s]+)$'   # å¤‡é€‰3ï¼šåŒ¹é…æœ€åä¸€ä¸ªéç©ºç™½/éé€—å·å†…å®¹
    ]
    
    for pattern in patterns:
        match = re.search(pattern, line, re.IGNORECASE)
        if match:
            channel_name = match.group(1).strip()
            # è¿‡æ»¤æ— æ•ˆé¢‘é“å
            if channel_name and not channel_name.isdigit():
                return channel_name
    return "æœªçŸ¥é¢‘é“"

# -------------------------- ç¼“å­˜å‡½æ•°ä¼˜åŒ–ï¼ˆå¥å£®æ€§æå‡ï¼Œå‡å°‘å¼‚å¸¸ï¼‰ --------------------------
def load_verified_cache():
    """åŠ è½½æœ¬åœ°å·²éªŒè¯æºçš„ç¼“å­˜ï¼ˆå¸¦è¿‡æœŸåˆ¤æ–­ï¼Œä¼˜åŒ–å¼‚å¸¸å¤„ç†ï¼‰"""
    global verified_urls
    try:
        cache_path = Path(CACHE_FILE)
        if not cache_path.exists():
            print(f"â„¹ï¸  æœªæ‰¾åˆ°ç¼“å­˜æ–‡ä»¶ {CACHE_FILE}ï¼Œå°†åœ¨è¿è¡Œååˆ›å»º")
            return
        
        # å®‰å…¨è¯»å–æ–‡ä»¶ï¼ŒæŒ‡å®šç¼–ç 
        with open(cache_path, "r", encoding="utf-8") as f:
            cache_data = json.load(f)
        
        # éªŒè¯ç¼“å­˜æ—¶é—´æˆ³æ ¼å¼
        cache_time_str = cache_data.get("cache_time", "")
        if not cache_time_str:
            print("âš ï¸  ç¼“å­˜æ–‡ä»¶æ— æœ‰æ•ˆæ—¶é—´æˆ³ï¼Œè·³è¿‡åŠ è½½")
            return
        
        try:
            cache_time = datetime.strptime(cache_time_str, '%Y-%m-%d %H:%M:%S')
        except ValueError:
            print("âš ï¸  ç¼“å­˜æ—¶é—´æˆ³æ ¼å¼é”™è¯¯ï¼Œè·³è¿‡åŠ è½½")
            return
        
        # åˆ¤æ–­ç¼“å­˜æ˜¯å¦è¿‡æœŸ
        expire_time = cache_time + timedelta(hours=CACHE_EXPIRE_HOURS)
        current_time = datetime.now()
        if current_time > expire_time:
            print(f"âš ï¸  ç¼“å­˜å·²è¿‡æœŸï¼ˆè¶…è¿‡{CACHE_EXPIRE_HOURS}å°æ—¶ï¼‰ï¼Œè·³è¿‡åŠ è½½")
            return
        
        # åŠ è½½æœ‰æ•ˆç¼“å­˜ï¼Œå»é‡
        valid_urls = cache_data.get("verified_urls", [])
        verified_urls = set(filter(filter_invalid_urls, valid_urls))  # æ–°å¢ï¼šè¿‡æ»¤ç¼“å­˜ä¸­çš„æ— æ•ˆURL
        print(f"âœ… æˆåŠŸåŠ è½½æœ¬åœ°ç¼“å­˜ï¼Œå…± {len(verified_urls)} ä¸ªæœ‰æ•ˆå·²éªŒè¯æºï¼ˆç¼“å­˜æ—¶é—´ï¼š{cache_time_str}ï¼‰")
    
    except json.JSONDecodeError:
        print(f"âš ï¸  ç¼“å­˜æ–‡ä»¶æ ¼å¼æŸåï¼Œæ— æ³•åŠ è½½")
    except Exception as e:
        print(f"âš ï¸  åŠ è½½ç¼“å­˜å¤±è´¥ï¼š{str(e)[:50]}")

def save_verified_cache():
    """ä¿å­˜å½“å‰å·²éªŒè¯æºåˆ°æœ¬åœ°ç¼“å­˜ï¼ˆä¼˜åŒ–è·¯å¾„å¤„ç†ï¼Œç¡®ä¿ç›®å½•å­˜åœ¨ï¼‰"""
    try:
        cache_path = Path(CACHE_FILE)
        # æ–°å¢ï¼šåˆ›å»ºç¼“å­˜æ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        cache_path.parent.mkdir(parents=True, exist_ok=True)
        
        cache_data = {
            "cache_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "verified_urls": list(verified_urls)
        }
        
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æˆåŠŸä¿å­˜ç¼“å­˜åˆ° {CACHE_FILE}ï¼Œå…± {len(verified_urls)} ä¸ªå·²éªŒè¯æº")
    except Exception as e:
        print(f"âŒ ä¿å­˜ç¼“å­˜å¤±è´¥ï¼š{str(e)[:50]}")

# -------------------------- æ•°æ®æºæŠ“å–ä¼˜åŒ–ï¼ˆç¼–ç å¤„ç†ã€çº¿ç¨‹æ•°ä¼˜åŒ–ã€èµ„æºå®‰å…¨ï¼‰ --------------------------
def fetch_single_source(url, idx):
    """å¹¶è¡ŒæŠ“å–å•ä¸ªæ•°æ®æºï¼Œè¿”å›(æ˜¯å¦æˆåŠŸ, æœ‰æ•ˆè¡Œåˆ—è¡¨)ï¼ˆä¼˜åŒ–ç¼–ç +èµ„æºå¤„ç†ï¼‰"""
    add_random_delay()
    
    try:
        response = requests.get(
            url,
            timeout=TIMEOUT_FETCH,
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Referer": "https://github.com/",
                "Accept": "*/*"
            },
            stream=False  # å°æ–‡ä»¶ç›´æ¥è·å–ï¼Œæ— éœ€æµå¼
        )
        response.raise_for_status()
        
        # ä¼˜åŒ–ï¼šè‡ªåŠ¨æ£€æµ‹ç¼–ç ï¼Œè§£å†³ä¹±ç é—®é¢˜
        response.encoding = response.apparent_encoding or "utf-8"
        lines = response.text.splitlines()
        
        # è¿‡æ»¤æ— æ•ˆè¡Œï¼šç©ºè¡Œã€//æ³¨é‡Šè¡Œã€çº¯ç©ºæ ¼è¡Œ
        valid_lines = []
        for line in lines:
            line_strip = line.strip()
            if line_strip and not line_strip.startswith("//"):
                valid_lines.append(line_strip)
        
        print(f"âœ… æ•°æ®æº {idx+1} æŠ“å–æˆåŠŸï¼Œæœ‰æ•ˆè¡Œ {len(valid_lines)}")
        return True, valid_lines
    
    except requests.exceptions.Timeout:
        print(f"âŒ æ•°æ®æº {idx+1} æŠ“å–è¶…æ—¶ï¼ˆè¶…è¿‡{TIMEOUT_FETCH}ç§’ï¼‰")
    except requests.exceptions.HTTPError as e:
        print(f"âŒ æ•°æ®æº {idx+1} HTTPé”™è¯¯ï¼š{str(e)[:50]}")
    except requests.exceptions.ConnectionError:
        print(f"âŒ æ•°æ®æº {idx+1} è¿æ¥å¤±è´¥")
    except Exception as e:
        print(f"âŒ æ•°æ®æº {idx+1} æŠ“å–å¤±è´¥ï¼š{str(e)[:50]}")
    
    return False, []

def fetch_raw_iptv_data_parallel(url_list):
    """å¹¶è¡ŒæŠ“å–æ‰€æœ‰æ•°æ®æºï¼ˆä¼˜åŒ–çº¿ç¨‹æ•°ï¼Œå‡å°‘èµ„æºæµªè´¹ï¼‰"""
    all_lines = []
    valid_source_count = 0
    # ä¼˜åŒ–ï¼šæŠ“å–é˜¶æ®µçº¿ç¨‹æ•°é€‚é…URLæ•°é‡ï¼ˆæœ€å¤š5ä¸ªï¼‰ï¼Œæ— éœ€30çº¿ç¨‹
    fetch_threads = min(MAX_THREADS_FETCH, len(url_list))
    
    with ThreadPoolExecutor(max_workers=fetch_threads) as executor:
        future_to_idx = {executor.submit(fetch_single_source, url, idx): idx for idx, url in enumerate(url_list)}
        for future in as_completed(future_to_idx):
            success, lines = future.result()
            if success and lines:
                all_lines.extend(lines)
                valid_source_count += 1
    
    print(f"\nğŸ“Š å¹¶è¡ŒæŠ“å–å®Œæˆï¼šå°è¯• {len(url_list)} æºï¼Œå¯ç”¨ {valid_source_count} æº")
    return all_lines

# -------------------------- æºéªŒè¯ä¼˜åŒ–ï¼ˆç¼©å°é”èŒƒå›´ã€æå‰å»é‡ã€èµ„æºå…³é—­ï¼‰ --------------------------
def verify_single_source(url, channel_name):
    """éªŒè¯å•ä¸ªæºæ˜¯å¦å¯ç”¨ï¼Œè¿”å›(é¢‘é“å, æœ‰æ•ˆurl)ï¼ˆä¼˜åŒ–é”èŒƒå›´ï¼Œæå‡å¹¶è¡Œæ•ˆç‡ï¼‰"""
    # å‰ç½®è¿‡æ»¤ï¼šæ— æ•ˆURLç›´æ¥è¿”å›
    if not filter_invalid_urls(url):
        return None, None
    
    add_random_delay()
    
    # ä¼˜åŒ–ï¼šæŸ¥è¯¢setæ— éœ€åŠ é”ï¼ˆsetæŸ¥è¯¢çº¿ç¨‹å®‰å…¨ï¼‰ï¼Œä»…ä¿®æ”¹æ—¶åŠ é”ï¼Œå‡å°‘é”ç«äº‰
    if url in verified_urls:
        return channel_name, url
    
    try:
        # ä¼˜åŒ–ï¼šstream=Trueæ—¶å…³é—­å“åº”ï¼Œé¿å…èµ„æºæ³„éœ²
        with requests.get(
            url,
            timeout=TIMEOUT_VERIFY,
            allow_redirects=True,
            headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36"},
            stream=True
        ) as response:
            # ä»…éªŒè¯çŠ¶æ€ç ï¼Œä¸ä¸‹è½½å†…å®¹
            valid_status_codes = [200, 206, 301, 302, 307, 308]
            if response.status_code in valid_status_codes:
                # ä»…ä¿®æ”¹verified_urlsæ—¶åŠ é”ï¼Œç¼©å°é”èŒƒå›´
                with url_lock:
                    verified_urls.add(url)
                return channel_name, url
    except:
        pass
    
    return None, None

def get_channel_group(channel_name):
    """é¢‘é“åˆ†ç»„é€»è¾‘ï¼ˆä¿ç•™åŸæœ‰åŠŸèƒ½ï¼Œæ ¼å¼ä¼˜åŒ–ï¼‰"""
    if not channel_name:
        return "ğŸ¬ å…¶ä»–é¢‘é“"
    
    cctv_keywords = ["CCTV", "å¤®è§†", "ä¸­å¤®", "å¤®è§†é¢‘", "CCTV-", "ä¸­è§†"]
    if any(keyword in channel_name for keyword in cctv_keywords):
        return "ğŸ“º å¤®è§†é¢‘é“"
    
    if "å«è§†" in channel_name:
        return "ğŸ“¡ å«è§†é¢‘é“"
    
    province_city = ["åŒ—äº¬", "ä¸Šæµ·", "å¤©æ´¥", "é‡åº†", "æ²³åŒ—", "å±±è¥¿", "è¾½å®", "å‰æ—", "é»‘é¾™æ±Ÿ",
                     "æ±Ÿè‹", "æµ™æ±Ÿ", "å®‰å¾½", "ç¦å»º", "æ±Ÿè¥¿", "å±±ä¸œ", "æ²³å—", "æ¹–åŒ—", "æ¹–å—",
                     "å¹¿ä¸œ", "å¹¿è¥¿", "æµ·å—", "å››å·", "è´µå·", "äº‘å—", "é™•è¥¿", "ç”˜è‚ƒ", "é’æµ·",
                     "å†…è’™å¤", "å®å¤", "æ–°ç–†", "è¥¿è—", "é¦™æ¸¯", "æ¾³é—¨", "å°æ¹¾",
                     "å¹¿å·", "æ·±åœ³", "æ­å·", "å—äº¬", "æˆéƒ½", "æ­¦æ±‰", "è¥¿å®‰", "éƒ‘å·", "é’å²›"]
    for area in province_city:
        if area in channel_name and "å«è§†" not in channel_name:
            return "ğŸ™ï¸ åœ°æ–¹é¢‘é“"
    
    return "ğŸ¬ å…¶ä»–é¢‘é“"

# -------------------------- M3U8ç”Ÿæˆä¼˜åŒ–ï¼ˆç”Ÿæ•ˆå»é‡ã€æ’åºã€ç›®å½•å¤„ç†ï¼‰ --------------------------
def generate_m3u8_parallel(raw_lines):
    """å¹¶è¡ŒéªŒè¯æº+ç”Ÿæˆm3u8æ–‡ä»¶ï¼ˆä¼˜åŒ–å»é‡ã€æ’åºã€å®¹é”™ï¼‰"""
    update_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    m3u8_header = f"""#EXTM3U x-tvg-url="https://iptv-org.github.io/epg/guides/cn/tv.cctv.com.epg.xml"
# æ›´æ–°æ—¶é—´ï¼š{update_time}
# æ”¯æŒå¤šæºåˆ‡æ¢+é¢‘é“åˆ†ç»„+æœ¬åœ°ç¼“å­˜ä¼˜åŒ–+è‡ªåŠ¨å»é‡+æ— æ•ˆURLè¿‡æ»¤
"""
    valid_lines = [m3u8_header]
    valid_lines.append(f"#EXTINF:-1 group-title='ğŸ“¢ ç³»ç»Ÿä¿¡æ¯',ğŸ“… ç›´æ’­æºæ›´æ–°æ—¶é—´ï¼š{update_time}")
    valid_lines.append("#")

    # ç¬¬ä¸€æ­¥ï¼šæå–æ‰€æœ‰å¾…éªŒè¯çš„(é¢‘é“å, url)å¯¹ï¼Œæå‰å»é‡é¿å…é‡å¤ä»»åŠ¡
    task_list = []
    temp_channel = None
    seen_urls = set()  # ä¸´æ—¶å»é‡ï¼Œé¿å…é‡å¤éªŒè¯åŒä¸€ä¸ªURL
    
    for line in raw_lines:
        line_strip = line.strip()
        if not line_strip:
            continue
        
        if line_strip.startswith("#EXTINF:"):
            temp_channel = safe_extract_channel_name(line_strip)
        elif filter_invalid_urls(line_strip) and temp_channel:
            # å»é‡ï¼šåŒä¸€ä¸ªURLä¸é‡å¤åŠ å…¥ä»»åŠ¡åˆ—è¡¨
            if line_strip not in seen_urls:
                seen_urls.add(line_strip)
                task_list.append((line_strip, temp_channel))
            temp_channel = None
    
    print(f"\nğŸ” å¾…éªŒè¯æºæ€»æ•°ï¼š{len(task_list)}ï¼ˆå·²å»é‡+è¿‡æ»¤æ— æ•ˆURL+å¤ç”¨æœ¬åœ°ç¼“å­˜ï¼‰")

    # ç¬¬äºŒæ­¥ï¼šå¹¶è¡ŒéªŒè¯æ‰€æœ‰æº
    total_valid = 0
    with ThreadPoolExecutor(max_workers=MAX_THREADS_VERIFY) as executor:
        future_to_task = {executor.submit(verify_single_source, url, chan): (url, chan) for url, chan in task_list}
        for future in as_completed(future_to_task):
            chan_name, valid_url = future.result()
            if chan_name and valid_url:
                total_valid += 1
                # çº¿ç¨‹å®‰å…¨æ›´æ–°é¢‘é“-æºæ˜ å°„ï¼ŒåŒæ—¶å»é‡ï¼ˆåŒä¸€ä¸ªé¢‘é“çš„é‡å¤URLï¼‰
                with map_lock:
                    if chan_name not in channel_sources_map:
                        channel_sources_map[chan_name] = []
                    if valid_url not in channel_sources_map[chan_name]:
                        channel_sources_map[chan_name].append(valid_url)

    # ç¬¬ä¸‰æ­¥ï¼šé¢‘é“å»é‡ï¼ˆç”Ÿæ•ˆREMOVE_DUPLICATE_CHANNELSé…ç½®ï¼‰
    if REMOVE_DUPLICATE_CHANNELS:
        # å»é‡é€»è¾‘ï¼šä¿ç•™æºæ•°é‡æœ€å¤šçš„é¢‘é“ï¼ˆåŒåé¢‘é“åˆå¹¶ï¼‰
        dedup_map = {}
        for chan_name, sources in channel_sources_map.items():
            if chan_name not in dedup_map or len(sources) > len(dedup_map[chan_name]):
                dedup_map[chan_name] = sources
        channel_sources_map.clear()
        channel_sources_map.update(dedup_map)
        print(f"\nâœ¨ é¢‘é“å»é‡å®Œæˆï¼Œå‰©ä½™æœ‰æ•ˆé¢‘é“ {len(channel_sources_map)} ä¸ª")

    # ç¬¬å››æ­¥ï¼šæŒ‰åˆ†ç»„ç”Ÿæˆæ–‡ä»¶ï¼Œæ”¯æŒé¢‘é“æ’åº
    grouped_channels = {"ğŸ“º å¤®è§†é¢‘é“": [], "ğŸ“¡ å«è§†é¢‘é“": [], "ğŸ™ï¸ åœ°æ–¹é¢‘é“": [], "ğŸ¬ å…¶ä»–é¢‘é“": []}
    
    for channel_name, sources in channel_sources_map.items():
        if sources:
            group = get_channel_group(channel_name)
            grouped_channels[group].append((channel_name, sources))

    # ä¼˜åŒ–ï¼šé¢‘é“æŒ‰åç§°æ’åºï¼Œç”Ÿæˆæ–‡ä»¶æ›´æ•´æ´
    for group_name, channels in grouped_channels.items():
        if not channels:
            continue
        # æ’åºå¼€å…³ç”Ÿæ•ˆ
        if CHANNEL_SORT_ENABLE:
            channels.sort(key=lambda x: x[0])  # æŒ‰é¢‘é“åç§°å­—æ¯åºæ’åº
        valid_lines.append(f"\n# {group_name}")
        for channel_name, sources in channels:
            valid_lines.append(f"#EXTINF:-1 group-title='{group_name}',{channel_name}ï¼ˆ{len(sources)}ä¸ªæºï¼‰")
            for idx, url in enumerate(sources, 1):
                # æ–°å¢ï¼šæºæ ‡æ³¨åºå·ï¼Œæ–¹ä¾¿è¯†åˆ«åˆ‡æ¢
                valid_lines.append(f"# æº{idx}ï¼š{url[:60]}...")
                valid_lines.append(url)
                print(f"ğŸ“º [{group_name}] [{channel_name}] - æœ‰æ•ˆæº{idx}ï¼š{url[:50]}...")

    # ç¬¬äº”æ­¥ï¼šå®¹é”™é€»è¾‘ä¼˜åŒ–ï¼ˆåˆ¤æ–­æœ‰æ•ˆé¢‘é“æ•°ï¼Œæ›´è´´åˆä¸šåŠ¡ï¼‰
    valid_channel_count = len(channel_sources_map)
    if valid_channel_count < MIN_VALID_CHANNELS:
        print(f"\nâš ï¸  æœ‰æ•ˆé¢‘é“({valid_channel_count})ä½äºé˜ˆå€¼({MIN_VALID_CHANNELS})ï¼Œç”ŸæˆåŸºç¡€æ–‡ä»¶")
        output_path = Path(OUTPUT_FILE)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(f"""#EXTM3U
# æ›´æ–°æ—¶é—´ï¼š{update_time}
#EXTINF:-1 group-title='ğŸ“¢ ç³»ç»Ÿä¿¡æ¯',ğŸ“… ç›´æ’­æºæ›´æ–°æ—¶é—´ï¼š{update_time}
#EXTINF:-1 group-title='ğŸ“¢ ç³»ç»Ÿä¿¡æ¯',âš ï¸  æœ‰æ•ˆé¢‘é“è¾ƒå°‘ï¼Œå»ºè®®ç¨åé‡è¯•
#""")
        return False

    # ç¬¬å…­æ­¥ï¼šå†™å…¥æœ€ç»ˆæ–‡ä»¶ï¼ˆä¼˜åŒ–è·¯å¾„å¤„ç†ï¼Œç¡®ä¿ç›®å½•å­˜åœ¨ï¼‰
    try:
        output_path = Path(OUTPUT_FILE)
        # æ–°å¢ï¼šåˆ›å»ºè¾“å‡ºæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, "w", encoding="utf-8") as f:
            f.write("\n".join(valid_lines))
    except Exception as e:
        print(f"âŒ å†™å…¥è¾“å‡ºæ–‡ä»¶å¤±è´¥ï¼š{str(e)[:50]}")
        return False

    # ç¬¬ä¸ƒæ­¥ï¼šè¾“å‡ºæœ€ç»ˆç»Ÿè®¡
    print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡ï¼šéªŒè¯ {len(task_list)} æºï¼Œæœ‰æ•ˆ {total_valid} æºï¼Œæœ‰æ•ˆé¢‘é“ {valid_channel_count} ä¸ª")
    for group_name, channels in grouped_channels.items():
        print(f"   ğŸ“‹ {group_name}ï¼š{len(channels)} é¢‘é“")
    print(f"âœ… ç”Ÿæˆå®Œæˆï¼š{OUTPUT_FILE}ï¼ˆè·¯å¾„ï¼š{Path(OUTPUT_FILE).absolute()}ï¼‰")
    return True

# -------------------------- ä¸»ç¨‹åºï¼ˆä¿æŒåŸæœ‰æµç¨‹ï¼Œä¼˜åŒ–è¾“å‡ºï¼‰ --------------------------
if __name__ == "__main__":
    start_time = time.time()
    print("========== å¹¶è¡ŒåŒ–IPTVæºæŠ“å–ï¼ˆç¼“å­˜+åçˆ¬+å»é‡ä¼˜åŒ–ç‰ˆï¼‰ ==========")
    
    # åŠ è½½æœ¬åœ°ç¼“å­˜
    load_verified_cache()
    
    # æŠ“å–åŸå§‹æ•°æ®
    raw_data = fetch_raw_iptv_data_parallel(IPTV_SOURCE_URLS)
    
    # ç”ŸæˆM3U8æ–‡ä»¶
    if raw_data:
        generate_m3u8_parallel(raw_data)
    else:
        print("\nâŒ æœªæŠ“å–åˆ°ä»»ä½•åŸå§‹æ•°æ®ï¼Œæ— æ³•ç”ŸæˆM3U8æ–‡ä»¶")
    
    # ä¿å­˜ç¼“å­˜åˆ°æœ¬åœ°
    save_verified_cache()
    
    # è®¡ç®—æ€»è€—æ—¶
    total_time = time.time() - start_time
    print(f"\nâ±ï¸  æ€»è¿è¡Œæ—¶é—´ï¼š{total_time:.2f} ç§’ï¼ˆçº¦ {total_time/60:.1f} åˆ†é’Ÿï¼‰")
    print("========== æŠ“å–å®Œæˆï¼Œç¼“å­˜å·²ä¿å­˜ï¼Œä¸‹æ¬¡è¿è¡Œå°†æ›´å¿« ==========")
